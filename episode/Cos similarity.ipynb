{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is used to find similar questions from SO given a question from SO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import collections\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "import heapq\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2010-07-19T19:12:12.510</td>\n",
       "      <td>36</td>\n",
       "      <td>2577.0</td>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-09-15T21:08:26.077</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>&lt;bayesian&gt;&lt;prior&gt;&lt;elicitation&gt;</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19T19:12:57.157</td>\n",
       "      <td>29</td>\n",
       "      <td>23368.0</td>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2016-06-27T06:44:40.147</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>&lt;distributions&gt;&lt;normality&gt;</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19T19:13:28.577</td>\n",
       "      <td>66</td>\n",
       "      <td>5792.0</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2013-05-27T14:48:36.927</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>&lt;software&gt;&lt;open-source&gt;</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>2010-07-19T19:13:31.617</td>\n",
       "      <td>17</td>\n",
       "      <td>26414.0</td>\n",
       "      <td>I have two groups of data.  Each with a differ...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-09-08T03:00:19.690</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>&lt;distributions&gt;&lt;statistical-significance&gt;</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19T19:14:43.050</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The R-project\\n\\nhttp://www.r-project.org/\\n\\n...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19T19:21:15.063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  PostTypeId  ParentId  AcceptedAnswerId             CreationDate  Score  \\\n",
       "0   1           1       NaN              15.0  2010-07-19T19:12:12.510     36   \n",
       "1   2           1       NaN              59.0  2010-07-19T19:12:57.157     29   \n",
       "2   3           1       NaN               5.0  2010-07-19T19:13:28.577     66   \n",
       "3   4           1       NaN             135.0  2010-07-19T19:13:31.617     17   \n",
       "4   5           2       3.0               NaN  2010-07-19T19:14:43.050     87   \n",
       "\n",
       "   ViewCount                                               Body  OwnerUserId  \\\n",
       "0     2577.0  How should I elicit prior distributions from e...          8.0   \n",
       "1    23368.0  In many different statistical methods there is...         24.0   \n",
       "2     5792.0  What are some valuable Statistical Analysis op...         18.0   \n",
       "3    26414.0  I have two groups of data.  Each with a differ...         23.0   \n",
       "4        NaN  The R-project\\n\\nhttp://www.r-project.org/\\n\\n...         23.0   \n",
       "\n",
       "          LastActivityDate                                              Title  \\\n",
       "0  2010-09-15T21:08:26.077                      Eliciting priors from experts   \n",
       "1  2016-06-27T06:44:40.147                                 What is normality?   \n",
       "2  2013-05-27T14:48:36.927  What are some valuable Statistical Analysis op...   \n",
       "3  2010-09-08T03:00:19.690  Assessing the significance of differences in d...   \n",
       "4  2010-07-19T19:21:15.063                                                NaN   \n",
       "\n",
       "                                        Tags  AnswerCount  CommentCount  \\\n",
       "0             <bayesian><prior><elicitation>          5.0             1   \n",
       "1                 <distributions><normality>          7.0             1   \n",
       "2                    <software><open-source>         19.0             4   \n",
       "3  <distributions><statistical-significance>          5.0             2   \n",
       "4                                        NaN          NaN             3   \n",
       "\n",
       "   FavoriteCount  \n",
       "0           23.0  \n",
       "1           10.0  \n",
       "2           39.0  \n",
       "3            5.0  \n",
       "4            NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "post_df = pd.read_csv('all_posts.csv', sep = \"\\t\")\n",
    "post_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>I have two groups of data.  Each with a differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>Last year, I read a blog post from Brendan O'C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                              Title  \\\n",
       "0   1                      Eliciting priors from experts   \n",
       "1   2                                 What is normality?   \n",
       "2   3  What are some valuable Statistical Analysis op...   \n",
       "3   4  Assessing the significance of differences in d...   \n",
       "5   6  The Two Cultures: statistics vs. machine learn...   \n",
       "\n",
       "                                                Body  \n",
       "0  How should I elicit prior distributions from e...  \n",
       "1  In many different statistical methods there is...  \n",
       "2  What are some valuable Statistical Analysis op...  \n",
       "3  I have two groups of data.  Each with a differ...  \n",
       "5  Last year, I read a blog post from Brendan O'C...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_df = post_df.loc[post_df['PostTypeId'] == 1][['Id', 'Title','Body']]\n",
    "\n",
    "Q_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word2vector_model_question_answer_200_6_2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 200\n",
    "min_count = 2\n",
    "window = 6\n",
    "key = 'word2vector_model_question_answer_' + str(int(size))+\"_\"+str(int(window))+\"_\"+str(int(min_count))\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.274664</td>\n",
       "      <td>-0.197037</td>\n",
       "      <td>1.621728</td>\n",
       "      <td>-0.237445</td>\n",
       "      <td>-0.288944</td>\n",
       "      <td>0.843277</td>\n",
       "      <td>0.617973</td>\n",
       "      <td>1.499649</td>\n",
       "      <td>0.842379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057703</td>\n",
       "      <td>-0.040805</td>\n",
       "      <td>0.639898</td>\n",
       "      <td>0.234292</td>\n",
       "      <td>0.024925</td>\n",
       "      <td>0.118979</td>\n",
       "      <td>-0.794148</td>\n",
       "      <td>-0.566620</td>\n",
       "      <td>0.138416</td>\n",
       "      <td>-0.548374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.009377</td>\n",
       "      <td>-0.493120</td>\n",
       "      <td>2.030924</td>\n",
       "      <td>-0.442848</td>\n",
       "      <td>-0.290093</td>\n",
       "      <td>0.380474</td>\n",
       "      <td>1.293419</td>\n",
       "      <td>1.768064</td>\n",
       "      <td>1.919836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605748</td>\n",
       "      <td>0.176924</td>\n",
       "      <td>0.618500</td>\n",
       "      <td>-0.351151</td>\n",
       "      <td>-0.435653</td>\n",
       "      <td>0.027308</td>\n",
       "      <td>-0.069966</td>\n",
       "      <td>0.080411</td>\n",
       "      <td>1.921239</td>\n",
       "      <td>-0.199846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.144012</td>\n",
       "      <td>-0.184814</td>\n",
       "      <td>0.698455</td>\n",
       "      <td>-0.249389</td>\n",
       "      <td>-0.030401</td>\n",
       "      <td>-0.010584</td>\n",
       "      <td>0.131361</td>\n",
       "      <td>0.616810</td>\n",
       "      <td>0.120255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412973</td>\n",
       "      <td>0.268411</td>\n",
       "      <td>-0.134549</td>\n",
       "      <td>-0.435909</td>\n",
       "      <td>-0.054599</td>\n",
       "      <td>0.377649</td>\n",
       "      <td>0.198194</td>\n",
       "      <td>-0.116156</td>\n",
       "      <td>0.011338</td>\n",
       "      <td>-0.133228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.119089</td>\n",
       "      <td>0.075075</td>\n",
       "      <td>1.400657</td>\n",
       "      <td>-0.792461</td>\n",
       "      <td>-0.304917</td>\n",
       "      <td>0.148512</td>\n",
       "      <td>0.428406</td>\n",
       "      <td>1.037877</td>\n",
       "      <td>0.446540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973830</td>\n",
       "      <td>-0.436764</td>\n",
       "      <td>0.745695</td>\n",
       "      <td>-0.057611</td>\n",
       "      <td>0.056279</td>\n",
       "      <td>-0.127634</td>\n",
       "      <td>-0.005717</td>\n",
       "      <td>0.302014</td>\n",
       "      <td>0.462430</td>\n",
       "      <td>0.046998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.217803</td>\n",
       "      <td>-0.220348</td>\n",
       "      <td>0.532312</td>\n",
       "      <td>-0.145882</td>\n",
       "      <td>-0.036074</td>\n",
       "      <td>0.122394</td>\n",
       "      <td>-0.028780</td>\n",
       "      <td>0.217096</td>\n",
       "      <td>0.266330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062220</td>\n",
       "      <td>0.167978</td>\n",
       "      <td>0.064264</td>\n",
       "      <td>-0.032044</td>\n",
       "      <td>-0.126803</td>\n",
       "      <td>0.170066</td>\n",
       "      <td>-0.218061</td>\n",
       "      <td>-0.247630</td>\n",
       "      <td>-0.101045</td>\n",
       "      <td>-0.194075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0           0 -0.274664 -0.197037  1.621728 -0.237445 -0.288944  0.843277   \n",
       "1           1 -0.009377 -0.493120  2.030924 -0.442848 -0.290093  0.380474   \n",
       "2           2  0.144012 -0.184814  0.698455 -0.249389 -0.030401 -0.010584   \n",
       "3           3  0.119089  0.075075  1.400657 -0.792461 -0.304917  0.148512   \n",
       "4           4  0.217803 -0.220348  0.532312 -0.145882 -0.036074  0.122394   \n",
       "\n",
       "          6         7         8    ...          190       191       192  \\\n",
       "0  0.617973  1.499649  0.842379    ...    -0.057703 -0.040805  0.639898   \n",
       "1  1.293419  1.768064  1.919836    ...     0.605748  0.176924  0.618500   \n",
       "2  0.131361  0.616810  0.120255    ...     0.412973  0.268411 -0.134549   \n",
       "3  0.428406  1.037877  0.446540    ...     0.973830 -0.436764  0.745695   \n",
       "4 -0.028780  0.217096  0.266330    ...     0.062220  0.167978  0.064264   \n",
       "\n",
       "        193       194       195       196       197       198       199  \n",
       "0  0.234292  0.024925  0.118979 -0.794148 -0.566620  0.138416 -0.548374  \n",
       "1 -0.351151 -0.435653  0.027308 -0.069966  0.080411  1.921239 -0.199846  \n",
       "2 -0.435909 -0.054599  0.377649  0.198194 -0.116156  0.011338 -0.133228  \n",
       "3 -0.057611  0.056279 -0.127634 -0.005717  0.302014  0.462430  0.046998  \n",
       "4 -0.032044 -0.126803  0.170066 -0.218061 -0.247630 -0.101045 -0.194075  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_vec_df = pd.read_csv('doc_vec/questions_vec_weighted.csv')\n",
    "questions_vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions_vec_df = questions_vec_df.drop(questions_vec_df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108954, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_vec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cos similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X =questions_vec_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster \n",
    "chunk_size = 500 \n",
    "matrix_len = X.shape[0] # Not sparse numpy.ndarray\n",
    "\n",
    "def similarity_cosine_by_chunk(start, end):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return cosine_similarity(X=X[start:end], Y=X) # scikit-learn function\n",
    "cosine_similarity_chunk = []\n",
    "for chunk_start in range(0, 500, chunk_size):\n",
    "     cosine_similarity_chunk.append(similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size))\n",
    "    # Handle cosine_similarity_chunk  ( Write it to file_timestamp and close the file )\n",
    "    # Do not open the same file again or you may end up with out of memory after few chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = cosine_similarity_chunk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 108954)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*************************question0**************************\n",
      "How should I elicit prior distributions from experts when fitting a Bayesian model?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "How should I elicit prior distributions from experts when fitting a Bayesian model?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I have a question regarding model comparison using Bayes factors. In many cases, statisticians are interested on using a Bayesian approach with improper priors (for example some Jeffreys priors and reference priors). \n",
      "\n",
      "My question is, in those cases where the posterior distribution of the model parameters is well-defined, is it valid to compare models using Bayes factors under the use of improper priors?\n",
      "\n",
      "As a simple example consider comparing a Normal model vs. a Logistic model with Jeffreys priors.\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "Bayesian methods have belief over the prior distribution over variables. They update this belief with evidence.\n",
      "\n",
      "We know that parametric statistical methods are considered to be methods which have an assumption on the distribution of the variables.\n",
      "\n",
      "My question: Since, Bayesian methods use a prior distribution over the variables, are all bayesian methods also parameteric?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "In Bayesian analysis we use the Normal-Inverse Wishart distribution for the parameters of multivariate models these prior distributions have some  hyperparameters. So how do we find the values of these hyperparameters to get  Bayes estimates of the parameters? One way to specify these values as diffuse but for an informative prior how do we find hyperparameters of the prior densities?\n",
      "\n",
      "\n",
      "\n",
      "*************************question1**************************\n",
      "In many different statistical methods there is an \"assumption of normality\".  What is \"normality\" and how do I know if there is normality?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "In many different statistical methods there is an \"assumption of normality\".  What is \"normality\" and how do I know if there is normality?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I am trying to do a one way ANOVA and having some difficulty proceeding it. Among all the assumptions, I am stuck with these two: normality and equal variance. My questions are, \n",
      "\n",
      "\n",
      "  \n",
      "  My independent variable has 4 categories. The normality assumption is satisfied for two of the 4 categories. I read that ANOVA is quite\n",
      "  robust so a small violation of normality is not a big deal. How can I\n",
      "  decide that the violation is acceptable?\n",
      "  If homogeneity of variances is violated, it is suggested to do a Welch's F test. I assume I can only do that if the normality\n",
      "  assumption is satisfied. Is that correct?\n",
      "  \n",
      "\n",
      "\n",
      "Looking forward to any suggestions! .\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "This issue seems to rear its ugly head all the time, and I'm trying to decapitate it for my own understanding of statistics (and sanity!). \n",
      "\n",
      "The assumptions of general linear models (t-test, ANOVA, regression etc.) include the \"assumption of normality\", but I have found this is rarely described clearly. \n",
      "\n",
      "I often come across statistics textbooks / manuals / etc. simply stating that the \"assumption of normality\" applies to each group (i.e., categorical X variables), and we should we examining departures from normality for each group. \n",
      "\n",
      "Questions:  \n",
      "\n",
      "\n",
      "does the assumption refer to the values of Y or the residuals of Y? \n",
      "for a particular group, is it possible to have a strongly non-normal distribution of Y values (e.g., skewed) BUT an approximately (or at least more normal) distribution of residuals of Y? \n",
      "\n",
      "Other sources describe that the assumption pertains to the residuals of the model (in cases where there are groups, e.g. t-tests / ANOVA), and we should be examining departures of normality of these residuals (i.e., only one Q-Q plot/test to run). \n",
      "does normality of residuals for the model imply normality of residuals for the groups? In other words, should we just examine the model residuals (contrary to instructions in many texts)?\n",
      "\n",
      "To put this in a context, consider this hypothetical example:\n",
      "\n",
      "\n",
      "I want to compare tree height (Y) between two populations (X).  \n",
      "In one population the distribution of Y is strongly right-skewed (i.e.,\n",
      "most trees short, very few tall), while the other is virtually normal \n",
      "Height is higher overall in the normally distributed population (suggesting there may be a 'real' difference).\n",
      "Transformation of the data does not substantially improve the distribution of the first    population.\n",
      "\n",
      "Firstly, is it valid to compare the groups given the radically different height distributions?\n",
      "How do I approach the \"assumption of normality\" here? Recall height in one population is not normally distributed. Do I examine residuals for both populations separately OR residuals for the model (t-test)?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please refer to questions by number in replies, experience has shown me people get lost or sidetracked easily (especially me!). Keep in mind I am not a statistician; though I have a reasonably conceptual (i.e., not technical!) understanding of statistics.\n",
      "\n",
      "P.S., I have searched the archives and read the following threads which have not cemented my understanding:\n",
      "\n",
      "\n",
      "ANOVA assumption normality/normal distribution of residuals\n",
      "Normality of residuals vs sample data; what about t-tests?\n",
      "Is normality testing 'essentially useless'?\n",
      "Testing normality\n",
      "Assessing normality of distribution\n",
      "What tests do I use to confirm that residuals are normally distributed?\n",
      "What to do when Kolmogorov-Smirnov test is significant for residuals of parametric test but skewness and kurtosis look normal?\n",
      "\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I have read enough threads on QQplots here to understand that a QQplot can be more informative than other normality tests. However, I am inexperienced with interpreting QQplots. I googled a lot; I found a lot of graphs of non-normal QQplots, but no clear rules on how to interpret them, other than what it seems to be comparison with know distributions plus \"gut feeling\". \n",
      "\n",
      "I would like to know if you have (or you know of) any rule of thumb to help you decide for non-normality.\n",
      "\n",
      "This question came up when I saw these two graphs:\n",
      "\n",
      "\n",
      "\n",
      "I understand that the decision of non-normality depends on the data and what I want to do with them; however, my question is: generally, when do the observed departures from the straight line constitute enough evidence to make unreasonable the approximation of normality?\n",
      "\n",
      "For what it's worth, the Shapiro-Wilk test failed to reject the hypothesis of non-normality in both cases.\n",
      "\n",
      "\n",
      "\n",
      "*************************question2**************************\n",
      "What are some valuable Statistical Analysis open source projects available right now?\n",
      "\n",
      "Edit: as pointed out by Sharpie, valuable could mean helping you get things done faster or more cheaply.\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "What are some valuable Statistical Analysis open source projects available right now?\n",
      "\n",
      "Edit: as pointed out by Sharpie, valuable could mean helping you get things done faster or more cheaply.\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "This question illustrates the difficulty of a person mastering statistics and probability on their own, in the face of weakly developed resources like Wikipedia.\n",
      "\n",
      "It occurred to me that consulting statisticians, and there are a few here, may routinely face the challenge of explaining certain concepts and methods to a client.  This is the flip side of the pedagogical coin.  When one has mastered the concept, it may make sense to conduct a particular avenue of analyses, but one's references may either be inappropriate or difficult to share with a client.  So, are there common resources that consulting statisticians like to suggest to their clients?  (See update #1 regarding more advanced or specialized topics.)\n",
      "\n",
      "I can think of a few books that may be useful, but I suspect that a lot of clients will go about searching the web, as Developer did, and will come across rather inane material on Wikipedia.  In my answer to Developer, I suggested the NIST Handbook as one such reference that could be used.  What else?\n",
      "\n",
      "\n",
      "\n",
      "Update 1: As Peter Flom has pointed out, for more advanced material or narrower pursuits, it may not be easy to offer a single point of reference.  This is correct and I should have worded the question differently for those cases.  In such cases, how do consultants find and share accessible references?  I believe that many consultants will take the time to write something new in order to explain things to their client, but those aren't references that are found and shared.\n",
      "\n",
      "Some ideas:\n",
      "\n",
      "\n",
      "Tutorials written by the consultant or others\n",
      "Case studies or analyses from projects that demonstrate the same concepts\n",
      "Excerpts of books (as I'd suggested in my answer to Developer), which describe the concept\n",
      "\n",
      "\n",
      "What else might be a source or how else do you actually go about finding such references?  I realize this is an open ended question, but my answer to Developer shows some of the ways I'd approach this problem.  I don't mean to ask of all the ways that one could address this, but in one's own experience, how have you typically provided such explanatory resources?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "The options I've seen and am looking at as possible in some combination here:\n",
      "\n",
      "\n",
      "R\n",
      "Java\n",
      "Python\n",
      "Matlab\n",
      "Visual Basic + Excel\n",
      "VBA\n",
      "SAS\n",
      "C/C++\n",
      "C#\n",
      "\n",
      "\n",
      "What I have experience in:\n",
      "Java\n",
      "JavaScript\n",
      "HTML/CSS\n",
      "(Also only have completed half a year in a high school college-level statistics course if you are looking for background information there given my detail-lacking explanation of my desires later)\n",
      "\n",
      "What I want to do:\n",
      "Learn a programming language that will:\n",
      "\n",
      "-best suit a possible future study of actuarial science.\n",
      "\n",
      "-assist me in making efficient my hobby of studying and analyzing sports statistics. This would entail being able to extract mass data from sites such as pro-football-reference.com or basketball-reference.com, and then perform analysis as needed that the written code can assist with in simplifying and automating much of the process in my tasks. \n",
      "\n",
      "(I am less certain of what it is I want in that latter regard as there is a lot in the field of statistics I have yet to learn, and thus there's a lot I wouldn't know about that I would want to look for if I did.)\n",
      "\n",
      "So, in general, I'm looking for a single program or combination of programs that I can write code in tailored to assisting me with specific needs from website data extraction to the actual data analysis. Some of this may already be written out via the programming language's libraries offered (depending on what it is) and so I'd like clarification there.\n",
      "\n",
      "Please let me know if there's a better place to ask this question, I understand this is a statistics forum and not a programming forum, some of you may only have deep knowledge in some of these things. I would post it there too, but I don't feel like it would be on topic enough to be received positively.\n",
      "\n",
      "If you feel there's something I don't quite have a strong grasp on here that is making my pursuit of what I want more unclear than it needs to be, please let me know what those things are too so I can set myself right.\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I had a plan of learning R in the near future. Reading another question I found out about Clojure. Now I don't know what to do.\n",
      "\n",
      "I think a big advantage of R for me is that some people in Economics use it, including one of my supervisors (though the other said: stay away from R!). One advantage of Clojure is that it is Lisp-based, and as I have started learning Emacs and I am keen on writing my own customisations, it would be helpful (yeah, I know Clojure and Elisp are different dialects of Lisp, but they are both Lisp and thus similar I would imagine).\n",
      "\n",
      "I can't ask which one is better, because I know this is very personal, but could someone give me the advantages (or advantages) of Clojure x R, especially in practical terms? For example, which one should be easier to learn, which one is more flexible or more powerful, which one has more libraries, more support, more users, etc?\n",
      "\n",
      "My intended use: The bulk of my estimation should be done using Matlab, so I am not looking for anything too deep in terms of statistical analysis, but rather a software to substitute Excel for the initial data manipulation and visualisation, summary statistics and charting, but also some basic statistical analysis or the initial attempts at my estimation.\n",
      "\n",
      "\n",
      "\n",
      "*************************question3**************************\n",
      "I have two groups of data.  Each with a different distribution of multiple variables.  I'm trying to determine if these two groups' distributions are different in a statistically significant way.  I have the data in both raw form and binned up in easier to deal with discrete categories with frequency counts in each.  \n",
      "\n",
      "What tests/procedures/methods should I use to determine whether or not these two groups are significantly different and how do I do that in SAS or R (or Orange)?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I have two groups of data.  Each with a different distribution of multiple variables.  I'm trying to determine if these two groups' distributions are different in a statistically significant way.  I have the data in both raw form and binned up in easier to deal with discrete categories with frequency counts in each.  \n",
      "\n",
      "What tests/procedures/methods should I use to determine whether or not these two groups are significantly different and how do I do that in SAS or R (or Orange)?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I need help to find a way to test two different histograms.\n",
      "I have two different population, males and females and two essays.\n",
      "From this essay I extracted the words used and the frequencies relative the two groups.\n",
      "I plotted the words frequencies (alphabetic order) in two histograms and now I need to perform a statistical test to affirm if the two distributions are statistically significantly different or not.\n",
      "Does anyone have any idea which test to use???\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I have a group of 17 subjects divided into 2 different groups (2 different types of one disease). I would like to perform a multiple linear regression analysis in R across these groups with 7 covariates in total. Is there a way to perform this in R that after I get the results I could associate them with a certain group? Basically, I'm interested in evaluate if the differences in the values of PCC_MC, SMA_MC and IPS_MC are correlated with one of the several covariates. But if I have a significant result, for example, cov1 how can I identify which of the two groups are associated to that result?\n",
      "\n",
      "\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I have following data and want to know is there really significant difference between those five objects: I have 5 objects and 3 different tests and their results like this. So this is very few rows and those tests are indipendent and different and if it is not possible to compare them as a group I would compare them each column at a time. So is there any method to find does they differ significantly and which of them (1,2,3,4,5) differs? How do you suggest to find that, maybe Kruskall-Wallis or something. I would do test in R.\n",
      "\n",
      "1)17--18--25\n",
      "2)18--15--23\n",
      "3)14--13--19\n",
      "4)22--13--18\n",
      "5)17--26--37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*************************question4**************************\n",
      "Last year, I read a blog post from Brendan O'Connor entitled \"Statistics vs. Machine Learning, fight!\" that discussed some of the differences between the two fields.  Andrew Gelman responded favorably to this:\n",
      "\n",
      "Simon Blomberg: \n",
      "\n",
      "\n",
      "  From R's fortunes\n",
      "  package: To paraphrase provocatively,\n",
      "  'machine learning is statistics minus\n",
      "  any checking of models and\n",
      "  assumptions'.\n",
      "  -- Brian D. Ripley (about the difference between machine learning\n",
      "  and statistics) useR! 2004, Vienna\n",
      "  (May 2004) :-) Season's Greetings!\n",
      "\n",
      "\n",
      "Andrew Gelman:\n",
      "\n",
      "\n",
      "  In that case, maybe we should get rid\n",
      "  of checking of models and assumptions\n",
      "  more often. Then maybe we'd be able to\n",
      "  solve some of the problems that the\n",
      "  machine learning people can solve but\n",
      "  we can't!\n",
      "\n",
      "\n",
      "There was also the \"Statistical Modeling: The Two Cultures\" paper by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the predictive accuracy of models.\n",
      "\n",
      "Has the statistics field changed over the last decade in response to these critiques?  Do the two cultures still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "Last year, I read a blog post from Brendan O'Connor entitled \"Statistics vs. Machine Learning, fight!\" that discussed some of the differences between the two fields.  Andrew Gelman responded favorably to this:\n",
      "\n",
      "Simon Blomberg: \n",
      "\n",
      "\n",
      "  From R's fortunes\n",
      "  package: To paraphrase provocatively,\n",
      "  'machine learning is statistics minus\n",
      "  any checking of models and\n",
      "  assumptions'.\n",
      "  -- Brian D. Ripley (about the difference between machine learning\n",
      "  and statistics) useR! 2004, Vienna\n",
      "  (May 2004) :-) Season's Greetings!\n",
      "\n",
      "\n",
      "Andrew Gelman:\n",
      "\n",
      "\n",
      "  In that case, maybe we should get rid\n",
      "  of checking of models and assumptions\n",
      "  more often. Then maybe we'd be able to\n",
      "  solve some of the problems that the\n",
      "  machine learning people can solve but\n",
      "  we can't!\n",
      "\n",
      "\n",
      "There was also the \"Statistical Modeling: The Two Cultures\" paper by Leo Breiman in 2001 which argued that statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the predictive accuracy of models.\n",
      "\n",
      "Has the statistics field changed over the last decade in response to these critiques?  Do the two cultures still exist or has statistics grown to embrace machine learning techniques such as neural networks and support vector machines?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I've been wondering what I should read next and thus came across Michael Jordan's and Bradford Cross's suggestions. Those unfortunately are huge lists and each text is a huge commitment of time. So I'd like some opinions on what I should prioritize to read next.\n",
      "\n",
      "\n",
      "I will be picking two texts, one more practical and one more\n",
      "theoretical. Alternating between the two daily to mix things up.\n",
      "Otherwise, I'd only focus on practical stuff since my current job is\n",
      "in applications in industry. \n",
      "My related course background: I took an\n",
      "undergraduate course in statistics (using Part I and II of\n",
      "Wasserman's All of Statistics), a graduate pattern recognition course\n",
      "(using Duda et al.'s Pattern Classification), and a graduate machine\n",
      "learning course (no text, selection of papers).\n",
      "Beyond that I did\n",
      "read Casella &amp; Berger's Statistical Inference (the one and only book\n",
      "in Jordan's list I can check off!) and Bishop's Pattern Recognition\n",
      "and Machine Learning. Using Bishop as the reference, my knowledge is\n",
      "mostly in the first half of the book (linear, basic ANNs, SVM) and\n",
      "the second half of the book I'm less familiar with (graphical\n",
      "models). I recall trying to read Robert's Bayesian Choice some years\n",
      "ago and it was beyond my level for the most part. Not sure if I could\n",
      "read it now, since I've sold it before my last move. \n",
      "My current\n",
      "reading is reviewing Wasserman's All of Statistics (Part I and II)\n",
      "and I'm about a third through Shalev-Shwartz &amp; Ben-David's\n",
      "Understanding Machine Learning. I'm enjoying the latter quite a bit\n",
      "since I glossed over the learning theory in school and so the\n",
      "perspective in this text is refreshing. \n",
      "For my next \"practical\" text,\n",
      "I'm kind of settling down on Aggarwal's Data Mining. Covers a wide\n",
      "range of topics I'm not familiar with (association, outlier, network\n",
      "analysis, etc.) that I need would be good to have in my toolbox\n",
      "(maybe lead to something useful if I play with the methods with data\n",
      "from work). But I'm welcome to other suggestions. \n",
      "For my next\n",
      "\"theory\" text, I'm less certain. Should I dive into one of Jordan's\n",
      "suggested statistics books (probably Lehmann's Elements of Large\n",
      "Sample Theory is the reasonable one for my level) or should I start\n",
      "his sequence of probability texts (starting with Grimmett &amp;\n",
      "Stirzaker's Probability and Random Processes) to build up to measure\n",
      "theoretic probability because that will make the statistics texts\n",
      "more understandable? Or should I follow Cross's suggestions and maybe\n",
      "read some linear models text (e.g. Faraway's Linear Models with R),\n",
      "robust statistics (e.g. Huber &amp; Ronchetti's Robust Statistics), or\n",
      "Bayesian statistics (e.g. Gelman et al.'s Bayesian Data Analysis)?\n",
      "Sooo many choices. \n",
      "\n",
      "\n",
      "Maybe in 10 years I could get through Jordan's and\n",
      "   Cross's lists haha\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "Quick question I guess, but is there a perceivable difference between the terms Statistical Learning and Machine Learning, or is it simply area jargon? I gather the computer scientists like to refer to machine learning while statisticians might refer to statistical learning (no less influenced by the famous book).\n",
      "\n",
      "\n",
      "\n",
      "This motif repeats across other questions in the site as well, with many questions like \"what's the difference between machine learning and something else\", but I'd like to have this one specifically answered (with some references if possible) to solve the possible merge of the terms.\n",
      "\n",
      "This question is rooted in a recent question on meta regarding The *learning tags, where SL and ML were agreed to be made synonyms (refer to my answer for some background and what I have gathered on the subject so far).\n",
      "\n",
      "\n",
      "\n",
      "Quoting my answer:\n",
      "\n",
      "\n",
      "  Perphaps the difference is simply cultural, like many discussions in\n",
      "  the main site pointed. Consider Stanford, where two courses are\n",
      "  taught: Stats 315a/315b - Statistical Learning and CS\n",
      "  229 - Machine Learning. Apart from being named different and being\n",
      "  in different concentrations areas, they also attract different\n",
      "  students.\n",
      "  \n",
      "  Tibshirani even shares his views in his page comparing both\n",
      "  courses and then both terms:\n",
      "  \n",
      "  \n",
      "    Machine learning research focusses more  on low noise situations, eg \n",
      "    engineering applications like robotics and physical sciences\n",
      "    \n",
      "    Statistical learning focusses  more on high noise, observational data\n",
      "    like medicine and genomics, and problems where interpretation of the\n",
      "    fitted model is important\n",
      "    \n",
      "    But more and more overlap in application areas!\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning (Vol. 6). New York: Springer.\n",
      "\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "My question comes from the following fact. I have been reading posts, blogs, lectures as well as books on machine learning. My impression is that machine learning practitioners seem to be indifferent to many things that statisticians/econometrics care about. In particular, machine learning practitioners emphasize prediction accuracy over inference.\n",
      "\n",
      "One such example occurred when I was taking Andrew Ng's Machine Learning on Coursera. When he discusses Simple Linear Model, he mentioned nothing about the BLUE property of the estimators, or how heteroskedasticity would \"invalidate\" confidence interval. Instead, he focuses on gradient descent implementation and the concept of cross validation/ROC curve. These topics were not covered in my econometrics/statistics classes.\n",
      "\n",
      "Another example occurred when I participated in Kaggle competitions. I was reading other people's code and thoughts. A large part of the participants just throw everything into SVM/random forest/XGBoost.\n",
      "\n",
      "Yet another example is about stepwise model selection. This technique is widely used, at least online and on Kaggle. Many classical machine learning textbooks also cover it, such as Introduction to Statistical Learning. However, according to this answer (which is quite convincing), stepwise model selection faces lots of problem especially when it comes down to \"discovering the true model\". It seems to be that there are only two possibilities: either machine learning practitioners do not know the problem with stepwise, or they do but they do not care.\n",
      "\n",
      "So here are my questions:\n",
      "\n",
      "\n",
      "Is it true that (in general) machine learning practitioners focus on prediction and thus do not care about a lot of things which statisticians/economists care about?\n",
      "If it is true, then what is the reason behind it? Is it because inference is more difficult in some sense?\n",
      "There are tons of materials on machine learning (or prediction) online. If I am interested in learning about doing inference, however, what are some resources online that I can consult?\n",
      "\n",
      "\n",
      "Update: I just realized that the word \"inference\" could potentially mean lots of stuff. What I meant by \"inference\" refers to questions such as \n",
      "\n",
      "\n",
      "Did $X$ cause $Y$ or $Y$ caused $X$? Or more generally, what's the causal relations among $X_1,X_2,\\cdots,X_n$?\n",
      "Since \"all models are wrong\", how \"wrong\" is our model from the true model?\n",
      "Given the information of a sample, what can we say about the population and how confident can we say that?\n",
      "\n",
      "\n",
      "Due to my very limited statistics knowledge, I am not even sure whether those questions fall into the realm of statistics or not. But those are the types of questions which machine learning practitioners do not seem to care about. Perhaps statisticians do not care neither? I don't know.\n",
      "\n",
      "\n",
      "\n",
      "*************************question5**************************\n",
      "I've been working on a new method for analyzing and parsing datasets to identify and isolate subgroups of a population without foreknowledge of any subgroup's characteristics.  While the method works well enough with artificial data samples (i.e. datasets created specifically for the purpose of identifying and segregating subsets of the population), I'd like to try testing it with live data.\n",
      "\n",
      "What I'm looking for is a freely available (i.e. non-confidential, non-proprietary) data source.  Preferably one containing bimodal or multimodal distributions or being obviously comprised of multiple subsets that cannot be easily pulled apart via traditional means.  Where would I go to find such information?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I've been working on a new method for analyzing and parsing datasets to identify and isolate subgroups of a population without foreknowledge of any subgroup's characteristics.  While the method works well enough with artificial data samples (i.e. datasets created specifically for the purpose of identifying and segregating subsets of the population), I'd like to try testing it with live data.\n",
      "\n",
      "What I'm looking for is a freely available (i.e. non-confidential, non-proprietary) data source.  Preferably one containing bimodal or multimodal distributions or being obviously comprised of multiple subsets that cannot be easily pulled apart via traditional means.  Where would I go to find such information?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "A growing amount of cluster algorithms have been developed for using large datasets such as CURE or BIRCH or filtering methods for common k-means. What is the advantage of using such an algorithm instead of talking a randomized sample from a large dataset for training purposes and then predict the cluster membership of the remaining data? \n",
      "\n",
      "One could assume that cluster-solutions differ depending on the sample but if the sample is large enough to be representative for the whole population this should not be the case. I could imagine that for very small groups and outliers this might be still the case but it could be at least tested if drawing several randomized samples. Such an approach is implemented for example in the CLARA algorithm. \n",
      "\n",
      "So why use potentially less efficient algorithms for the whole dataset if well established methods could work on a sample?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "Suppose you've got two high-dimensional datasets with no cases in common and most, but not all, variables in common. To answer your primary research question, you outer-join them, cluster them, and obtain a list of variables that best distinguish each cluster from the rest. For example, suppose the observations are biological cells and variables are gene expression levels; you've identified subpopulations of cells and found marker genes for them. Your two datasets are biological replicates -- cells from two different sources. \n",
      "\n",
      "You now want to claim that the two datasets give similar clusterings (or not) based on the data. You can cluster each of them separately, but what's a reasonable descriptive statistic or formal test to use in this scenario? The obstacle is that unlike in \n",
      "\n",
      "Looking for a metric to compare clustering solutions to a reference clustering for a large dataset\n",
      "\n",
      ", our two clusterings are on separate datasets.\n",
      "\n",
      "Possibilities: \n",
      "\n",
      "\n",
      "If you're using a probabilistic clustering method, then use a likelihood ratio test or an information criterion to decide whether separate models for each dataset are warranted. Unfortunately, I'm not using one.\n",
      "Duality: the cases are not in common, but the variables (mostly) are. Pretend the sets of cluster markers are your clusterings and use tactics like these. This makes me nervous; can someone either justify it or explain why it's unreliable?\n",
      "\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "As context: When working with a very large data set, I am sometimes asked if we can create a synthetic data set where we \"know\" the relationship between predictors and the response variable, or relationships among predictors.  \n",
      "\n",
      "Over the years, I seem to encounter either one-off synthetic data sets, which look like they were cooked up in an ad hoc manner, or more structured data sets that seem especially favorable for the researcher's proposed modeling method.\n",
      "\n",
      "I believe that I'm over looking standard methods for creating synthetic data sets.  Although bootstrap resampling is one common method for creating synthetic data set, it doesn't satisfy the condition that we know the structure a priori.  Moreover, exchanging bootstrap samples with others essentially requires the exchange of data, rather than of a data generating method.\n",
      "\n",
      "If we can fit a parametric distribution to the data, or find a sufficiently close parametrized model, then this is one example where we can generate synthetic data sets.\n",
      "\n",
      "What other methods exist?  I am especially interested in high dimensional data, sparse data, and time series data.  For high dimensional data, I'd look for methods that can generate structures (e.g. covariance structure, linear models, trees, etc.) of interest.  For time series data, from distributions over FFTs, AR models, or various other filtering or forecasting models seems like a start.  For sparse data, reproducing a sparsity pattern seems useful.\n",
      "\n",
      "I believe these only scratch the surface - these are heuristic, not formal practices.  Are there references or resources for generating synthetic data that should be known to \n",
      "practitioners?\n",
      "\n",
      "\n",
      "\n",
      "Note 1: I realize that this question addresses the literature on how one may generate data like a particular time series model.  The distinction here is on practices, especially in order to indicate a known structure (my question), versus similarity / fidelity to an existing data set.  It's not necessary in my case to have similarity, as much as known structure, though similarity is greatly preferred to dissimilarity.  An exotic synthetic data set for which a model shows promise is less preferred than a realistic simulation.\n",
      "\n",
      "Note 2: The Wikipedia entry for synthetic data points out that luminaries such as Rubin and Fienberg have addressed this issue, though I have found no references on best practices.  It would be interesting to know what would pass muster with, say, the Annals of Applied Statistics (or the AOS), or in review works in these or other journals.  In simple and whimsical terms, one may ask where does the threshold between \"(acceptably) cooked up\" and \"too cooked up\" exist?\n",
      "\n",
      "Note 3: Although it doesn't affect the question, the usage scenario is in modeling of vary large, high dimensional data sets, where the research agenda is to learn (both by human and machine ;-)) the structure of the data.  Unlike univariate, bivariate, and other low dimensional scenarios, the structure isn't readily inferred.  As we step toward a better understanding of the structure, being able to generate data sets with similar properties is of interest in order to see how a modeling method interacts with the data (e.g. to examine parameter stability).  Nonetheless, older guides on low dimensional synthetic data can be a starting point that may be extended or adapted for higher dimensional data sets.\n",
      "\n",
      "\n",
      "\n",
      "*************************question6**************************\n",
      "Sorry, but the emptyness was a bit overwhelming. And this has been stuck in my head since it got asked at Area51!\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "Sorry, but the emptyness was a bit overwhelming. And this has been stuck in my head since it got asked at Area51!\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I have tried asking this on another forum, but after 4 months there are no posted answers, so I am asking it here:\n",
      "\n",
      "What is the current state of the art approach for determining the pose of a person (including close poses like a head shot, and hand shot, etc...), and for segmenting the person from the background?\n",
      "\n",
      "The setting here is single still images.\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "So yesterday was my 50th birthday, and it just so happened that my first grandchild was also born yesterday.  Which got me to thinking - what are the odds that this would happen?  Specifically on the BIG 5-0.  I've known others who have shared a birthday with a grandchild, but haven't heard of this.  I figure it is somewhat rare.  Just curious.\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "\n",
      "  Possible Duplicate:\n",
      "  What are the chances my wife has lupus?  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A day or two ago a man posted a question on math.stackexchenge about whether his wife likely has lupus, given the diagnosis. First,  https://math.stackexchange.com/questions/256079/what-are-the-chances-my-wife-has-lupus  \n",
      "\n",
      "It appears I will be able to post a screenshot of the question and of my answer, which i deleted after the OP expressed a lack of satisfaction.\n",
      "\n",
      "However, the screenshots take up quite a lot of room, so I will try to make a question here. On the odd chance that this man ever looks at his MSE question again, is there anything else of value he could be told? Note that there are two answers after mine, explaining rather better than i did that he has not given enough information.  \n",
      "\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "\n",
      "\n",
      "\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-= \n",
      "\n",
      "\n",
      "\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*************************question7**************************\n",
      "Many studies in the social sciences use Likert scales.  When is it appropriate to use Likert data as ordinal and when is it appropriate to use it as interval data?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "Many studies in the social sciences use Likert scales.  When is it appropriate to use Likert data as ordinal and when is it appropriate to use it as interval data?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "Can I use Likert scale data to do parametric statistical procedures such as linear regression and ANOVA? I've read that you can use Likert data in parametric analysis but in certain circumstances. Which are those? My independent variables are Likert items and my dependent variable is the mean score of a Likert scale. Also, I have 90 samples.\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "In educational research, some researchers say likert type scale is interval scale and some others say it is ordinal scale. which one is correct?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I am conducting a research based on Technology Acceptance Model, where variables are 5 point Likert scale. I need to test the hypothesis between these Likert scale variables (e.g. Perceived Useful and Attitude to use, Perceived Ease of Use and Attitude to use, Attitude to Use and Behavioral Intention etc.)\n",
      "\n",
      "Now my question is, what is the appropriate hypothesis testing for it (SPSS)?\n",
      "\n",
      "I have searched for Correlation analysis, t-test, but I am confused as the variables are Likert scale, discrete, and sample size about 100.\n",
      "\n",
      "\n",
      "\n",
      "*************************question8**************************\n",
      "Is there a good, modern treatment covering the various methods of multivariate interpolation, including which methodologies are typically best for particular types of problems? I'm interested in a solid statistical treatment including error estimates under various model assumptions.\n",
      "\n",
      "An example:\n",
      "\n",
      "Shepard's method\n",
      "\n",
      "Say we're sampling from a multivariate normal distribution with unknown parameters. What can we say about the standard error of the interpolated estimates?\n",
      "\n",
      "I was hoping for a pointer to a general survey addressing similar questions for the various types of multivariate interpolations in common use. \n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "Is there a good, modern treatment covering the various methods of multivariate interpolation, including which methodologies are typically best for particular types of problems? I'm interested in a solid statistical treatment including error estimates under various model assumptions.\n",
      "\n",
      "An example:\n",
      "\n",
      "Shepard's method\n",
      "\n",
      "Say we're sampling from a multivariate normal distribution with unknown parameters. What can we say about the standard error of the interpolated estimates?\n",
      "\n",
      "I was hoping for a pointer to a general survey addressing similar questions for the various types of multivariate interpolations in common use. \n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I am working on a small research project trying to estimate regression function nonparametrically when I have only one regressor. Basically, I am trying to estimate the regression function\n",
      "$$r(x)=E[Y∣X=x] $$ \n",
      "when I have i.i.d. pairs $(X_i,Y_i)$, $i=1,\\ldots,n$. \n",
      "\n",
      "I looked through the literature and I found that there are two main nonparametric techniques employed: \n",
      "\n",
      "\n",
      "using kernels\n",
      "series estimation using polynomials\n",
      "\n",
      "\n",
      "My questions are:\n",
      "\n",
      "\n",
      "Are there any any rules of thumb that I shall use when selecting one of these nonparametric procedures?\n",
      "Does either of the above two approaches have some well known statistical or approximation advantages over the other? \n",
      "\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I mean exact likelihood based estimation instead of these LS methods.\n",
      "\n",
      "There are more general nonlinear optimzation methods, but in terms of performance, are there any specific methods for this type of problems? thanks.\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I am looking for studies regarding the robustness of a multivariate Gaussian copula. Specifically I am wondering whether estimates of the dependence parameter in a multivariate Gaussian Copula (Sigma) are robust to violations of the underlying assumptions. Many approaches that rely on normal distribution assumptions tend to be robust when those assumptions are violated. I am wondering whether the same extends to the Gaussian copula.\n",
      "\n",
      "Let me give you some more details on the problem I am struggling with: I am using the approach proposed by Hoff 2007 to estimate the dependence structure between a set of demographic variables. The problem is that the sample I am using is biased in the sense that some demographic groups are over- and others under-represented (e.g., too many old people). I am wondering to which extent my estimate of the dependence structure is robust to mild forms of bias.\n",
      "\n",
      "Thanks very much for your help in advance.\n",
      "\n",
      "Michael\n",
      "\n",
      "Hoff, Peter D. (2007), \"Extending the Rank Likelihood for Semiparametric Copula Estimation,\" The annals of applied statistics, 1 (1), 265 - 83\n",
      "\n",
      "\n",
      "\n",
      "*************************question9**************************\n",
      "I have four competing models which I use to predict a binary outcome variable (say, employment status after graduating, 1 = employed, 0 = not-employed) for n subjects. A natural metric of model performance is hit rate which is the percentage of correct predictions for each one of the models. \n",
      "\n",
      "It seems to me that I cannot use ANOVA in this setting as the data violates the assumptions underlying ANOVA. Is there an equivalent procedure I could use instead of ANOVA in the above setting to test for the hypothesis that all four models are equally effective?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I have four competing models which I use to predict a binary outcome variable (say, employment status after graduating, 1 = employed, 0 = not-employed) for n subjects. A natural metric of model performance is hit rate which is the percentage of correct predictions for each one of the models. \n",
      "\n",
      "It seems to me that I cannot use ANOVA in this setting as the data violates the assumptions underlying ANOVA. Is there an equivalent procedure I could use instead of ANOVA in the above setting to test for the hypothesis that all four models are equally effective?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I have a dataset from an experiment in which a sample of participants repeatedly done a variation of visual search task (for each participant there is about 400 trials) in two conditions. The predictions for this experiments suggest a comparison between two non-linear models that have the same number of parameters and are non-nested, with one model hypothetically better at predicting results in one condition and the other model better at predicting the other condition. I can fit the models to the data (with MLE) and obtain BIC scores. Using the difference in BIC scores, I can show that the prediction is correct. \n",
      "\n",
      "Now, I want to show that this prediction also holds for analyses based on individual participants' data. I thought of two ways of doing this:\n",
      "\n",
      "1) Fit the model to each subject data, choose best model, make a 2x2 table \"best model\" x condition, use $\\chi^2$ test or a Fisher's test to see whether the distribution of best models differ between conditions.\n",
      "\n",
      "2) Fit the model to each subject data, obtain a measure of fit (BIC?) and use it as dependent variable with traditional tests (e.g., t-test or Wilcoxon test).\n",
      "\n",
      "While the first option makes sense to me, I'm having some doubts about the second one, because model fits measures are data-dependent. Accordingly, comparison between fits of different participants does not make sense. On the other hand, because two models are fitted to each condition within each subjects, the difference between the fits should be comparable between participants. So is it viable to use the second approach? Are there better alternatives?\n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "Currently I am working on my thesis and I’m struggling on how to test whether a certain model outperforms competing models. \n",
      "\n",
      "I have a scenario where 10 models are used to model some variable in 7 countries/samples. After this, the models are ranked based on their capability of modeling the variable with 1 being the highest/best score. Sometimes the model in a certain country did not fit the selection criteria and is not allowed to be ranked (it is too bad in modeling the variable), and therefore it does not have a ranking. \n",
      "\n",
      "This could lead for example to the following scenario:\n",
      "\n",
      "\n",
      "\n",
      "I want to test whether a model has a significant higher ranking than other models within these 7 countries. My first thought was to look at the average score of all models, but this is far from optimal. For example, the average of Model #5 equals 1.14 and Model #10 1.0. Based on this, one could falsely conclude that Model #10 is the best choice within this group of countries, however, this is most definitely not the case. Model #10 did not make the screening procedure in 6 countries as it was too bad to be even concluded in the ranking. On the other hand, Model #5 was correct in 7 countries and even number one 6 times. \n",
      "\n",
      "Question: Is there any statistical test that allows to compare the performance of these models when the ranking is of unequal size? Or should I perhaps include a penalty of e.g. 11 when the model did not make the selection procedure, which makes it that the average ranking of Model #10 is not high anymore?  \n",
      "\n",
      "-------------------------------related questions------------------------\n",
      "I'm looking to model a variable for two classes A and B where the goal is prediction. There are a lot of covariates, and the dependant variable isn't binary, it's a \"cost\", but this question equally applies to something like height/weight.\n",
      "\n",
      "It has been proposed to run separate models for each class. To me this implies that there is an implicit interaction between the classes and the (potentially different) model factors that end up being selected.\n",
      "\n",
      "Alternatively I could run one model, with Class as a covariate and force in interactions as they arise. I see the benefit of this as data/trends in both classes can enhance the predictability of the model, however this comes at the cost of searching for individual interactions across the other covariates should they exist.\n",
      "\n",
      "What is the best way I can test whether this \"implicit\" interaction is worth splitting the models at the model construction time. \n",
      "\n",
      "Would I need to construct models both ways then compare the output on a test dataset?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    most_similar = c[i].argsort()[-4:][::-1]\n",
    "    print()\n",
    "    print()\n",
    "    print(\"*************************question\" + str(i) +\"**************************\")\n",
    "    print(Q_df.iloc[i]['Body'])\n",
    "    for j in most_similar:\n",
    "        \n",
    "        \n",
    "        print(\"-------------------------------\"+'related questions'+\"------------------------\")\n",
    "        print(Q_df.iloc[j]['Body'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K means model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = questions_vec_weighted_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=100, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
