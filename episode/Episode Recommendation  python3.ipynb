{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import configparser\n",
    "import json\n",
    "import requests\n",
    "import xmltodict\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import gensim\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import smart_open\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve episode descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'feed.xml'\n",
    "url = 'http://dataskeptic.com/feed.rss'\n",
    "\n",
    "if not(os.path.isfile(fname)):\n",
    "    print('fetching')\n",
    "    r = requests.get(url)\n",
    "    f = open(fname, 'wb')\n",
    "    f.write(r.text.encode('utf-8'))\n",
    "    f.close()\n",
    "\n",
    "with open(fname) as fd:\n",
    "    xml = xmltodict.parse(fd.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bargaining is the process of two (or more) parties attempting to\\nagree on the price for a transaction. \\xa0Game theoretic\\napproaches attempt to find two strategies from which neither party\\nis motivated to deviate. \\xa0These strategies are said to be in\\nequilibrium with one another. \\xa0The equilibriums available in\\nbargaining depend on the the transaction mechanism and the\\ninformation of the parties. \\xa0Discounting (how long parties are\\nwilling to wait) has a significant effect in this process.\\n\\xa0This episode discusses some of the choices Kyle and Linh Da\\nmade in deciding what offer to make on a house.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = xml['rss']['channel']['item'][72]['description']\n",
    "a = BeautifulSoup(test,'lxml').text\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episodes = xml['rss']['channel']['item']\n",
    "descriptions = []\n",
    "descToTitle = {}\n",
    "descToLink = {}\n",
    "descToNum = {}\n",
    "l = len(episodes)\n",
    "for episode in episodes:\n",
    "    enclosure = episode['enclosure']\n",
    "    \n",
    "    desc = episode['description']\n",
    "    desc = desc.replace(u'\\xa0', u' ')\n",
    "    desc = desc.replace(u'\\n', u' ')\n",
    "    desc = desc.replace(u'\\xc2', u' ')\n",
    "\n",
    "    \n",
    "    desc = BeautifulSoup(desc, \"lxml\").text\n",
    "    descriptions.append(desc)\n",
    "    \n",
    "    descToTitle[desc] = episode['title']\n",
    "    descToLink[desc] = episode['link']\n",
    "    descToNum[desc] = l\n",
    "    l = l - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['title', 'pubDate', 'guid', 'link', 'itunes:image', 'description', 'content:encoded', 'enclosure', 'itunes:duration', 'itunes:explicit', 'itunes:keywords', 'itunes:subtitle', 'itunes:episodeType'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "[MINI] One Shot Learning\n",
      "https://dataskeptic.com/blog/episodes/2017/one-shot-learning\n",
      "177\n",
      "Recommender Systems Live from FARCON 2017\n",
      "https://dataskeptic.com/blog/episodes/2017/recommender-systems-live-from-farcon\n",
      "176\n",
      "[MINI] Long Short Term Memory\n",
      "https://dataskeptic.com/blog/episodes/2017/long-short-term-memory\n",
      "175\n",
      "Zillow Zestimate\n",
      "https://dataskeptic.com/blog/episodes/2017/zillow-zestimate\n",
      "174\n",
      "Cardiologist Level Arrhythmia Detection with CNNs\n",
      "https://dataskeptic.com/blog/episodes/2017/cardiologist-level-arrhythmia-detection-with-cnns\n",
      "173\n",
      "[MINI] Recurrent Neural Networks\n",
      "https://dataskeptic.com/blog/episodes/2017/recurrent-neural-networks\n",
      "172\n",
      "Project Common Voice\n",
      "https://dataskeptic.com/blog/episodes/2017/project-common-voice\n",
      "171\n",
      "[MINI] Bayesian Belief Networks\n",
      "http://dataskeptic.com/blog/episodes/2017/bayesian-belief-networks\n",
      "170\n",
      "pix2code\n",
      "https://dataskeptic.com/blog/episodes/2017/pix2code\n",
      "169\n",
      "[MINI] Conditional Independence\n",
      "http://dataskeptic.com/blog/episodes/2017/conditional-independence\n",
      "168\n",
      "Estimating Sheep Pain with Facial Recognition\n",
      "http://dataskeptic.com/blog/episodes/2017/estimating-sheep-pain-with-facial-recognition\n",
      "167\n",
      "CosmosDB\n",
      "https://dataskeptic.com/blog/episodes/2017/cosmos-db\n",
      "166\n",
      "[MINI] The Vanishing Gradient\n",
      "http://dataskeptic.com/blog/episodes/2017/the-vanishing-gradient\n",
      "165\n",
      "Doctor AI\n",
      "https://dataskeptic.com/blog/episodes/2017/doctor-ai\n",
      "164\n",
      "[MINI] Activation Functions\n",
      "https://dataskeptic.com/blog/episodes/2017/activation-functions\n",
      "163\n",
      "MS Build 2017\n",
      "https://dataskeptic.com/blog/episodes/2017/ms-build-2017\n",
      "162\n",
      "[MINI] Max-pooling\n",
      "http://dataskeptic.com/blog/episodes/2017/max-pooling\n",
      "161\n",
      "Unsupervised Depth Perception\n",
      "https://dataskeptic.com/blog/episodes/2017/unsupervised-depth-perception\n",
      "160\n",
      "[MINI] Convolutional Neural Networks\n",
      "https://dataskeptic.com/blog/episodes/2017/convolutional-neural-networks\n",
      "159\n",
      "Multi-Agent Diverse Generative Adversarial Networks\n",
      "http://dataskeptic.com/blog/episodes/2017/multi-agent-diverse-generative-adversarial-networks\n",
      "158\n",
      "[MINI] Generative Adversarial Networks\n",
      "https://dataskeptic.com/blog/episodes/2017/generative-adversarial-networks\n",
      "157\n",
      "Opinion Polls for Presidential Elections\n",
      "https://dataskeptic.com/blog/episodes/2017/polling\n",
      "156\n",
      "OpenHouse\n",
      "https://dataskeptic.com/blog/episodes/2017/openhouse\n",
      "155\n",
      "[MINI] GPU CPU\n",
      "https://dataskeptic.com/blog/episodes/2017/gpu-cpu\n",
      "154\n",
      "[MINI] Backpropagation\n",
      "http://dataskeptic.com/blog/episodes/2017/backpropagation\n",
      "153\n",
      "Data Science at Patreon\n",
      "http://dataskeptic.com/blog/episodes/2017/data-science-at-patreon\n",
      "152\n",
      "[MINI] Feed Forward Neural Networks\n",
      "http://dataskeptic.com/blog/episodes/2017/feed-forward-neural-networks\n",
      "151\n",
      "Reinventing Sponsored Search Auctions\n",
      "https://dataskeptic.com/blog/episodes/2017/reinventing-sponsored-search-auctions\n",
      "150\n",
      "[MINI] The Perceptron\n",
      "https://dataskeptic.com/blog/episodes/2017/the-perceptron\n",
      "149\n",
      "The Data Refuge Project\n",
      "https://dataskeptic.com/blog/episodes/2017/the-data-refuge-project\n",
      "148\n",
      "[MINI] Automated Feature Engineering\n",
      "https://dataskeptic.com/blog/episodes/2017/automated-feature-engineering\n",
      "147\n",
      "Big Data Tools and Trends\n",
      "https://dataskeptic.com/blog/episodes/2017/big-data-tools-and-trends\n",
      "146\n",
      "[MINI] Primer on Deep Learning\n",
      "https://dataskeptic.com/blog/episodes/2017/primer-on-deep-learning\n",
      "145\n",
      "Data Provenance and Reproducibility with Pachyderm\n",
      "http://dataskeptic.com/blog/episodes/2017/data-provenance-and-reproducibility-with-pachyderm\n",
      "144\n",
      "[MINI] Logistic Regression on Audio Data\n",
      "http://dataskeptic.com/blog/episodes/2017/logistic-regression-on-audio-data\n",
      "143\n",
      "Studying Competition and Gender Through Chess\n",
      "http://dataskeptic.com/blog/episodes/2017/studying-competition-and-gender-through-chess\n",
      "142\n",
      "[MINI] Dropout\n",
      "http://dataskeptic.com/blog/episodes/2017/dropout\n",
      "141\n",
      "The Police Data and the Data Driven Justice Initiatives\n",
      "http://dataskeptic.com/blog/episodes/2017/the-police-data-initiative-and-the-data-driven-justice-initiative\n",
      "140\n",
      "The Library Problem\n",
      "http://dataskeptic.com/blog/episodes/2016/the-library-problem\n",
      "139\n",
      "2016 Holiday Special\n",
      "http://dataskeptic.com/blog/episodes/2016/holiday-special\n",
      "138\n",
      "[MINI] Entropy\n",
      "http://dataskeptic.com/blog/episodes/2016/entropy\n",
      "137\n",
      "MS Connect Conference\n",
      "http://dataskeptic.com/blog/episodes/2016/ms-connect-conference\n",
      "136\n",
      "Causal Impact\n",
      "http://dataskeptic.com/blog/episodes/2016/causal-impact\n",
      "135\n",
      "[MINI] The Bootstrap\n",
      "http://dataskeptic.com/blog/episodes/2016/the-bootstrap\n",
      "134\n",
      "[MINI] Gini Coefficients\n",
      "http://dataskeptic.com/blog/episodes/2016/gini-coefficient\n",
      "133\n",
      "Unstructured Data for Finance\n",
      "http://dataskeptic.com/epnotes/unstructured-data-for-finance.php\n",
      "132\n",
      "[MINI] AdaBoost\n",
      "http://dataskeptic.com/epnotes/adaboost.php\n",
      "131\n",
      "Stealing Models from the Cloud\n",
      "http://dataskeptic.com/epnotes/stealing-models-from-the-cloud.php\n",
      "130\n",
      "[MINI] Calculating Feature Importance\n",
      "http://dataskeptic.com/epnotes/calculating-feature-importance.php\n",
      "129\n",
      "NYC Bike Share Rebalancing\n",
      "http://dataskeptic.com/epnotes/nyc-bikeshare-rebalancing.php\n",
      "128\n",
      "[MINI] Random Forest\n",
      "http://dataskeptic.com/epnotes/random-forest.php\n",
      "127\n",
      "Election Predictions\n",
      "http://dataskeptic.com/epnotes/election-predictions.php\n",
      "126\n",
      "[MINI] F1 Score\n",
      "http://dataskeptic.com/epnotes/f1-score.php\n",
      "125\n",
      "Urban Congestion\n",
      "http://dataskeptic.com/epnotes/urban-congestion.php\n",
      "124\n",
      "[MINI] Heteroskedasticity\n",
      "http://dataskeptic.com/epnotes/heteroskedasticity.php\n",
      "123\n",
      "Music21\n",
      "http://dataskeptic.com/epnotes/music21.php\n",
      "122\n",
      "[MINI] Paxos\n",
      "http://dataskeptic.com/epnotes/paxos.php\n",
      "121\n",
      "Trusting Machine Learning Models with LIME\n",
      "http://dataskeptic.com/epnotes/trusting-machine-learning-models-with-lime.php\n",
      "120\n",
      "[MINI] ANOVA\n",
      "http://dataskeptic.com/epnotes/anova.php\n",
      "119\n",
      "Machine Learning on Images with Noisy Human-centric Labels\n",
      "http://dataskeptic.com/epnotes/machine-learning-on-images-with-noisy-human-centric-labels.php\n",
      "118\n",
      "[MINI] Survival Analysis\n",
      "http://dataskeptic.com/epnotes/survival-analysis.php\n",
      "117\n",
      "Predictive Models on Random Data\n",
      "http://dataskeptic.com/epnotes/predictive-models-on-random-data.php\n",
      "116\n",
      "[MINI] Receiver Operating Characteristic (ROC) Curve\n",
      "http://dataskeptic.com/epnotes/roc.php\n",
      "115\n",
      "Multiple Comparisons and Conversion Optimization\n",
      "http://dataskeptic.com/epnotes/multiple-comparisons.php\n",
      "114\n",
      "[MINI] Leakage\n",
      "http://dataskeptic.com/epnotes/leakage.php\n",
      "113\n",
      "Predictive Policing\n",
      "http://dataskeptic.com/epnotes/predictive-policing.php\n",
      "112\n",
      "[MINI] The CAP Theorem\n",
      "http://dataskeptic.com/epnotes/cap-theorem.php\n",
      "111\n",
      "Detecting Terrorists with Facial Recognition?\n",
      "http://dataskeptic.com/epnotes/detecting-terrorists-with-facial-recognition.php\n",
      "110\n",
      "[MINI] Goodhart's Law\n",
      "http://dataskeptic.com/epnotes/goodharts-law.php\n",
      "109\n",
      "Data Science at eHarmony\n",
      "http://dataskeptic.com/epnotes/data-science-at-eharmony.php\n",
      "108\n",
      "[MINI] Stationarity and Differencing\n",
      "http://dataskeptic.com/epnotes/stationarity-and-differencing.php\n",
      "107\n",
      "Feather\n",
      "http://dataskeptic.com/blog/episodes/2016/feather\n",
      "106\n",
      "[MINI] Bargaining\n",
      "http://dataskeptic.com/epnotes/bargaining.php\n",
      "105\n",
      "deepjazz\n",
      "http://dataskeptic.com/epnotes/deepjazz.php\n",
      "104\n",
      "[MINI] Auto-correlative functions and correlograms\n",
      "http://dataskeptic.com/epnotes/acf-correlograms.php\n",
      "103\n",
      "Early Identification of Violent Criminal Gang Members\n",
      "http://dataskeptic.com/epnotes/early-identification-of-violent-criminal-gang-members.php\n",
      "102\n",
      "[MINI] Fractional Factorial Design\n",
      "http://dataskeptic.com/epnotes/fractional-factorial-design.php\n",
      "101\n",
      "Machine Learning Done Wrong\n",
      "http://dataskeptic.com/epnotes/machine-learning-done-wrong.php\n",
      "100\n",
      "Potholes\n",
      "http://dataskeptic.com/epnotes/potholes.php\n",
      "99\n",
      "[MINI] The Elbow Method\n",
      "http://dataskeptic.com/epnotes/the-elbow-method.php\n",
      "98\n",
      "Too Good to be True\n",
      "http://dataskeptic.com/epnotes/too-good-to-be-true.php\n",
      "97\n",
      "[MINI] R-squared\n",
      "http://dataskeptic.com/epnotes/r-squared.php\n",
      "96\n",
      "Models of Mental Simulation\n",
      "http://dataskeptic.com/epnotes/models-of-mental-simulation.php\n",
      "95\n",
      "[MINI] Multiple Regression\n",
      "http://dataskeptic.com/epnotes/multiple-regression.php\n",
      "94\n",
      "Scientific Studies of People's Relationship to Music\n",
      "http://dataskeptic.com/epnotes/scientific-studies-of-peoples-relationship-to-music.php\n",
      "93\n",
      "[MINI] k-d trees\n",
      "http://dataskeptic.com/epnotes/k-d-trees.php\n",
      "92\n",
      "Auditing Algorithms\n",
      "http://dataskeptic.com/epnotes/auditing-algorithms.php\n",
      "91\n",
      "[MINI] The Bonferroni Correction\n",
      "http://dataskeptic.com/epnotes/bonferroni-correction.php\n",
      "90\n",
      "Detecting Pseudo-profound BS\n",
      "http://dataskeptic.com/epnotes/detecting-pseudo-profound-bs.php\n",
      "89\n",
      "[MINI] Gradient Descent\n",
      "http://dataskeptic.com/epnotes/gradient-descent.php\n",
      "88\n",
      "Let's Kill the Word Cloud\n",
      "http://dataskeptic.com/epnotes/kill-the-word-cloud.php\n",
      "87\n",
      "2015 Holiday Special\n",
      "http://dataskeptic.com/epnotes/2015-holiday-special.php\n",
      "86\n",
      "Wikipedia Revision Scoring as a Service\n",
      "http://dataskeptic.com/epnotes/wikipedia-revision-scoring-as-a-service.php\n",
      "85\n",
      "[MINI] Term Frequency - Inverse Document Frequency\n",
      "http://dataskeptic.com/epnotes/tf-idf.php\n",
      "84\n",
      "The Hunt for Vulcan\n",
      "http://dataskeptic.com/epnotes/the-hunt-for-vulcan.php\n",
      "83\n",
      "[MINI] The Accuracy Paradox\n",
      "http://dataskeptic.com/epnotes/the-accuracy-paradox.php\n",
      "82\n",
      "Neuroscience from a Data Scientist's Perspective\n",
      "http://dataskeptic.com/epnotes/neuroscience-from-a-data-scientists-perspective.php\n",
      "81\n",
      "[MINI] Bias Variance Tradeoff\n",
      "http://dataskeptic.com/epnotes/bias-variance-tradeoff.php\n",
      "80\n",
      "Big Data Doesn't Exist\n",
      "http://dataskeptic.com/epnotes/big-data-doesnt-exist.php\n",
      "79\n",
      "[MINI] Covariance and Correlation\n",
      "http://dataskeptic.com/epnotes/ep79_covariance-and-correlation.php\n",
      "78\n",
      "Bayesian A/B Testing\n",
      "http://dataskeptic.com/epnotes/ep78_bayesian-a-b-testing.php\n",
      "77\n",
      "[MINI] The Central Limit Theorem\n",
      "http://dataskeptic.com/epnotes/ep77_central-limit-theorem.php\n",
      "76\n",
      "Accessible Technology\n",
      "http://dataskeptic.com/epnotes/ep76_accessible-technology.php\n",
      "75\n",
      "[MINI] Multi-armed Bandit Problems\n",
      "http://dataskeptic.com/epnotes/ep75_multi-armed-bandit-problems.php\n",
      "74\n",
      "Shakespeare, Abiogenesis, and Exoplanets\n",
      "http://dataskeptic.com/epnotes/ep74_shakespeare-abiogenesis-and-exoplanets.php\n",
      "73\n",
      "[MINI] Sample Sizes\n",
      "http://dataskeptic.com/epnotes/ep73_small-sample-sizes.php\n",
      "72\n",
      "The Model Complexity Myth\n",
      "http://dataskeptic.com/epnotes/ep72_model-complexity-myth.php\n",
      "71\n",
      "[MINI] Distance Measures\n",
      "http://dataskeptic.com/epnotes/ep71_distance-measures.php\n",
      "70\n",
      "ContentMine\n",
      "http://dataskeptic.com/epnotes/ep70_contentmine.php\n",
      "69\n",
      "[MINI] Structured and Unstructured Data\n",
      "http://dataskeptic.com/epnotes/ep69_structured-and-unstructured.php\n",
      "68\n",
      "Measuring the Influence of Fashion Designers\n",
      "http://dataskeptic.com/epnotes/ep68_measuring-the-influence-of-fashion-designers.php\n",
      "67\n",
      "[MINI] PageRank\n",
      "http://dataskeptic.com/epnotes/ep67_pagerank.php\n",
      "66\n",
      "Data Science at Work in LA County\n",
      "http://dataskeptic.com/epnotes/ep66_data-science-at-work-in-la-county.php\n",
      "65\n",
      "[MINI] k-Nearest Neighbors\n",
      "http://dataskeptic.com/epnotes/ep65_k-nearest-neighbors.php\n",
      "64\n",
      "Crypto\n",
      "http://dataskeptic.com/blog/episodes/2015/crypto\n",
      "63\n",
      "[MINI] MapReduce\n",
      "http://dataskeptic.com/epnotes/ep63_map-reduce.php\n",
      "62\n",
      "Genetically Engineered Food and Trends in Herbicide Usage\n",
      "http://dataskeptic.com/epnotes/ep62_genetically-engineered-food-and-trends-in-herbicide-usage.php\n",
      "61\n",
      "[MINI] The Curse of Dimensionality\n",
      "http://dataskeptic.com/epnotes/ep61_the-curse-of-dimensionality.php\n",
      "60\n",
      "Video Game Analytics\n",
      "http://dataskeptic.com/epnotes/ep60_game-analytics.php\n",
      "59\n",
      "[MINI] Anscombe's Quartet\n",
      "http://dataskeptic.com/epnotes/ep59_anscombes_quartet.php\n",
      "58\n",
      "Proposing Annoyance Mining\n",
      "http://dataskeptic.com/epnotes/proposing-annoyance-mining.php\n",
      "57\n",
      "Preserving History at Cyark\n",
      "http://dataskeptic.com/epnotes/ep57_preserving-history-at-cyark.php\n",
      "56\n",
      "[MINI] A Critical Examination of a Study of Marriage by Political Affiliation\n",
      "http://dataskeptic.com/epnotes/ep56_a-critical-examination-of-a-study-of-marriage-by-political-affiliation.php\n",
      "55\n",
      "Detecting Cheating in Chess\n",
      "http://dataskeptic.com/epnotes/ep55_detecting-cheating-in-chess.php\n",
      "54\n",
      "[MINI] z-scores\n",
      "http://dataskeptic.com/epnotes/ep54_z-scores.php\n",
      "53\n",
      "Using Data to Help Those in Crisis\n",
      "http://dataskeptic.com/epnotes/ep53_using-data-to-help-those-in-crisis.php\n",
      "52\n",
      "The Ghost in the MP3\n",
      "http://dataskeptic.com/epnotes/ep52_the-ghost-in-the-mp3-with-Ryan-Maguire.php\n",
      "51\n",
      "Data Fest 2015\n",
      "http://dataskeptic.com/epnotes/ep51_data-fest-2015.php\n",
      "50\n",
      "[MINI] Cornbread and Overdispersion\n",
      "http://dataskeptic.com/epnotes/ep50_the-cornbread-episode-on-over-dispersion.php\n",
      "49\n",
      "[MINI] Natural Language Processing\n",
      "http://dataskeptic.com/epnotes/ep49_natural-language-processing.php\n",
      "48\n",
      "Computer-based Personality Judgments\n",
      "http://dataskeptic.com/epnotes/ep48_computer-based-personality-judgments-with-Youyou-Wu.php\n",
      "47\n",
      "[MINI] Markov Chain Monte Carlo\n",
      "http://dataskeptic.com/epnotes/ep47_Markov-chain-monte-carlo.php\n",
      "46\n",
      "[MINI] Markov Chains\n",
      "http://dataskeptic.com/epnotes/ep46_Markov-Chains.php\n",
      "45\n",
      "Oceanography and Data Science\n",
      "http://dataskeptic.com/epnotes/ep45_Oceanography-and-Data-Science.php\n",
      "44\n",
      "[MINI] Ordinary Least Squares Regression\n",
      "http://dataskeptic.com/epnotes/ordinary-least-squares.php\n",
      "43\n",
      "NYC Speed Camera Analysis with Tim Schmeier\n",
      "http://dataskeptic.com/epnotes/ep43_NYC-Speed-Camera-Analysis-with-Tim-Schmeier.php\n",
      "42\n",
      "[MINI] k-means clustering\n",
      "http://dataskeptic.com/epnotes/k-means-clustering.php\n",
      "41\n",
      "Shadow Profiles on Social Networks\n",
      "http://dataskeptic.com/epnotes/ep41_Shadow-Profiles-on-Social-Networks-with-Emre-Sarigol.php\n",
      "40\n",
      "[MINI] The Chi-Squared Test\n",
      "http://dataskeptic.com/epnotes/ep40_chi_sq_test.php\n",
      "39\n",
      "Mapping Reddit Topics with Randy Olson\n",
      "http://dataskeptic.com/epnotes/ep39_mapping-reddit-topics.php\n",
      "38\n",
      "[MINI] Partially Observable State Spaces\n",
      "http://dataskeptic.com/epnotes/partially-observable-state-spaces.php\n",
      "37\n",
      "Easily Fooling Deep Neural Networks\n",
      "http://dataskeptic.com/epnotes/ep37_easily-fooling-deep-neural-networks.php\n",
      "36\n",
      "[MINI] Data Provenance\n",
      "http://dataskeptic.com/epnotes/ep36_data-provenance.php\n",
      "35\n",
      "Doubtful News, Geology, Investigating Paranormal Groups, and Thinking Scientifically with Sharon Hill\n",
      "http://dataskeptic.com/epnotes/ep35_doubtful-news-geology-and-thinking-scientifically-with-Sharon-Hill.php\n",
      "34\n",
      "[MINI] Belief in Santa\n",
      "http://dataskeptic.com/epnotes/belief-in-santa.php\n",
      "33\n",
      "Economic Modeling and Prediction, Charitable Giving, and a Follow Up with Peter Backus\n",
      "http://dataskeptic.com/epnotes/ep33_Economic-Modeling-and-Prediction-with-Peter-Backus.php\n",
      "32\n",
      "[MINI] The Battle of the Sexes\n",
      "http://dataskeptic.com/epnotes/ep32_battle-of-the-sexes.php\n",
      "31\n",
      "The Science of Online Data at Plenty of Fish with Thomas Levi\n",
      "http://dataskeptic.com/epnotes/ep31_plenty-of-fish-data-science-approaches-with-thomas-levi.php\n",
      "30\n",
      "[MINI] The Girlfriend Equation\n",
      "http://dataskeptic.com/epnotes/ep30_the_girlfriend_equation.php\n",
      "29\n",
      "The Secret and the Global Consciousness Project with Alex Boklin\n",
      "http://dataskeptic.com/epnotes/ep29_the-secret-and-the-global-consciousness-project.php\n",
      "28\n",
      "[MINI] Monkeys on Typewriters\n",
      "http://dataskeptic.com/epnotes/ep28_random-numbers.php\n",
      "27\n",
      "Mining the Social Web with Matthew Russell\n",
      "http://dataskeptic.com/epnotes/ep27_mining-the-social-web.php\n",
      "26\n",
      "[MINI] Is the Internet Secure?\n",
      "http://dataskeptic.com/epnotes/is-the-internet-secure.php\n",
      "25\n",
      "Practicing and Communicating Data Science with Jeff Stanton\n",
      "http://dataskeptic.com/epnotes/practicing-and-communicating-data-science.php\n",
      "24\n",
      "[MINI] The T-Test\n",
      "http://dataskeptic.com/epnotes/t-test.php\n",
      "23\n",
      "Data Myths with Karl Mamer\n",
      "http://dataskeptic.com/epnotes/ep023.php\n",
      "22\n",
      "Contest Announcement\n",
      "http://dataskeptic.com/epnotes/ep022.php\n",
      "21\n",
      "[MINI] Selection Bias\n",
      "http://dataskeptic.com/epnotes/selection-bias.php\n",
      "20\n",
      "[MINI] Confidence Intervals\n",
      "http://dataskeptic.com/epnotes/confidence-intervals.php\n",
      "19\n",
      "[MINI] Value of Information\n",
      "http://dataskeptic.com/epnotes/value-of-information.php\n",
      "18\n",
      "Game Science Dice with Louis Zocchi\n",
      "http://dataskeptic.com/epnotes/ep017.php\n",
      "17\n",
      "Data Science at ZestFinance with Marick Sinay\n",
      "http://dataskeptic.com/epnotes/ep17_zest-finance-with-marick-sinay.php\n",
      "16\n",
      "[MINI] Decision Tree Learning\n",
      "http://dataskeptic.com/epnotes/decision-tree-learning.php\n",
      "15\n",
      "Jackson Pollock Authentication Analysis with Kate Jones-Smith\n",
      "http://dataskeptic.com/epnotes/ep014.php\n",
      "14\n",
      "[MINI] Noise!!\n",
      "http://dataskeptic.com/epnotes/noise.php\n",
      "13\n",
      "Guerilla Skepticism on Wikipedia with Susan Gerbic\n",
      "http://dataskeptic.com/epnotes/ep012.php\n",
      "12\n",
      "[MINI] Ant Colony Optimization\n",
      "http://dataskeptic.com/epnotes/ant-colony-optimization.php\n",
      "11\n",
      "Data in Healthcare IT with Shahid Shah\n",
      "http://dataskeptic.com/epnotes/ep010.php\n",
      "10\n",
      "[MINI] Cross Validation\n",
      "http://dataskeptic.com/epnotes/cross-validation.php\n",
      "9\n",
      "Streetlight Outage and Crime Rate Analysis with Zach Seeskin\n",
      "http://dataskeptic.com/epnotes/ep008.php\n",
      "8\n",
      "[MINI] Experimental Design\n",
      "http://dataskeptic.com/epnotes/experimental-design.php\n",
      "7\n",
      "The Right (big data) Tool for the Job with Jay Shankar\n",
      "http://dataskeptic.com/epnotes/ep006.php\n",
      "6\n",
      "[MINI] Bayesian Updating\n",
      "http://dataskeptic.com/blog/episodes/2014/bayesian-updating\n",
      "5\n",
      "Personalized Medicine with Niki Athanasiadou\n",
      "http://dataskeptic.com/blog/episodes/2014/personalized-medicine\n",
      "4\n",
      "[MINI] p-values\n",
      "http://dataskeptic.com/epnotes/p-values.php\n",
      "3\n",
      "Advertising Attribution with Nathan Janos\n",
      "http://dataskeptic.com/epnotes/ep002.php\n",
      "2\n",
      "[MINI] type i / type ii errors\n",
      "http://dataskeptic.com/epnotes/type_i_type_ii.php\n",
      "1\n",
      "Introduction\n",
      "http://dataskeptic.com/epnotes/ep001.php\n"
     ]
    }
   ],
   "source": [
    "for desc in descriptions:\n",
    "    print(descToNum[desc])\n",
    "    print(descToTitle[desc])\n",
    "    print(descToLink[desc])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save description in txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thefile = open('./text/episode_descs.txt', 'w')\n",
    "\n",
    "for i, desc in enumerate(descriptions):\n",
    "    desc = desc.encode('utf-8').strip()\n",
    "    desc = \"*\"+ str(i)+str(desc).replace('\\n', \"\") \n",
    "    thefile.write(\"%s\\n\" % desc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "*0b'One Shot Learning is the c\n",
      "2\n",
      "*1b'Recommender systems play a\n",
      "3\n",
      "*2b'Thanks to our sponsor bril\n",
      "4\n",
      "*3b'Zillow is a leading real e\n",
      "5\n",
      "*4b'Our guest Pranav Rajpurkar\n",
      "6\n",
      "*5b'RNNs are a class of deep l\n",
      "7\n",
      "*6b\"Thanks to our sponsor Spri\n",
      "8\n",
      "*7b\"A Bayesian Belief Network \n",
      "9\n",
      "*8b'In this episode, Tony Belt\n",
      "10\n",
      "*9b\"In statistics, two random \n",
      "11\n",
      "*10b'Animals can\\'t tell us wh\n",
      "12\n",
      "*11b'This episode collects int\n",
      "13\n",
      "*12b'This episode discusses th\n",
      "14\n",
      "*13b'hen faced with medical is\n",
      "15\n",
      "*14b'In a neural network, the \n",
      "16\n",
      "*15b'This episode recaps the M\n",
      "17\n",
      "*16b\"Max-pooling is a procedur\n",
      "18\n",
      "*17b'This episode is an interv\n",
      "19\n",
      "*18b\"CNNs are characterized by\n",
      "20\n",
      "*19b\"Despite the success of GA\n",
      "21\n",
      "*20b\"GANs are an unsupervised \n",
      "22\n",
      "*21b'Recently, we\\'ve seen opi\n",
      "23\n",
      "*22b\"No reliable, complete dat\n",
      "24\n",
      "*23b'There\\'s more than one ty\n",
      "25\n",
      "*24b'Backpropagation is a comm\n",
      "26\n",
      "*25b\"In this week's episode of\n",
      "27\n",
      "*26b\"Feed Forward Neural Netwo\n",
      "28\n",
      "*27b'In this Data Skeptic epis\n",
      "29\n",
      "*28b\"Today's episode overviews\n",
      "30\n",
      "*29b'DataRefuge is a public co\n",
      "31\n",
      "*30b'If a CEO wants to know th\n",
      "32\n",
      "*31b'In this episode, I speak \n",
      "33\n",
      "*32b'In this episode, we talk \n",
      "34\n",
      "*33b\"Versioning isn't just for\n",
      "35\n",
      "*34b'Logistic Regression is a \n",
      "36\n",
      "*35b\"Prior work has shown that\n",
      "37\n",
      "*36b\"Deep learning can be pron\n",
      "38\n",
      "*37b\"In this episode I speak w\n",
      "39\n",
      "*38b'We close out 2016 with a \n",
      "40\n",
      "*39b\"Today's episode is a read\n",
      "41\n",
      "*40b\"Classically, entropy is a\n",
      "42\n",
      "*41b'Cloud services are now ub\n",
      "43\n",
      "*42b\"Today's episode is all ab\n",
      "44\n",
      "*43b\"The Bootstrap is a method\n",
      "45\n",
      "*44b'The Gini Coefficient (as \n",
      "46\n",
      "*45b\"Financial analysis techni\n",
      "47\n",
      "*46b'AdaBoost is a canonical e\n",
      "48\n",
      "*47b'Platform as a service is \n",
      "49\n",
      "*48b'For machine learning mode\n",
      "50\n",
      "*49b'As cities provide bike sh\n",
      "51\n",
      "*50b'Random forest is a popula\n",
      "52\n",
      "*51b\"Jo Hardin joins us this w\n",
      "53\n",
      "*52b'The F1 score is a model d\n",
      "54\n",
      "*53b\"Urban congestion effects \n",
      "55\n",
      "*54b\"Heteroskedasticity is a t\n",
      "56\n",
      "*55b\"Our guest today is Michae\n",
      "57\n",
      "*56b'Paxos is a protocol for a\n",
      "58\n",
      "*57b'Machine learning models a\n",
      "59\n",
      "*58b'Analysis of variance is a\n",
      "60\n",
      "*59b'When humans describe imag\n",
      "61\n",
      "*60b'Survival analysis techniq\n",
      "62\n",
      "*61b'This week is an insightfu\n",
      "63\n",
      "*62b\"An ROC curve is a plot th\n",
      "64\n",
      "*63b\"I'm joined by Chris Stucc\n",
      "65\n",
      "*64b\"If you'd like to make a g\n",
      "66\n",
      "*65b'Kristian Lum (@KLdivergen\n",
      "67\n",
      "*66b'Distributed computing can\n",
      "68\n",
      "*67b'A startup is claiming tha\n",
      "69\n",
      "*68b'Goodhart\\'s law states th\n",
      "70\n",
      "*69b\"I'm joined this week by J\n",
      "71\n",
      "*70b'Mystery shoppers and frui\n",
      "72\n",
      "*71b\"I'm joined by Wes McKinne\n",
      "73\n",
      "*72b'Bargaining is the process\n",
      "74\n",
      "*73b'Deepjazz is a project fro\n",
      "75\n",
      "*74b'When working with time se\n",
      "76\n",
      "*75b'This week I spoke with El\n",
      "77\n",
      "*76b'A dinner party at Data Sk\n",
      "78\n",
      "*77b'Cheng-tao Chu (@chengtao_\n",
      "79\n",
      "*78b\"Co-host Linh Da was in a \n",
      "80\n",
      "*79b'Certain data mining algor\n",
      "81\n",
      "*80b'Today on Data Skeptic, La\n",
      "82\n",
      "*81b\"How well does your model \n",
      "83\n",
      "*82b'Jessica Hamrick joins us \n",
      "84\n",
      "*83b'This episode is a discuss\n",
      "85\n",
      "*84b\"Samuel Mehr joins us this\n",
      "86\n",
      "*85b'This episode reviews the \n",
      "87\n",
      "*86b'Algorithms are pervasive \n",
      "88\n",
      "*87b\"Today's episode begins by\n",
      "89\n",
      "*88b\"A recent paper in the jou\n",
      "90\n",
      "*89b\"Today's mini episode disc\n",
      "91\n",
      "*90b\"This episode is a discuss\n",
      "92\n",
      "*91b\"Today's episode is a read\n",
      "93\n",
      "*92b'In this interview with Aa\n",
      "94\n",
      "*93b\"Today's topic is term fre\n",
      "95\n",
      "*94b'Early astronomers could s\n",
      "96\n",
      "*95b\"Today's episode discusses\n",
      "97\n",
      "*96b\"... or should this have b\n",
      "98\n",
      "*97b\"A discussion of the expec\n",
      "99\n",
      "*98b'The recent opinion piece \n",
      "100\n",
      "*99b'The degree to which two v\n",
      "101\n",
      "*100b\"Today's guest is Cameron\n",
      "102\n",
      "*101b'The central limit theore\n",
      "103\n",
      "*102b\"Today's guest is Chris H\n",
      "104\n",
      "*103b'The multi-armed bandit p\n",
      "105\n",
      "*104b'Our episode this week be\n",
      "106\n",
      "*105b\"There are several factor\n",
      "107\n",
      "*106b\"There's an old adage whi\n",
      "108\n",
      "*107b'There are many occasions\n",
      "109\n",
      "*108b\"ContentMine is a project\n",
      "110\n",
      "*109b\"Today's mini-episode exp\n",
      "111\n",
      "*110b\"Yusan Lin shares her res\n",
      "112\n",
      "*111b'PageRank is the algorith\n",
      "113\n",
      "*112b'In this episode, Benjami\n",
      "114\n",
      "*113b'This episode explores th\n",
      "115\n",
      "*114b'How do people think rati\n",
      "116\n",
      "*115b'This mini-episode is a h\n",
      "117\n",
      "*116b'The Credible Hulk joins \n",
      "118\n",
      "*117b\"More features are not al\n",
      "119\n",
      "*118b'This episode discusses v\n",
      "120\n",
      "*119b\"This mini-episode discus\n",
      "121\n",
      "*120b'A recent episode of the \n",
      "122\n",
      "*121b\"Elizabeth Lee from CyArk\n",
      "123\n",
      "*122b'Linhda and Kyle review a\n",
      "124\n",
      "*123b\"With the advent of algor\n",
      "125\n",
      "*124b\"This week's episode dicu\n",
      "126\n",
      "*125b\"This week Noelle Sio Sal\n",
      "127\n",
      "*126b\"Have you ever wondered w\n",
      "128\n",
      "*127b'This episode contains co\n",
      "129\n",
      "*128b'For our 50th episode we \n",
      "130\n",
      "*129b'This episode overviews s\n",
      "131\n",
      "*130b'Guest Youyou Wu discuses\n",
      "132\n",
      "*131b'This episode explores ho\n",
      "133\n",
      "*132b\"This episode introduces \n",
      "134\n",
      "*133b\"Nicole Goebel joins us t\n",
      "135\n",
      "*134b'This episode explores Or\n",
      "136\n",
      "*135b\"New York State approved \n",
      "137\n",
      "*136b'The k-means clustering a\n",
      "138\n",
      "*137b'Emre Sarigol joins me th\n",
      "139\n",
      "*138b'The \\xcf\\x872 (Chi-Squar\n",
      "140\n",
      "*139b\"My quest this week is no\n",
      "141\n",
      "*140b'When dealing with dynami\n",
      "142\n",
      "*141b'My guest this week is An\n",
      "143\n",
      "*142b'This episode introduces \n",
      "144\n",
      "*143b'I had the change to spea\n",
      "145\n",
      "*144b'In this quick holiday ep\n",
      "146\n",
      "*145b'Economist Peter Backus j\n",
      "147\n",
      "*146b'Love and Data is the con\n",
      "148\n",
      "*147b'Can algorithms help you \n",
      "149\n",
      "*148b'Economist Peter Backus p\n",
      "150\n",
      "*149b'I\\'m joined this week by\n",
      "151\n",
      "*150b'What is randomness? How \n",
      "152\n",
      "*151b'This week\\'s episode exp\n",
      "153\n",
      "*152b'This episode explores th\n",
      "154\n",
      "*153b'Jeff Stanton joins me in\n",
      "155\n",
      "*154b\"The t-test is this week'\n",
      "156\n",
      "*155b\"This week I'm joined by \n",
      "157\n",
      "*156b'The Data Skeptic Podcast\n",
      "158\n",
      "*157b'A discussion about condu\n",
      "159\n",
      "*158b'Commute times and BBQ in\n",
      "160\n",
      "*159b'A discussion about getti\n",
      "161\n",
      "*160b'In this bonus episode, g\n",
      "162\n",
      "*161b'Marick Sinay from ZestFi\n",
      "163\n",
      "*162b'Linhda and Kyle talk abo\n",
      "164\n",
      "*163b\"Our guest this week is H\n",
      "165\n",
      "*164b'Our topic for this week \n",
      "166\n",
      "*165b\"Our guest this week is S\n",
      "167\n",
      "*166b\"In this week's mini epis\n",
      "168\n",
      "*167b'Our guest this week is S\n",
      "169\n",
      "*168b'This miniepisode discuss\n",
      "170\n",
      "*169b\"This episode features a \n",
      "171\n",
      "*170b'This episode loosely exp\n",
      "172\n",
      "*171b\"In this week's episode, \n",
      "173\n",
      "*172b\"In this minisode, we dis\n",
      "174\n",
      "*173b'In the second full lengt\n",
      "175\n",
      "*174b'In this mini, we discuss\n",
      "176\n",
      "*175b\"A conversation with Conv\n",
      "177\n",
      "*176b'In this first mini-episo\n",
      "178\n",
      "*177b'The Data Skeptic Podcast\n"
     ]
    }
   ],
   "source": [
    "with open('./text/episode_descs.txt', 'r') as f:\n",
    "    i=0\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        print(i)\n",
    "        print(line[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i # it should be 178 before 9/28/2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the word vectors trained from SO to represent episode descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get word vectors trained from SO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a_</th>\n",
       "      <td>-0.713271</td>\n",
       "      <td>-0.077981</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>-0.697069</td>\n",
       "      <td>-2.598913</td>\n",
       "      <td>-1.080629</td>\n",
       "      <td>-1.033209</td>\n",
       "      <td>0.885287</td>\n",
       "      <td>0.872299</td>\n",
       "      <td>-1.011196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674927</td>\n",
       "      <td>0.418498</td>\n",
       "      <td>-0.977101</td>\n",
       "      <td>-1.157536</td>\n",
       "      <td>-0.027437</td>\n",
       "      <td>-0.224027</td>\n",
       "      <td>-0.545397</td>\n",
       "      <td>-0.176365</td>\n",
       "      <td>2.447978</td>\n",
       "      <td>0.182951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a__</th>\n",
       "      <td>0.013685</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.025492</td>\n",
       "      <td>-0.012469</td>\n",
       "      <td>-0.009770</td>\n",
       "      <td>-0.009924</td>\n",
       "      <td>-0.004867</td>\n",
       "      <td>0.006001</td>\n",
       "      <td>0.038898</td>\n",
       "      <td>-0.027364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053458</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>-0.048481</td>\n",
       "      <td>-0.054677</td>\n",
       "      <td>-0.030503</td>\n",
       "      <td>-0.012346</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.056696</td>\n",
       "      <td>-0.029260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_a</th>\n",
       "      <td>0.020415</td>\n",
       "      <td>0.156389</td>\n",
       "      <td>-0.060513</td>\n",
       "      <td>-0.028313</td>\n",
       "      <td>-0.116366</td>\n",
       "      <td>-0.042931</td>\n",
       "      <td>-0.130101</td>\n",
       "      <td>-0.006461</td>\n",
       "      <td>0.008261</td>\n",
       "      <td>-0.065245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067757</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>-0.111685</td>\n",
       "      <td>0.160803</td>\n",
       "      <td>0.014258</td>\n",
       "      <td>-0.012060</td>\n",
       "      <td>0.078916</td>\n",
       "      <td>0.054315</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>-0.026724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_adjusted</th>\n",
       "      <td>-0.046735</td>\n",
       "      <td>0.041834</td>\n",
       "      <td>-0.061836</td>\n",
       "      <td>0.021160</td>\n",
       "      <td>0.046803</td>\n",
       "      <td>-0.024107</td>\n",
       "      <td>-0.032979</td>\n",
       "      <td>0.053604</td>\n",
       "      <td>0.037605</td>\n",
       "      <td>-0.080140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083108</td>\n",
       "      <td>-0.026857</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>-0.040803</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>0.027903</td>\n",
       "      <td>0.024604</td>\n",
       "      <td>-0.021590</td>\n",
       "      <td>-0.051460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_after_est</th>\n",
       "      <td>-0.011185</td>\n",
       "      <td>0.017912</td>\n",
       "      <td>-0.080105</td>\n",
       "      <td>-0.030186</td>\n",
       "      <td>0.052851</td>\n",
       "      <td>-0.022680</td>\n",
       "      <td>-0.036077</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>-0.008886</td>\n",
       "      <td>-0.044048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066457</td>\n",
       "      <td>-0.011853</td>\n",
       "      <td>-0.007649</td>\n",
       "      <td>0.017082</td>\n",
       "      <td>-0.006570</td>\n",
       "      <td>-0.020994</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>0.006295</td>\n",
       "      <td>-0.008900</td>\n",
       "      <td>-0.035567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1         2         3         4         5  \\\n",
       "a_          -0.713271 -0.077981  0.010418 -0.697069 -2.598913 -1.080629   \n",
       "a__          0.013685 -0.000013 -0.025492 -0.012469 -0.009770 -0.009924   \n",
       "a_a          0.020415  0.156389 -0.060513 -0.028313 -0.116366 -0.042931   \n",
       "a_adjusted  -0.046735  0.041834 -0.061836  0.021160  0.046803 -0.024107   \n",
       "a_after_est -0.011185  0.017912 -0.080105 -0.030186  0.052851 -0.022680   \n",
       "\n",
       "                    6         7         8         9    ...          190  \\\n",
       "a_          -1.033209  0.885287  0.872299 -1.011196    ...     0.674927   \n",
       "a__         -0.004867  0.006001  0.038898 -0.027364    ...     0.053458   \n",
       "a_a         -0.130101 -0.006461  0.008261 -0.065245    ...    -0.067757   \n",
       "a_adjusted  -0.032979  0.053604  0.037605 -0.080140    ...     0.083108   \n",
       "a_after_est -0.036077  0.007863 -0.008886 -0.044048    ...     0.066457   \n",
       "\n",
       "                  191       192       193       194       195       196  \\\n",
       "a_           0.418498 -0.977101 -1.157536 -0.027437 -0.224027 -0.545397   \n",
       "a__          0.000878  0.002365 -0.048481 -0.054677 -0.030503 -0.012346   \n",
       "a_a          0.007425 -0.111685  0.160803  0.014258 -0.012060  0.078916   \n",
       "a_adjusted  -0.026857  0.024127  0.040530 -0.040803 -0.038954  0.027903   \n",
       "a_after_est -0.011853 -0.007649  0.017082 -0.006570 -0.020994  0.003839   \n",
       "\n",
       "                  197       198       199  \n",
       "a_          -0.176365  2.447978  0.182951  \n",
       "a__          0.029457  0.056696 -0.029260  \n",
       "a_a          0.054315  0.021641 -0.026724  \n",
       "a_adjusted   0.024604 -0.021590 -0.051460  \n",
       "a_after_est  0.006295 -0.008900 -0.035567  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 'word2vector_model_question_answer_200_6_2'\n",
    "fname = './word_vec/'+key+\".csv\"\n",
    "word_vecs_df = pd.read_csv(fname,index_col=0)\n",
    "word_vecs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100269"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = word_vecs_df.index\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './vocab_dict/vocab_dict_question_answer_200_6_2.csv'\n",
    "with open(fname, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    vocab_dic = dict(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, value in vocab_dic.items():\n",
    "    vocab_dic[k] = int(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dic['a_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the text in episode descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                \n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "                #This lowercases, tokenizes, de-accents (optional). – the output are final tokens = unicode strings, that won’t be processed any further.\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fname = './text/episode_descs.txt'\n",
    "episode_desc_corpus = list(read_corpus(fname, tokens_only= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for desc in episode_desc_corpus:\n",
    "    corpus.append(\" \".join(desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tf_idf features of episode descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1,vocabulary = vocab_dic)\n",
    "X = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: [vocabulary] Mapping or iterable, optional\n",
    "Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words are in episode descriptions but not in the vocab of SO?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************\n",
      "22\n",
      "{'jawbone', 'rosevere', 'aglpjrmp', 'zehr', 'blueplastic', 'dashboarding', 'zareen', 'periscopedata', 'joytafty', 'openhouse', 'iamzareenf'}\n",
      "No reliable, complete database cataloging home sales data at a transaction level is available for the average person to access. To a data scientist interesting in studying this data, our hands are complete tied. Opportunities like testing sociological theories, exploring economic impacts, study market forces, or simply research the value of an investment when buying a home are all blocked by the lack of easy access to this dataset. OpenHouse seeks to correct that by centralizing and standardizing all publicly available home sales transactional data. In this episode, we discuss the achievements of OpenHouse to date, and what plans exist for the future.     Check out the OpenHouse gallery.    I also encourage everyone to check out the project Zareen mentioned which was her Harry Potter word2vec webapp and Joy's project doing data visualization on Jawbone data. Guests Thanks again to @iamzareenf, @blueplastic, and @joytafty for coming on the show. Thanks to the numerous other volunteers who have helped with the project as well! Announcements and details   If you're interested in getting involved in OpenHouse, check out the OpenHouse contributor's quickstart page.   Kyle is giving a  machine learning talk in Los Angeles on May 25th, 2017 at Zehr.   Sponsor Thanks to our sponsor for this episode Periscope Data. The blog post demoing their maps option is on our blog titled Periscope Data Maps.  To start a free trial of their dashboarding too, visit http://periscopedata.com/skeptics Kyle recently did a youtube video exploring the Data Skeptic podcast download numbers using Periscope Data. Check it out at https://youtu.be/aglpJrMp0M4. Supplemental music is Lee Rosevere's Let's Start at the Beginning.  \n",
      "*******************************************************\n",
      "25\n",
      "{'patreon', 'maura', 'omr', 'meneses', 'benefactors', 'polich'}\n",
      "  In this week's episode of Data Skeptic, host Kyle Polich talks with guest Maura Church, Patreon's data science manager. Patreon is a fast-growing crowdfunding platform that allows artists and creators of all kinds build their own subscription content service. The platform allows fans to become patrons of their favorite artists- an idea similar the Renaissance times, when musicians would rely on benefactors to become their patrons so they could make more art. At Patreon, Maura's data science team strives to provide creators with insight, information, and tools, so that creators can focus on what they do best-- making art. On the show, Maura talks about some of her projects with the data science team at Patreon. Among the several topics discussed during the episode include: optical music recognition (OMR) to translate musical scores to electronic format, network analysis to understand the connection between creators and patrons, growth forecasting and modeling in a new market, and churn modeling to determine predictors of long time support. A more detailed explanation of Patreon's A/B testing framework can be found  here Other useful links to topics mentioned during the show:  OMR research Patreon blog Patreon HQ blog Amanda Palmer Fran Meneses\n",
      "*******************************************************\n",
      "33\n",
      "{'rosevere', 'gopher', 'whitenack', 'periscopedata', 'containerized', 'pachyderm'}\n",
      "Versioning isn't just for source code. Being able to track changes to data is critical for answering questions about data provenance, quality, and reproducibility. Daniel Whitenack joins me this week to talk about these concepts and share his work on Pachyderm. Pachyderm is an open source containerized data lake. During the show, Daniel mentioned the Gopher Data Science github repo as a great resource for any data scientists interested in the Go language. Although we didn't mention it, Daniel also did an interesting analysis on the 2016 world chess championship that complements our recent episode on chess well. You can find that post  here Supplemental music is Lee Rosevere's Let's Start at the Beginning.   Thanks to Periscope Data for sponsoring this episode. More about them at periscopedata.com/skeptics       \n",
      "*******************************************************\n",
      "75\n",
      "{'shakarian', 'shaabani', 'pauloshakasu', 'elham', 'onarxiv', 'cysis'}\n",
      "This week I spoke with Elham Shaabani and Paulo Shakarian (@PauloShakASU) about their recent paper Early Identification of Violent Criminal Gang Members (also available onarXiv). In this paper, they use social network analysis techniques and machine learning to provide early detection of known criminal offenders who are in a high risk group for committing violent crimes in the future. Their techniques outperform existing techniques used by the police. Elham and Paulo are part of the Cyber-Socio Intelligent Systems (CySIS) Lab.\n",
      "*******************************************************\n",
      "88\n",
      "{'gordonpennycook', 'pennycook', 'profundity', 'nonesense', 'bsr', 'deepak', 'pseudoprofound', 'receptivity'}\n",
      " A recent paper in the journal of Judgment and Decision Making titled On the reception and detection of pseudo-profound bullshit explores empirical questions around a reader's ability to detect statements which may sound profound but are actually a collection of buzzwords that fail to contain adequate meaning or truth. These statements are definitively different from lies and nonesense, as we discuss in the episode. This paper proposes the Bullshit Receptivity scale (BSR) and empirically demonstrates that it correlates with existing metrics like the Cognitive Reflection Test, building confidence that this can be a useful, repeatable, empirical measure of a person's ability to detect pseudo-profound statements as being different from genuinely profound statements. Additionally, the correlative results provide some insight into possible root causes for why individuals might find great profundity in these statements based on other beliefs or cognitive measures. The paper's lead author Gordon Pennycook joins me to discuss this study's results. If you'd like some examples of pseudo-profound bullshit, you can randomly generate some based on Deepak Chopra's twitter feed. To read other work from Gordon, check out his Google Scholar page and find him on twitter via @GordonPennycook. And just for fun, if you think you've dreamed up a Data Skeptic related pseudo-profound bullshit statement, tweet it with hashtag #pseudoprofound. If I see an especially clever or humorous one, I might want to send you a free Data Skeptic sticker.   \n",
      "*******************************************************\n",
      "94\n",
      "{'cosmos', 'verrier', 'inversesquare', 'urbain', 'levenson', 'tomlevenson', 'vulcan', 'xac', 'neptune', 'documentaries', 'athttps', 'counterfeiter'}\n",
      "Early astronomers could see several of the planets with the naked eye. The invention of the telescope allowed for further understanding of our solar system. The work of Isaac Newton allowed later scientists to accurately predict Neptune, which was later observationally confirmed exactly where predicted. It seemed only natural that a similar unknown body might explain anomalies in the orbit of Mercury, and thus began the search for the hypothesized planet Vulcan. Thomas Levenson's book \"The Hunt for Vulcan\" is a narrative of the key scientific minds involved in the search and eventual refutation of an unobserved planet between Mercury and the sun. Thomas joins me in this episode to discuss his book and the fascinating story of the quest to find this planet. During the discussion, we mention one of the contributions made by Urbain-Jean-Joseph Le Verrier which involved some complex calculations which enabled him to predict where to find the planet that would eventually be called Neptune. The calculus behind this work is difficult, and some of that work is demonstrated in a Jupyter notebook I recently discovered from Paulo Marques titled The-Body Problem.      Thomas Levenson is a professor at MIT and head of its science writing program. He is the author of several books, including Einstein in Berlin and Newton and the Counterfeiter: The Unknown Detective Career of the Worldâ€™s Greatest Scientist. He has also made ten feature-length documentaries (including a two-hour Nova program on Einstein) for which he has won numerous awards. In his most recent book \"The Hunt for Vulcan\", explores the century spanning quest to explain the movement of the cosmos via theory and the role the hypothesized planet Vulcan played in the story. Follow Thomas on twitter @tomlevenson and check out his blog athttps://inversesquare.wordpress.com/. Pick up your copy of The Hunt for Vulcan at your local bookstore, preferred book buying place, or at the Penguin Random House site.    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************\n",
      "102\n",
      "{'atheist', 'bandcamp', 'gonz_blinko', 'refreshable', 'chrishofstader', 'skepchick', 'shelley', 'skeptibility', 'nomads', 'activist', 'tenon', 'hofstader', 'braille', 'dqtech'}\n",
      "Today's guest is Chris Hofstader (@gonz_blinko), an accessibility researcher and advocate, as well as an activist for causes such as improving access to information for blind and vision impaired people. His background in computer programming enabled him to be the leader of JAWS, a Windows program that allowed people with a visual impairment to read their screen either through text-to-speech or a refreshable braille display. He's the Managing Member of 3 Mouse Technology. He's also a frequent blogger primarily at chrishofstader.com. For web developers and site owners, Chris recommends two tools to help test for accessibility issues: tenon.io and dqtech.co. A guest post from Chris appeared on the Skepchick blogged titled Skepticism and Disability which lead to the formation of the sister site Skeptibility. In a discussion of skepticism and favorite podcasts, Chris mentioned a number of great shows, most notably The Pod Delusion to which he was a contributor. Additionally, Chris has also appeared on The Atheist Nomads. Lastly, a shout out from Chris to musician Shelley Segal whom he hosted just before the date of recording of this episode. Her music can be found on her site or via bandcamp.\n",
      "*******************************************************\n",
      "160\n",
      "{'outro', 'awesomedice', 'unnoticably', 'satanic', 'dicecollector', 'swears', 'zocchi'}\n",
      "In this bonus episode, guest Louis Zocchi discusses his background in the gaming industry, specifically, how he became a manufacturer of dice designed to produce statistically uniform outcomes. During the show Louis mentioned a two part video listeners might enjoy: part 1 and part 2 can both be found on youtube. Kyle mentioned a robot capable of unnoticably cheating at Rock Paper Scissors / Ro Sham Bo. More details can be found here. Louis mentioned dice collector Kevin Cook whose website is DiceCollector.com While we're on the subject of table top role playing games, Kyle recommends these two related podcasts listeners might enjoy: The Conspiracy Skeptic podcast (on which host Kyle was recently a guest) had a great episode \"Dungeons and Dragons - The Devil's Game?\" which explores claims of D&Ds alleged ties to skepticism. Also, Kyle swears there's a great Monster Talk episode discussing claims of a satanic connection to Dungeons and Dragons, but despite mild efforts to locate it, he came up empty. Regardless, listeners of the Data Skeptic Podcast are encouraged to explore the back catalog to try and find the aforementioned episode of this great podcast. Last but not least, as mentioned in the outro, awesomedice.com did some great independent empirical testing that confirms Game Science dice are much closer to the desired uniform distribution over possible outcomes when compared to one leading manufacturer.\n",
      "*******************************************************\n",
      "163\n",
      "{'falco', 'gogh', 'micolich', 'quasicrystal', 'mathur', 'hockney', 'drip', 'brushstroke'}\n",
      "Our guest this week is Hamilton physics professor Kate Jones-Smith who joins us to discuss the evidence for the claim that drip paintings of Jackson Pollock contain fractal patterns. This hypothesis originates in a paper by Taylor, Micolich, and Jonas titled Fractal analysis of Pollock's drip paintings which appeared in Nature. Kate and co-author Harsh Mathur wrote a paper titled Revisiting Pollock's Drip Paintings which also appeared in Nature. A full text PDF can be found here, but lacks the helpful figures which can be found here, although two images are blurred behind a paywall. Their paper was covered in the New York Times as well as in USA Today (albeit with with a much more delightful headline: Never mind the Pollock's [sic]). While discussing the intersection of science and art, the conversation also touched briefly on a few other intersting topics. For example, Penrose Tiles appearing in islamic art (pre-dating Roger Penrose's investigation of the interesting properties of these tiling processes), Quasicrystal designs in art, Automated brushstroke analysis of the works of Vincent van Gogh, and attempts to authenticate a possible work of Leonardo Da Vinci of uncertain provenance. Last but not least, the conversation touches on the particularly compellingHockney-Falco Thesis which is also covered in David Hockney's book Secret Knowledge. For those interested in reading some of Kate's other publications, many Katherine Jones-Smith articles can be found at the given link, all of which have downloadable PDFs.\n",
      "*******************************************************\n",
      "167\n",
      "{'supersite', 'cio', 'osehra', 'hitsphere', 'shahid', 'chairperson', 'netspective'}\n",
      "Our guest this week is Shahid Shah. Shahid is CEO at Netspective, and writes three blogs: Health Care Guy, Shahid Shah, and HitSphere - the Healthcare IT Supersite.  During the program, Kyle recommended a talk from the 2014 MIT Sloan CIO Symposium entitled  Transforming \"Digital Silos\" to \"Digital Care Enterprise\"  which was hosted by our guest Shahid Shah.  In addition to his work in Healthcare IT, he also the chairperson for Open Source Electronic Health Record Alliance, an non-profit organization that, amongst other activities, is hosting an upcoming conference. The 3rd annual  OSEHRA Open Source Summit: Global Collaboration in Healthcare IT , which will be taking place September 3-5, 2014 in Washington DC.  For our benevolent recommendation, Shahid suggested listeners may benefit from taking the time to read  books on leadership for the insights they provide. For our self-serving recommendation, Shahid recommended listeners check out his company Netspective , if you are working with a company looking for help getting started building software utilizing next generation technologies.\n"
     ]
    }
   ],
   "source": [
    "# What words are not in SO?\n",
    "for i in range(178):\n",
    "#     print(len(X[i,:].nonzero()[1]))\n",
    "#     print(len(set(corpus[i].split(' '))))\n",
    "    diff = set(corpus[i].split(' ')).difference(set(vocab))\n",
    "    if len(diff) >5:\n",
    "        print('*******************************************************')\n",
    "        print(i)\n",
    "        print(diff)  \n",
    "        print(descriptions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 100269)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get weighted doc vectors for all episode description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "episode_desc_corpus[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get the weighted vectors of the episode descriptions?\n",
    "\n",
    "....\n",
    "\n",
    "\n",
    "<img src=\"pictures/tf_idf_matrix.png\">\n",
    "<img src=\"pictures/word_vec_df.png\">\n",
    "\n",
    "\n",
    "- For example, doc has three words: doc = [word1, word2, word3].\n",
    "- vec_word_i = [d1, d2, ..., dn] \n",
    "- n = size in hidden layer.\n",
    "\n",
    "- tf_idf_ji = tf_idf of word i in doc_j; Scale them such that sum_i tf_idf_ji = 1. \n",
    "\n",
    "- Then the vector of doc_j = sum_i (vec_word_i * tf_dif_ji) which is a vector with the same len as vec_word_i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doc_weighted_vec(i, doc_corpus , tf_idf = X, weighted = True): # ith documents. doc_corpus a list of words\n",
    "    \n",
    "    df = word_vecs_df \n",
    "    related_rows = df.loc[sorted(list(set(doc_corpus).intersection(set(vocab)))), :] \n",
    "    \n",
    "    if weighted:\n",
    "        weights = []\n",
    "        ind = sorted(tf_idf[i,:].nonzero()[1])\n",
    "        if sum([vectorizer.vocabulary_[related_rows.index[j]] != ind[j] for j in range(len(ind))]) != 0:\n",
    "            print(\"words position don't match\")\n",
    "            return \n",
    "        for j in ind:\n",
    "            weights.append(tf_idf[i,j])\n",
    "        weights = np.array(weights)/sum(weights)\n",
    "    else:\n",
    "        weights = [1/related_rows.shape[0]] * related_rows.shape[0]\n",
    "    \n",
    "    if related_rows.shape[0] != len(weights):\n",
    "        print(i)\n",
    "        print(related_rows.shape[0])\n",
    "        print(len(weights))\n",
    "    \n",
    "    result = related_rows.T * weights\n",
    "    return result.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_vec_weighted = []\n",
    "total = len(descriptions)\n",
    "for i in range(total):\n",
    "    doc_corpus = episode_desc_corpus[i]\n",
    "    episode_vec_weighted.append(get_doc_weighted_vec(i,doc_corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the episode weighted vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('episode_vec/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('episode_vec/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(episode_vec_weighted, \"episode_vec_weighted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.321742</td>\n",
       "      <td>-0.091622</td>\n",
       "      <td>1.020477</td>\n",
       "      <td>-0.105819</td>\n",
       "      <td>-0.154422</td>\n",
       "      <td>0.356765</td>\n",
       "      <td>-0.185670</td>\n",
       "      <td>0.368219</td>\n",
       "      <td>0.242798</td>\n",
       "      <td>-0.084168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051760</td>\n",
       "      <td>0.202284</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.152225</td>\n",
       "      <td>0.290375</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>-0.216377</td>\n",
       "      <td>-0.123192</td>\n",
       "      <td>-0.171273</td>\n",
       "      <td>-0.096611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.299671</td>\n",
       "      <td>0.405244</td>\n",
       "      <td>0.184728</td>\n",
       "      <td>-0.109321</td>\n",
       "      <td>-0.225838</td>\n",
       "      <td>-0.050448</td>\n",
       "      <td>-0.330242</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.482650</td>\n",
       "      <td>-0.160271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138463</td>\n",
       "      <td>0.477873</td>\n",
       "      <td>0.308865</td>\n",
       "      <td>-0.174514</td>\n",
       "      <td>0.092894</td>\n",
       "      <td>-0.009273</td>\n",
       "      <td>-0.265906</td>\n",
       "      <td>-0.131924</td>\n",
       "      <td>-0.161570</td>\n",
       "      <td>-0.341431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027937</td>\n",
       "      <td>0.030910</td>\n",
       "      <td>0.619684</td>\n",
       "      <td>0.278698</td>\n",
       "      <td>-0.556750</td>\n",
       "      <td>0.100715</td>\n",
       "      <td>-0.504604</td>\n",
       "      <td>0.197664</td>\n",
       "      <td>-0.160889</td>\n",
       "      <td>-0.020245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080473</td>\n",
       "      <td>-0.403859</td>\n",
       "      <td>0.054638</td>\n",
       "      <td>-0.110568</td>\n",
       "      <td>0.197121</td>\n",
       "      <td>-0.138362</td>\n",
       "      <td>0.070910</td>\n",
       "      <td>-0.140718</td>\n",
       "      <td>0.170639</td>\n",
       "      <td>-0.120497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.373949</td>\n",
       "      <td>-0.285547</td>\n",
       "      <td>0.399565</td>\n",
       "      <td>0.015362</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>-0.152608</td>\n",
       "      <td>-0.271353</td>\n",
       "      <td>0.148875</td>\n",
       "      <td>0.536143</td>\n",
       "      <td>-0.274921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116648</td>\n",
       "      <td>0.076367</td>\n",
       "      <td>0.222625</td>\n",
       "      <td>0.152811</td>\n",
       "      <td>0.032114</td>\n",
       "      <td>0.141993</td>\n",
       "      <td>-0.044989</td>\n",
       "      <td>-0.173306</td>\n",
       "      <td>-0.469663</td>\n",
       "      <td>-0.072720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.357071</td>\n",
       "      <td>0.262387</td>\n",
       "      <td>0.379264</td>\n",
       "      <td>0.206661</td>\n",
       "      <td>-0.253764</td>\n",
       "      <td>-0.231780</td>\n",
       "      <td>-0.189448</td>\n",
       "      <td>0.087050</td>\n",
       "      <td>0.250881</td>\n",
       "      <td>0.023786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267398</td>\n",
       "      <td>0.018956</td>\n",
       "      <td>0.037620</td>\n",
       "      <td>-0.261148</td>\n",
       "      <td>-0.000223</td>\n",
       "      <td>-0.194334</td>\n",
       "      <td>-0.048889</td>\n",
       "      <td>-0.305128</td>\n",
       "      <td>-0.061272</td>\n",
       "      <td>-0.238710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.321742 -0.091622  1.020477 -0.105819 -0.154422  0.356765 -0.185670   \n",
       "1  0.299671  0.405244  0.184728 -0.109321 -0.225838 -0.050448 -0.330242   \n",
       "2  0.027937  0.030910  0.619684  0.278698 -0.556750  0.100715 -0.504604   \n",
       "3  0.373949 -0.285547  0.399565  0.015362  0.009504 -0.152608 -0.271353   \n",
       "4  0.357071  0.262387  0.379264  0.206661 -0.253764 -0.231780 -0.189448   \n",
       "\n",
       "          7         8         9    ...          190       191       192  \\\n",
       "0  0.368219  0.242798 -0.084168    ...     0.051760  0.202284  0.269600   \n",
       "1  0.002496  0.482650 -0.160271    ...     0.138463  0.477873  0.308865   \n",
       "2  0.197664 -0.160889 -0.020245    ...    -0.080473 -0.403859  0.054638   \n",
       "3  0.148875  0.536143 -0.274921    ...    -0.116648  0.076367  0.222625   \n",
       "4  0.087050  0.250881  0.023786    ...    -0.267398  0.018956  0.037620   \n",
       "\n",
       "        193       194       195       196       197       198       199  \n",
       "0  0.152225  0.290375  0.030409 -0.216377 -0.123192 -0.171273 -0.096611  \n",
       "1 -0.174514  0.092894 -0.009273 -0.265906 -0.131924 -0.161570 -0.341431  \n",
       "2 -0.110568  0.197121 -0.138362  0.070910 -0.140718  0.170639 -0.120497  \n",
       "3  0.152811  0.032114  0.141993 -0.044989 -0.173306 -0.469663 -0.072720  \n",
       "4 -0.261148 -0.000223 -0.194334 -0.048889 -0.305128 -0.061272 -0.238710  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_vec_weighted_df = pd.DataFrame(episode_vec_weighted)\n",
    "episode_vec_weighted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_vec_weighted_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make recomendation: find related episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user's strings: \n",
    "\n",
    "some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "user_requests = [\n",
    "    \"Could you recommend some episodes on decision tree and random forests?\",\n",
    "    \"Can you talk about Convolutional neural network and recurrent neural network?\",\n",
    "    \"Could you recommend some episodes on data science projects for beginners?\",\n",
    "    \"artificial intelligence\",\n",
    "    \"What can artificial intelligence do for human beings? What is the future of artificial intelligence?\",\n",
    "    \"What is natural language processing? \",\n",
    "    \"The error percentage of regression changes with change in the train and test data which I am deciding randomly. Cross validation can overcome this but how do I apply it for my regression model?\",\n",
    "    \"I have a precision recall curve for two separate algorithms. If I want to calculate the F-Measure I have to use the precision and recall values at a particular point on each curve. How is this point decided? For example on curve one there is a point where recall is 0.9 and precision is 0.87 and the other curve there is a point of recall at 0.95 and precision at 0.84. Alternatively, should I plot a F-measure curve for every precision recall value?\",\n",
    "    \"Suppose I want to make predictions of a response from predictors but I have some autocorrelation in the response variable. Under OLS this would be a problem as the residuals would have autocorrelation. What if I just want to predict the response and I use regularized least squares, like lasso or ridge or elastic net? I don't care about variances of the coefficients or anything of that nature as I'm not testing any hypotheses but I feel like I might be missing something.\",\n",
    "    \"Evaluating the quality of data.\",\n",
    "    \"Is there any episode on Facial Recognition? How does Facial Recognition work?\",\n",
    "    \"I am interested in knowing musical stuff.\",\n",
    "    \"What is the trend of big data? What is big data? How to learn big data?\",\n",
    "    \"How to learn machine learning? What books or website do you recommend?\",\n",
    "    \"Looking for projects on criminal analysis? \",\n",
    "    \"How to take advantage of Internet, computer,  cloud and other  platform in an effective way?\",\n",
    "    \"What are the most important knowledge in statistics or probability when doing machine learning?\"\n",
    "]  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_episode = episode_vec_weighted_df.values\n",
    "with open('some examples.txt', 'w') as f:\n",
    "    for j in range(len(user_requests)):\n",
    "        f.write(\"*****************************************************\" + \"\\n\")\n",
    "        user_request = user_requests[j]\n",
    "        user_request_corpus = gensim.utils.simple_preprocess(user_request)\n",
    "        X_user = vectorizer.fit_transform([\" \".join(user_request_corpus)])\n",
    "        f.write(str(X_user.shape) + \"\\n\")\n",
    "        user_weighted_vec = get_doc_weighted_vec(0, user_request_corpus , tf_idf = X_user, weighted = True)\n",
    "        cos_similarities = cosine_similarity(X=user_weighted_vec, Y=all_episode)\n",
    "\n",
    "        cos_similarities = cos_similarities[0]\n",
    "        cos_similarities.shape\n",
    "\n",
    "\n",
    "        most_similar = cos_similarities.argsort()[-4:][::-1]\n",
    "        f.write(str(most_similar) + \"\\n\")\n",
    "\n",
    "        threshold = 0.60\n",
    "        f.write(\"User's request is: \" + user_request + \"\\n\" )\n",
    "        for i in most_similar:\n",
    "\n",
    "            if cos_similarities[i] > threshold:\n",
    "                f.write(\"--------------------------\"+str(cos_similarities[i])+\"-----------------------------------\\n\")\n",
    "                f.write( \"\\n\")\n",
    "                f.write(str(descToTitle[descriptions[i]]) + \"\\n\")\n",
    "                f.write(str(descToLink[descriptions[i]]) + \"\\n\")\n",
    "                f.write(str(descriptions[i].encode('utf-8')) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the result at some example.txt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of all episode is handlable, let's have a look at the similarity between all episodes. By this, I also want to know the levels of the cosine similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 178)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = cosine_similarity(X=all_episode)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAFMCAYAAADcJFvDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCwsHERHJ8v/WiNh+zLS7VwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e6fd748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = plt.imshow(A[20:40,20:40])\n",
    "plt.colorbar(im)\n",
    "plt.show()\n",
    "\n",
    "# very diversity. so it is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what topics are interesting to you? I want to learn adboost tree, adboost tree, adboost tree.\n"
     ]
    }
   ],
   "source": [
    "user_request = input('what topics are interesting to you? ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. I want to learn adboost tree, adboost tree, adboost tree.\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello.\", user_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to-do: reorganize the code and write a function recommend_episode.\n",
    "\n",
    "def recommend_episode(string):\n",
    "    all_episode = episode_vec_weighted_df.values\n",
    "\n",
    "    \n",
    "    print(\"*****************************************************\" + \"\\n\")\n",
    "    user_request = string\n",
    "    user_request_corpus = gensim.utils.simple_preprocess(user_request)\n",
    "    X_user = vectorizer.fit_transform([\" \".join(user_request_corpus)])\n",
    "    #print(str(X_user.shape) + \"\\n\")\n",
    "    user_weighted_vec = get_doc_weighted_vec(0, user_request_corpus , tf_idf = X_user, weighted = True)\n",
    "    cos_similarities = cosine_similarity(X=user_weighted_vec, Y=all_episode)\n",
    "\n",
    "    cos_similarities = cos_similarities[0]\n",
    "    cos_similarities.shape\n",
    "\n",
    "\n",
    "    most_similar = cos_similarities.argsort()[-4:][::-1]\n",
    "    #print(str(most_similar) + \"\\n\")\n",
    "\n",
    "    threshold = 0.60\n",
    "    print(\"User's request is: \" + user_request + \"\\n\" )\n",
    "    for i in most_similar:\n",
    "\n",
    "        if cos_similarities[i] > threshold:\n",
    "            print(\"--------------------The episode has cosine similarity is \"+str(cos_similarities[i])+\" with user's request-------------------------\\n\")\n",
    "            print( \"\\n\")\n",
    "            print(str(descToTitle[descriptions[i]]) + \"\\n\")\n",
    "            print(str(descToLink[descriptions[i]]) + \"\\n\")\n",
    "            print(str(descriptions[i].encode('utf-8')) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************\n",
      "\n",
      "User's request is: I want to learn adboost tree, adboost tree, adboost tree.\n",
      "\n",
      "--------------------The episode has cosine similarity is 0.757532381884 with user's request-------------------------\n",
      "\n",
      "\n",
      "\n",
      "[MINI] Decision Tree Learning\n",
      "\n",
      "http://dataskeptic.com/epnotes/decision-tree-learning.php\n",
      "\n",
      "b'Linhda and Kyle talk about Decision Tree Learning in this miniepisode.  Decision Tree Learning is the algorithmic process of trying to generate an optimal decision tree to properly classify or forecast some future unlabeled element based by following each step in the tree.'\n",
      "\n",
      "--------------------The episode has cosine similarity is 0.601830622663 with user's request-------------------------\n",
      "\n",
      "\n",
      "\n",
      "[MINI] Gini Coefficients\n",
      "\n",
      "http://dataskeptic.com/blog/episodes/2016/gini-coefficient\n",
      "\n",
      "b'The Gini Coefficient (as it relates to decision trees) is one approach to determining the optimal decision to introduce which splits your dataset as part of a decision tree. To pick the right feature to split on, it considers the frequency of the values of that feature and how well the values correlate with specific outcomes that you are trying to predict.'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('title', 'Introduction'),\n",
       "             ('pubDate', 'Fri, 23 May 2014 10:00:00 +0000'),\n",
       "             ('guid',\n",
       "              OrderedDict([('@isPermaLink', 'false'),\n",
       "                           ('#text', '80d137c5e4a81e2b34752f33db7c03c0')])),\n",
       "             ('link', 'http://dataskeptic.com/epnotes/ep001.php'),\n",
       "             ('itunes:image',\n",
       "              OrderedDict([('@href',\n",
       "                            'http://static.libsyn.com/p/assets/1/1/a/6/11a6925ea0f4027a/introduction.png')])),\n",
       "             ('description',\n",
       "              '<p>The Data Skeptic Podcast features conversations with topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the veracity of claims and efficacy of approaches.</p>\\n<p>This first episode is a\\xa0short discussion about what this podcast is all about.</p>'),\n",
       "             ('content:encoded',\n",
       "              '<p>The Data Skeptic Podcast features conversations with topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the veracity of claims and efficacy of approaches.</p>\\n<p>This first episode is a\\xa0short discussion about what this podcast is all about.</p>'),\n",
       "             ('enclosure',\n",
       "              OrderedDict([('@length', '3481373'),\n",
       "                           ('@type', 'audio/mpeg'),\n",
       "                           ('@url',\n",
       "                            'http://traffic.libsyn.com/dataskeptic/Data_Skeptic_Podcast_ep000_-_Introduction.mp3?dest-id=201630')])),\n",
       "             ('itunes:duration', '03:56'),\n",
       "             ('itunes:explicit', 'clean'),\n",
       "             ('itunes:keywords', 'science,data,skepticism'),\n",
       "             ('itunes:subtitle',\n",
       "              'The Data Skeptic Podcast features conversations with topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the...')])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_episode(user_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'adboost' in vocab # so no matter how many times the word 'adboost' is in the string, it won't find "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
