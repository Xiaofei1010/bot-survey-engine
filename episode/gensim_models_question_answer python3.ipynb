{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "import heapq\n",
    "import pickle\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2010-07-19T19:12:12.510</td>\n",
       "      <td>36</td>\n",
       "      <td>2577.0</td>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-09-15T21:08:26.077</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>&lt;bayesian&gt;&lt;prior&gt;&lt;elicitation&gt;</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19T19:12:57.157</td>\n",
       "      <td>29</td>\n",
       "      <td>23368.0</td>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2016-06-27T06:44:40.147</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>&lt;distributions&gt;&lt;normality&gt;</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19T19:13:28.577</td>\n",
       "      <td>66</td>\n",
       "      <td>5792.0</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2013-05-27T14:48:36.927</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>&lt;software&gt;&lt;open-source&gt;</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>2010-07-19T19:13:31.617</td>\n",
       "      <td>17</td>\n",
       "      <td>26414.0</td>\n",
       "      <td>I have two groups of data.  Each with a differ...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-09-08T03:00:19.690</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>&lt;distributions&gt;&lt;statistical-significance&gt;</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19T19:14:43.050</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The R-project\\n\\nhttp://www.r-project.org/\\n\\n...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19T19:21:15.063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  PostTypeId  ParentId  AcceptedAnswerId             CreationDate  Score  \\\n",
       "0   1           1       NaN              15.0  2010-07-19T19:12:12.510     36   \n",
       "1   2           1       NaN              59.0  2010-07-19T19:12:57.157     29   \n",
       "2   3           1       NaN               5.0  2010-07-19T19:13:28.577     66   \n",
       "3   4           1       NaN             135.0  2010-07-19T19:13:31.617     17   \n",
       "4   5           2       3.0               NaN  2010-07-19T19:14:43.050     87   \n",
       "\n",
       "   ViewCount                                               Body  OwnerUserId  \\\n",
       "0     2577.0  How should I elicit prior distributions from e...          8.0   \n",
       "1    23368.0  In many different statistical methods there is...         24.0   \n",
       "2     5792.0  What are some valuable Statistical Analysis op...         18.0   \n",
       "3    26414.0  I have two groups of data.  Each with a differ...         23.0   \n",
       "4        NaN  The R-project\\n\\nhttp://www.r-project.org/\\n\\n...         23.0   \n",
       "\n",
       "          LastActivityDate                                              Title  \\\n",
       "0  2010-09-15T21:08:26.077                      Eliciting priors from experts   \n",
       "1  2016-06-27T06:44:40.147                                 What is normality?   \n",
       "2  2013-05-27T14:48:36.927  What are some valuable Statistical Analysis op...   \n",
       "3  2010-09-08T03:00:19.690  Assessing the significance of differences in d...   \n",
       "4  2010-07-19T19:21:15.063                                                NaN   \n",
       "\n",
       "                                        Tags  AnswerCount  CommentCount  \\\n",
       "0             <bayesian><prior><elicitation>          5.0             1   \n",
       "1                 <distributions><normality>          7.0             1   \n",
       "2                    <software><open-source>         19.0             4   \n",
       "3  <distributions><statistical-significance>          5.0             2   \n",
       "4                                        NaN          NaN             3   \n",
       "\n",
       "   FavoriteCount  \n",
       "0           23.0  \n",
       "1           10.0  \n",
       "2           39.0  \n",
       "3            5.0  \n",
       "4            NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "post_df = pd.read_csv('all_posts.csv', sep = \"\\t\")\n",
    "post_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215957</th>\n",
       "      <td>299976</td>\n",
       "      <td>2</td>\n",
       "      <td>299970.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-27T01:55:50.117</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n  We cannot simulate separate values (becaus...</td>\n",
       "      <td>8336.0</td>\n",
       "      <td>2017-08-27T01:55:50.117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215958</th>\n",
       "      <td>299977</td>\n",
       "      <td>2</td>\n",
       "      <td>299963.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-27T02:07:55.553</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neural networks could \"learn\" where the labels...</td>\n",
       "      <td>30621.0</td>\n",
       "      <td>2017-08-27T02:07:55.553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215959</th>\n",
       "      <td>299978</td>\n",
       "      <td>2</td>\n",
       "      <td>299669.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-27T03:26:04.497</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calculating $\\displaystyle\\hat{t}_i=\\int t\\, p...</td>\n",
       "      <td>8336.0</td>\n",
       "      <td>2017-08-27T03:26:04.497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215960</th>\n",
       "      <td>299979</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-27T03:52:26.230</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I have collected primary data of BISP with hel...</td>\n",
       "      <td>175092.0</td>\n",
       "      <td>2017-08-27T03:52:26.230</td>\n",
       "      <td>What should be the solution of insignificant (...</td>\n",
       "      <td>&lt;statistical-significance&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215961</th>\n",
       "      <td>299980</td>\n",
       "      <td>2</td>\n",
       "      <td>299446.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-27T04:05:22.103</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>While it's possible to combine word embeddings...</td>\n",
       "      <td>107579.0</td>\n",
       "      <td>2017-08-27T04:05:22.103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  PostTypeId  ParentId  AcceptedAnswerId  \\\n",
       "215957  299976           2  299970.0               NaN   \n",
       "215958  299977           2  299963.0               NaN   \n",
       "215959  299978           2  299669.0               NaN   \n",
       "215960  299979           1       NaN               NaN   \n",
       "215961  299980           2  299446.0               NaN   \n",
       "\n",
       "                   CreationDate  Score  ViewCount  \\\n",
       "215957  2017-08-27T01:55:50.117      0        NaN   \n",
       "215958  2017-08-27T02:07:55.553      0        NaN   \n",
       "215959  2017-08-27T03:26:04.497      0        NaN   \n",
       "215960  2017-08-27T03:52:26.230      0        2.0   \n",
       "215961  2017-08-27T04:05:22.103      0        NaN   \n",
       "\n",
       "                                                     Body  OwnerUserId  \\\n",
       "215957  \\n  We cannot simulate separate values (becaus...       8336.0   \n",
       "215958  Neural networks could \"learn\" where the labels...      30621.0   \n",
       "215959  Calculating $\\displaystyle\\hat{t}_i=\\int t\\, p...       8336.0   \n",
       "215960  I have collected primary data of BISP with hel...     175092.0   \n",
       "215961  While it's possible to combine word embeddings...     107579.0   \n",
       "\n",
       "               LastActivityDate  \\\n",
       "215957  2017-08-27T01:55:50.117   \n",
       "215958  2017-08-27T02:07:55.553   \n",
       "215959  2017-08-27T03:26:04.497   \n",
       "215960  2017-08-27T03:52:26.230   \n",
       "215961  2017-08-27T04:05:22.103   \n",
       "\n",
       "                                                    Title  \\\n",
       "215957                                                NaN   \n",
       "215958                                                NaN   \n",
       "215959                                                NaN   \n",
       "215960  What should be the solution of insignificant (...   \n",
       "215961                                                NaN   \n",
       "\n",
       "                              Tags  AnswerCount  CommentCount  FavoriteCount  \n",
       "215957                         NaN          NaN             0            NaN  \n",
       "215958                         NaN          NaN             0            NaN  \n",
       "215959                         NaN          NaN             0            NaN  \n",
       "215960  <statistical-significance>          0.0             0            NaN  \n",
       "215961                         NaN          NaN             0            NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215962"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = post_df.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_df['Id'].unique()) == post_df.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107008"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "215962 - 108954"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    108954\n",
       "2    104797\n",
       "5      1091\n",
       "4      1091\n",
       "6        18\n",
       "3         6\n",
       "7         5\n",
       "Name: PostTypeId, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df['PostTypeId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108954, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Type1 = post_df.loc[post_df['PostTypeId'] == 1]\n",
    "Type1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108937"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Type1['Body'].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Type1['Body'].isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So the unique values of \"Body\" is less than the number of questions. This is because some asked their\n",
    "questions more than once and maybe the titles are different, but the contents are exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Confirm answers to questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the answers to question_id = 1 online and get the rows from the table to check that their parentId is indeed 1. \n",
    "But only on answer is accepted and is called accepted_answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19T19:19:46.160</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Cook gives some interesting recommendatio...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2010-07-19T19:19:46.160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  PostTypeId  ParentId  AcceptedAnswerId             CreationDate  \\\n",
       "14  15           2       1.0               NaN  2010-07-19T19:19:46.160   \n",
       "\n",
       "    Score  ViewCount                                               Body  \\\n",
       "14     17        NaN  John Cook gives some interesting recommendatio...   \n",
       "\n",
       "    OwnerUserId         LastActivityDate Title Tags  AnswerCount  \\\n",
       "14          6.0  2010-07-19T19:19:46.160   NaN  NaN          NaN   \n",
       "\n",
       "    CommentCount  FavoriteCount  \n",
       "14             0            NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = 'John Cook gives some interesting recommendations.' # answer to question id = 1\n",
    "post_df.loc[post_df['Body'].apply(lambda x: answer in str(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>154</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19T22:40:47.947</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am currently researching the trial roulette ...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>2010-09-03T17:46:44.017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  PostTypeId  ParentId  AcceptedAnswerId             CreationDate  \\\n",
       "142  154           2       1.0               NaN  2010-07-19T22:40:47.947   \n",
       "\n",
       "     Score  ViewCount                                               Body  \\\n",
       "142     25        NaN  I am currently researching the trial roulette ...   \n",
       "\n",
       "     OwnerUserId         LastActivityDate Title Tags  AnswerCount  \\\n",
       "142        108.0  2010-09-03T17:46:44.017   NaN  NaN          NaN   \n",
       "\n",
       "     CommentCount  FavoriteCount  \n",
       "142             2            NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = 'Experts are given counters (or what one can think of as casino chips) representing equal densities whose total would sum up' # answer to question id = 1\n",
    "post_df.loc[post_df['Body'].apply(lambda x: answer in str(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are PostTypeIds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only post with postTypeId = 1 have titles. Because only questions have titles and answers don't have title.  Or if PostTypeId != 1, then title is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(post_df[\"Title\"].isnull()) + 108954 == post_df.shape[0] # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A guess: if PostTypeId is not 1 and 2, then they are answers and they are not accepted answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "acceptedAnswerId = post_df['AcceptedAnswerId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#34167 answers are accepted.\n",
    "len(acceptedAnswerId)\n",
    "not_accepted = post_df.loc[post_df['Id'].apply(lambda x: x not in acceptedAnswerId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 7, 5, 4, 6, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_accepted['PostTypeId'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So PostTypeId is not related to whether an answer is accepted or not.\n",
    "\n",
    "** What is it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2182    Use this tag for any *on-topic* question that ...\n",
       "2264    Mixed (aka multilevel or hierarchical) models ...\n",
       "2419    Psychometrics has evolved as a subfield of psy...\n",
       "2778    Model selection is a problem of judging which ...\n",
       "2780    Cluster analysis is the task of partitioning d...\n",
       "2782    Time series are data observed over time (eithe...\n",
       "2784    Hypothesis testing assesses whether data suppo...\n",
       "2935    Prediction of the future events. It is a speci...\n",
       "3714    Stata is a proprietary  cross-platform general...\n",
       "5647    IBM SPSS Statistics (formerly SPSS, i.e. \"Stat...\n",
       "Name: Body, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.loc[post_df['PostTypeId'] == 4]['Body'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question posts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>I have two groups of data.  Each with a differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>Last year, I read a blog post from Brendan O'C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                              Title  \\\n",
       "0   1                      Eliciting priors from experts   \n",
       "1   2                                 What is normality?   \n",
       "2   3  What are some valuable Statistical Analysis op...   \n",
       "3   4  Assessing the significance of differences in d...   \n",
       "5   6  The Two Cultures: statistics vs. machine learn...   \n",
       "\n",
       "                                                Body  \n",
       "0  How should I elicit prior distributions from e...  \n",
       "1  In many different statistical methods there is...  \n",
       "2  What are some valuable Statistical Analysis op...  \n",
       "3  I have two groups of data.  Each with a differ...  \n",
       "5  Last year, I read a blog post from Brendan O'C...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_df = post_df.loc[post_df['PostTypeId'] == 1][['Id', 'Title','Body']]\n",
    "\n",
    "Q_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The R-project\\n\\nhttp://www.r-project.org/\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Incanter is a Clojure-based, R-like platform (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>See my response to \"Datasets for Running Stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Machine Learning seems to have its basis in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>I second that Jay. Why is R valuable? Here's a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                               Body\n",
       "4    5  The R-project\\n\\nhttp://www.r-project.org/\\n\\n...\n",
       "8    9  Incanter is a Clojure-based, R-like platform (...\n",
       "11  12  See my response to \"Datasets for Running Stati...\n",
       "12  13  Machine Learning seems to have its basis in th...\n",
       "13  14  I second that Jay. Why is R valuable? Here's a..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_df =  post_df.loc[post_df['PostTypeId'] != 1][['Id', 'Body']]\n",
    "A_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original post data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     How should I elicit prior distributions from e...\n",
      "1     In many different statistical methods there is...\n",
      "2     What are some valuable Statistical Analysis op...\n",
      "3     I have two groups of data.  Each with a differ...\n",
      "5     Last year, I read a blog post from Brendan O'C...\n",
      "6     I've been working on a new method for analyzin...\n",
      "7     Sorry, but the emptyness was a bit overwhelmin...\n",
      "9     Many studies in the social sciences use Likert...\n",
      "10    Is there a good, modern treatment covering the...\n",
      "16    I have four competing models which I use to pr...\n",
      "Name: Body, dtype: object\n",
      "4     The R-project\\n\\nhttp://www.r-project.org/\\n\\n...\n",
      "8     Incanter is a Clojure-based, R-like platform (...\n",
      "11    See my response to \"Datasets for Running Stati...\n",
      "12    Machine Learning seems to have its basis in th...\n",
      "13    I second that Jay. Why is R valuable? Here's a...\n",
      "14    John Cook gives some interesting recommendatio...\n",
      "15    Two projects spring to mind:\\n\\n\\nBugs - takin...\n",
      "17    Also see the UCI machine learning Data Reposit...\n",
      "18    Gapminder has a number (430 at the last look) ...\n",
      "19    The assumption of normality assumes your data ...\n",
      "Name: Body, dtype: object\n",
      "0                         Eliciting priors from experts\n",
      "1                                    What is normality?\n",
      "2     What are some valuable Statistical Analysis op...\n",
      "3     Assessing the significance of differences in d...\n",
      "5     The Two Cultures: statistics vs. machine learn...\n",
      "6                Locating freely available data samples\n",
      "7     So how many staticians *does* it take to screw...\n",
      "9     Under what conditions should Likert scales be ...\n",
      "10                Multivariate Interpolation Approaches\n",
      "16               How can I adapt ANOVA for binary data?\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "original_q_posts = Q_df['Body']\n",
    "original_a_posts = A_df['Body']\n",
    "\n",
    "original_q_titles = Q_df['Title']\n",
    "\n",
    "\n",
    "print(original_q_posts[0:10])\n",
    "print(original_a_posts[0:10])\n",
    "\n",
    "print(original_q_titles[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108954,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_q_titles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original_q_titles is series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "thefile = open('./text/questions_body.txt', 'w')\n",
    "for post in original_q_posts:\n",
    "    post = str(post).replace('\\n', ' ')\n",
    "    thefile.write(\"%s\\n\" % post)\n",
    "\n",
    "thefile = open('./text/answers_body.txt', 'w')\n",
    "for post in original_a_posts:\n",
    "    post = str(post).replace('\\n', ' ')\n",
    "    thefile.write(\"%s\\n\" % post)\n",
    "\n",
    "thefile = open('./text/questions_title.txt', 'w')\n",
    "for i, post in enumerate(original_q_titles):\n",
    "    post = \"*\"+ str(i+1) + str(post).replace('\\n', \"\") \n",
    "    thefile.write(\"%s\\n\" % post)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_work_status(singleCount, totalCount, currentCount = 0):\n",
    "    currentCount += singleCount\n",
    "    percentage = currentCount/totalCount *100\n",
    "    status = \">\" * int(percentage) +  \" \" * (100-int(percentage))\n",
    "    sys.stdout.write('\\rStatus:[{0}] {1:.2f}%'.format(status, percentage))\n",
    "    sys.stdout.flush()\n",
    "    if percentage >= 100:\n",
    "    \tprint('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                \n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "                #This lowercases, tokenizes, de-accents (optional). – the output are final tokens = unicode strings, that won’t be processed any further.\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It takes some time.\n",
    "fname = './text/'\n",
    "word2vec_question_title_corpus = list(read_corpus(fname + \"questions_title.txt\", tokens_only = True ))\n",
    "word2vec_question_corpus = list(read_corpus(fname + \"questions_body.txt\", tokens_only = True ))\n",
    "word2vec_answer_corpus = list(read_corpus(fname + \"answers_body.txt\", tokens_only = True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['how', 'should', 'elicit', 'prior', 'distributions', 'from', 'experts', 'when', 'fitting', 'bayesian', 'model'], ['in', 'many', 'different', 'statistical', 'methods', 'there', 'is', 'an', 'assumption', 'of', 'normality', 'what', 'is', 'normality', 'and', 'how', 'do', 'know', 'if', 'there', 'is', 'normality'], ['what', 'are', 'some', 'valuable', 'statistical', 'analysis', 'open', 'source', 'projects', 'available', 'right', 'now', 'edit', 'as', 'pointed', 'out', 'by', 'sharpie', 'valuable', 'could', 'mean', 'helping', 'you', 'get', 'things', 'done', 'faster', 'or', 'more', 'cheaply']]\n",
      "[['the', 'project', 'http', 'www', 'project', 'org', 'is', 'valuable', 'and', 'significant', 'because', 'it', 'was', 'the', 'first', 'widely', 'accepted', 'open', 'source', 'alternative', 'to', 'big', 'box', 'packages', 'it', 'mature', 'well', 'supported', 'and', 'standard', 'within', 'many', 'scientific', 'communities', 'some', 'reasons', 'why', 'it', 'is', 'useful', 'and', 'valuable', 'there', 'are', 'some', 'nice', 'tutorials', 'here'], ['incanter', 'is', 'clojure', 'based', 'like', 'platform', 'environment', 'libraries', 'for', 'statistical', 'computing', 'and', 'graphics'], ['see', 'my', 'response', 'to', 'datasets', 'for', 'running', 'statistical', 'analysis', 'on', 'in', 'reference', 'to', 'datasets', 'in']]\n",
      "[['eliciting', 'priors', 'from', 'experts'], ['what', 'is', 'normality'], ['what', 'are', 'some', 'valuable', 'statistical', 'analysis', 'open', 'source', 'projects']]\n",
      "108954\n",
      "107008\n",
      "108854\n",
      "(108954, 3)\n",
      "(107008, 2)\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_question_corpus[0:3])\n",
    "print(word2vec_answer_corpus[0:3])\n",
    "print(word2vec_question_title_corpus[0:3])\n",
    "\n",
    "print(len(word2vec_question_corpus))\n",
    "print(len(word2vec_answer_corpus))\n",
    "print(len(word2vec_question_title_corpus))\n",
    "\n",
    "print(Q_df.shape)\n",
    "print(A_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for word2vec models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 100.  200.]\n",
      "[ 6.]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "sizes = np.linspace(100, 200, num = 2)\n",
    "print(sizes)\n",
    "windows = np.linspace(6, 6,num = 1 )\n",
    "print(windows)\n",
    "min_counts = [2]\n",
    "print(min_counts )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Later make it more dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After tuning the model and find appropriate parameters, save them in the config file and read from the config file. \n",
    "# paras = json.loads(open('./configure/word2vec.json').read())\n",
    "# print(paras)\n",
    "\n",
    "# min_count = paras['min_count']\n",
    "# size = paras['size']\n",
    "# window = paras['window']\n",
    "# workers = paras['workers']\n",
    "# sg = paras['sg']\n",
    "# alpha = paras[\"alpha\"]\n",
    "# hs = paras[\"hs\"]\n",
    "# negative = paras[\"negative\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: sentence: a list of lists of words.\n",
    "# paras is a dictionary{ para: value, para: value}\n",
    "\n",
    "def train_word_model(sentences, modelname, **paras):\n",
    "    min_count = paras['min_count']\n",
    "    size = paras['size']\n",
    "    window = paras['window']\n",
    "    model = gensim.models.Word2Vec(sentences, min_count = min_count, size = size, window = window, workers = 4)\n",
    "    fname = './model/'+ modelname\n",
    "    model.save(fname)\n",
    "    return model\n",
    "\n",
    "def filter_corpus(sentences,model, doc = 0):\n",
    "    vocab = list(model.wv.vocab.keys())\n",
    "    print(vocab[0:10])\n",
    "    filtered_sentences = []\n",
    "    i = 0\n",
    "    total_count = len(sentences)\n",
    "    current_count = 0\n",
    "    for sentence in sentences:\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            show_work_status(1000, total_count, current_count)\n",
    "            current_count += 1000\n",
    "        if doc == 0:\n",
    "            words = list(filter(lambda x: x in vocab, sentence))\n",
    "            #words = [word for word in words]\n",
    "        if doc == 1:\n",
    "            words = list(filter(lambda x: x in vocab, sentence.words))\n",
    "            #words = [word for word in words]\n",
    "        filtered_sentences.append(words)\n",
    "    return filtered_sentences\n",
    "\n",
    "\n",
    "def glance_word(model):\n",
    "    print(model)\n",
    "    print('*************most similar words to \\'vector\\'***************')\n",
    "    print(model.most_similar('vector'))\n",
    "    print('\\n')\n",
    "    print(\"**************Similarity of \\'probability\\' and \\'distribution\\'******************\")\n",
    "    print(model.similarity('probability','distribution'))\n",
    "    print('\\n')\n",
    "    print(\"**************Similarity of \\'gaussian\\' and \\'normal\\'******************\")\n",
    "    print(model.similarity('gaussian','normal'))\n",
    "    print('\\n')\n",
    "\n",
    "# topk pick the top k similar words.    \n",
    "\n",
    "# todo: filter_corpus is not related to model. It is only related to min_count. So it should be take out from evaluate_word_model.\n",
    "\n",
    "def evaluate_word_model(model,topk, min_count):\n",
    "    n =len(word2vec_question_corpus)\n",
    "    random.seed(2017)\n",
    "    doc_id = random.randint(0,n)\n",
    "    # pick up a question randomly and find similar questions.\n",
    "    if min_count == 1: # all words are in the vocab of the model.\n",
    "        word2vec_question_corpus_filter = word2vec_question_corpus\n",
    "    else:\n",
    "        word2vec_question_corpus_filter = filter_corpus(word2vec_question_corpus, model) \n",
    "    sen_interest = word2vec_question_corpus_filter[doc_id]\n",
    "    print('filtering is done.')\n",
    "    sims = []\n",
    "    current_n = 0\n",
    "    for i in range(int(len(Q_df['Body'])/1000)): # only compared with the first n questions. \n",
    "        compared_sen = word2vec_question_corpus_filter[i]\n",
    "        sim = model.wv.n_similarity(sen_interest, compared_sen)\n",
    "        sims.append(sim)\n",
    "    most_similar_index = heapq.nlargest(topk, range(len(sims)), key=sims.__getitem__)\n",
    "    print('*****************The question we are interested in is: ************************')\n",
    "    print(Q_df.iloc[doc_id]['Body'])\n",
    "    print(\"*******************Similar questions are **********************\")\n",
    "    for i in most_similar_index :\n",
    "        print(i)\n",
    "        print('similarity is ', sims[i])\n",
    "        print(Q_df.iloc[i]['Body'])\n",
    "        print(\"**********************\")\n",
    "    return sims, doc_id\n",
    "\n",
    "# word_vec_dic is a dictionary that stores the vectors of all words in the vocab of the word2vec model.\n",
    "def save_vectors(model):\n",
    "    word_vec_dic = {}\n",
    "    vocab = model.wv.vocab.keys()\n",
    "    for word in vocab:\n",
    "        word_vec_dic[word] = model[word]\n",
    "    return word_vec_dic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training  models or loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning parameters on grids\n",
    "# train the models.\n",
    "# It takes a while. So we only train it once and next time, load the models using the code in the next block.\n",
    "models = {}\n",
    "for size in sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            paras =  {'size':int(size), 'window' : int(window), \"min_count\" :int(min_count)}\n",
    "            model_name = 'word2vector_model_question_answer_' + str(int(size))+\"_\"+str(int(window))+\"_\"+str(int(min_count))\n",
    "            models[model_name] = train_word_model(word2vec_question_corpus+word2vec_answer_corpus,model_name, **paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # load models\n",
    "# models = {}\n",
    "# for size in sizes:\n",
    "#     for window in windows:\n",
    "#         for min_count in min_counts:\n",
    "#             paras =  {'size':int(size), 'window' : int(window), \"min_count\" :int(min_count)}\n",
    "#             model_name = \"word2vector_model_question_answer_\"+str(int(size))+\"_\"+str(int(window))+\"_\"+str(int(min_count))\n",
    "#             models[model_name] = Word2Vec.load( './model/'+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=100269, size=100, alpha=0.025)\n",
      "*************most similar words to vector***************\n",
      "[('vectors', 0.6975052952766418), ('scalar', 0.6546276807785034), ('matrix', 0.6162911057472229), ('mathbf', 0.6119372844696045), ('element', 0.6053691506385803), ('d_y', 0.5990526080131531), ('tuple', 0.589658260345459), ('concatenation', 0.5894056558609009), ('column', 0.5801124572753906), ('input', 0.5794710516929626)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.474026335809\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.700982417219\n",
      "\n",
      "\n",
      "['how', 'should', 'elicit', 'prior', 'distributions', 'from', 'experts', 'when', 'fitting', 'bayesian']\n",
      "Status:[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 99.12%filtering is done.\n",
      "*****************The question we are interested in is: ************************\n",
      "Let $F:(-\\infty,\\infty)\\rightarrow[0,1]$ and $G:(-\\infty,\\infty)\\rightarrow[0,1]$ be two CDFs with PDFs $f$ and $g$, respectively. Is there a connection/inequality between:\n",
      "\n",
      "$$d_1 = \\int_{-\\infty}^{\\infty}\\vert f(t) - g(t) \\vert dt,$$\n",
      "and\n",
      "$$d_2 = \\int_{-\\infty}^{\\infty}\\vert F(t) - G(t) \\vert dt?$$\n",
      "Assuming $d_2$ exists ($d_1$ is always finite).\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "51\n",
      "similarity is  0.679306056146\n",
      "I know this must be standard material, but I had difficulty in finding a proof in this form.\n",
      "\n",
      "Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.\n",
      "\n",
      "Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let\n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\n",
      "\\bar e &amp;= v - X\\bar y\n",
      "\\end{align}\\right.\\quad.$$\n",
      "\n",
      "If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and \n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "u &amp;= c^T\\bar y\\\\\n",
      "s^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\n",
      "\\end{align}\\right.\\quad,$$\n",
      "\n",
      "then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.\n",
      "\n",
      "I would be grateful if you could provide an outline for its proof.\n",
      "\n",
      "**********************\n",
      "71\n",
      "similarity is  0.588967358647\n",
      "If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?\n",
      "\n",
      "**********************\n",
      "12\n",
      "similarity is  0.557021421931\n",
      "How can I find the PDF (probability density function) of a distribution given the CDF (cumulative distribution function)?\n",
      "\n",
      "**********************\n",
      "Word2Vec(vocab=100269, size=200, alpha=0.025)\n",
      "*************most similar words to vector***************\n",
      "[('vectors', 0.6750472187995911), ('matrix', 0.5586477518081665), ('scalar', 0.557653546333313), ('mathbf', 0.5509334206581116), ('column', 0.5177068114280701), ('nx', 0.505388081073761), ('tuple', 0.5049560070037842), ('concatenation', 0.5007933974266052), ('d_y', 0.4893472194671631), ('orthonormal', 0.48638349771499634)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.409715330771\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.660268612203\n",
      "\n",
      "\n",
      "['how', 'should', 'elicit', 'prior', 'distributions', 'from', 'experts', 'when', 'fitting', 'bayesian']\n",
      "Status:[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ] 99.12%filtering is done.\n",
      "*****************The question we are interested in is: ************************\n",
      "Let $F:(-\\infty,\\infty)\\rightarrow[0,1]$ and $G:(-\\infty,\\infty)\\rightarrow[0,1]$ be two CDFs with PDFs $f$ and $g$, respectively. Is there a connection/inequality between:\n",
      "\n",
      "$$d_1 = \\int_{-\\infty}^{\\infty}\\vert f(t) - g(t) \\vert dt,$$\n",
      "and\n",
      "$$d_2 = \\int_{-\\infty}^{\\infty}\\vert F(t) - G(t) \\vert dt?$$\n",
      "Assuming $d_2$ exists ($d_1$ is always finite).\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "51\n",
      "similarity is  0.664469660789\n",
      "I know this must be standard material, but I had difficulty in finding a proof in this form.\n",
      "\n",
      "Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.\n",
      "\n",
      "Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let\n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\n",
      "\\bar e &amp;= v - X\\bar y\n",
      "\\end{align}\\right.\\quad.$$\n",
      "\n",
      "If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and \n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "u &amp;= c^T\\bar y\\\\\n",
      "s^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\n",
      "\\end{align}\\right.\\quad,$$\n",
      "\n",
      "then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.\n",
      "\n",
      "I would be grateful if you could provide an outline for its proof.\n",
      "\n",
      "**********************\n",
      "71\n",
      "similarity is  0.581837835786\n",
      "If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?\n",
      "\n",
      "**********************\n",
      "12\n",
      "similarity is  0.557554130965\n",
      "How can I find the PDF (probability density function) of a distribution given the CDF (cumulative distribution function)?\n",
      "\n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "# # It is slow to run evaluate_word_model function. or slow to run filter function.\n",
    "# topk = 3\n",
    "# for size in sizes:\n",
    "#     for window in windows:\n",
    "#         for min_count in min_counts:\n",
    "#             paras =  {'size':int(size), 'window' : int(window), \"min_count\" :int(min_count)}\n",
    "#             model_name = 'word2vector_model_question_answer_' + str(int(size))+\"_\"+str(int(window))+\"_\"+str(int(min_count))\n",
    "#             model = models[model_name] \n",
    "#             glance_word(model)\n",
    "#             evaluate_word_model(model,topk, min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['word2vector_model_question_answer_100_6_2', 'word2vector_model_question_answer_200_6_2'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word vectors for all models in model_word_vec_dic, model_word_vec_df and also in csv form.\n",
    "model_word_vec_dic = {}\n",
    "model_word_vec_df = {}\n",
    "vocabs = {}\n",
    "for key in models.keys():\n",
    "    model_word_vec_dic[key] = save_vectors(models[key])\n",
    "    model_word_vec_df[key] = pd.DataFrame(model_word_vec_dic[key]).T\n",
    "    model_word_vec_df[key].to_csv('./word_vec/'+key+\".csv\")\n",
    "    vocabs[key] =  list(model_word_vec_df[key].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   0         1         2         3         4         5   \\\n",
      "a_           0.378482  0.663518 -0.521119  0.131379 -0.563702 -1.411734   \n",
      "a__         -0.042884  0.034728 -0.071833 -0.028312  0.015510 -0.055432   \n",
      "a_a          0.052079  0.142021 -0.020253 -0.078256 -0.213958  0.062145   \n",
      "a_adjusted   0.003180  0.035082 -0.092537  0.009307 -0.053584 -0.052832   \n",
      "a_after_est -0.023196  0.026089 -0.051318 -0.024839 -0.008315 -0.033886   \n",
      "\n",
      "                   6         7         8         9     ...           90  \\\n",
      "a_          -1.031343  2.487007  0.574007 -1.922254    ...     1.142844   \n",
      "a__         -0.068161 -0.028867 -0.106915 -0.062627    ...    -0.021886   \n",
      "a_a         -0.212747  0.185872 -0.026197 -0.236039    ...    -0.036024   \n",
      "a_adjusted  -0.094893 -0.048316 -0.059345 -0.095425    ...    -0.038192   \n",
      "a_after_est -0.032286 -0.046486 -0.053690 -0.029133    ...    -0.038316   \n",
      "\n",
      "                   91        92        93        94        95        96  \\\n",
      "a_          -1.599426 -1.063837 -1.416399  1.700068 -0.254143  0.369492   \n",
      "a__          0.042544 -0.031846  0.024131 -0.068432  0.021333 -0.076932   \n",
      "a_a          0.078013  0.078061  0.048084  0.161128  0.063395 -0.043900   \n",
      "a_adjusted   0.045422  0.044761  0.036319 -0.025189  0.022936 -0.095712   \n",
      "a_after_est  0.039434 -0.030298  0.018142 -0.020161 -0.002346 -0.032038   \n",
      "\n",
      "                   97        98        99  \n",
      "a_          -0.583022 -2.328119 -0.666974  \n",
      "a__         -0.018687 -0.065316  0.025734  \n",
      "a_a         -0.156414 -0.090219 -0.153035  \n",
      "a_adjusted  -0.071282 -0.066927  0.023640  \n",
      "a_after_est -0.052283 -0.039752  0.022703  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "(100269, 100)\n",
      "['a_', 'a__', 'a_a', 'a_adjusted', 'a_after_est', 'a_and_b', 'a_anom', 'a_answers', 'a_arclength', 'a_b']\n"
     ]
    }
   ],
   "source": [
    "print(model_word_vec_df[list(model_word_vec_df.keys())[0]].head())\n",
    "print(model_word_vec_df[list(model_word_vec_df.keys())[0]].shape)\n",
    "print(vocabs[list(model_word_vec_df.keys())[0]][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From vectors of words, how to get the vectors of docs? n_similarity uses the mean of the vectors of all words in the doc. We will use the tf-idf weighted vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting tf_idf of all words  in the vocab in all documents as weights  for all models.\n",
    "They are to be the weights of the words when we calculate the vector of docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vector_model_question_answer_200_6_2\n"
     ]
    }
   ],
   "source": [
    "# now we only use the second model as an example. The other one is similar: only reset paras and key.\n",
    "size =200\n",
    "min_count = 2\n",
    "window = 6\n",
    "# It corresponds to the second model key.\n",
    "\n",
    "key = list(models.keys())[1]\n",
    "print(key)\n",
    "vocab = vocabs[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating displaystyle hat int vert overrightarrow dt is approximating int vert overrightarrow overrightarrow dt int overrightarrow dt displaystyle hat overrightarrow because overrightarrow sim overrightarrow on the other hand just sampling overrightarrow t_i from overrightarrow vert t_i overrightarrow t_i means that t_i sim t_i the variance of the first one by the law of the unconscious statistician is taken with respect to the distribution overrightarrow while the second one is taken with respect to the prior t_i\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for post in word2vec_question_corpus + word2vec_answer_corpus:\n",
    "    corpus.append(\" \".join(post))\n",
    "\n",
    "print(corpus[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215962, 100269)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1, vocabulary=vocab)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weighted vectors for all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a_</th>\n",
       "      <td>-0.713271</td>\n",
       "      <td>-0.077981</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>-0.697069</td>\n",
       "      <td>-2.598913</td>\n",
       "      <td>-1.080629</td>\n",
       "      <td>-1.033209</td>\n",
       "      <td>0.885287</td>\n",
       "      <td>0.872299</td>\n",
       "      <td>-1.011196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674927</td>\n",
       "      <td>0.418498</td>\n",
       "      <td>-0.977101</td>\n",
       "      <td>-1.157536</td>\n",
       "      <td>-0.027437</td>\n",
       "      <td>-0.224027</td>\n",
       "      <td>-0.545397</td>\n",
       "      <td>-0.176365</td>\n",
       "      <td>2.447978</td>\n",
       "      <td>0.182951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a__</th>\n",
       "      <td>0.013685</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.025492</td>\n",
       "      <td>-0.012469</td>\n",
       "      <td>-0.009770</td>\n",
       "      <td>-0.009924</td>\n",
       "      <td>-0.004867</td>\n",
       "      <td>0.006001</td>\n",
       "      <td>0.038898</td>\n",
       "      <td>-0.027364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053458</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>-0.048481</td>\n",
       "      <td>-0.054677</td>\n",
       "      <td>-0.030503</td>\n",
       "      <td>-0.012346</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.056696</td>\n",
       "      <td>-0.029260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_a</th>\n",
       "      <td>0.020415</td>\n",
       "      <td>0.156389</td>\n",
       "      <td>-0.060513</td>\n",
       "      <td>-0.028313</td>\n",
       "      <td>-0.116366</td>\n",
       "      <td>-0.042931</td>\n",
       "      <td>-0.130101</td>\n",
       "      <td>-0.006461</td>\n",
       "      <td>0.008261</td>\n",
       "      <td>-0.065245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067757</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>-0.111685</td>\n",
       "      <td>0.160803</td>\n",
       "      <td>0.014258</td>\n",
       "      <td>-0.012060</td>\n",
       "      <td>0.078916</td>\n",
       "      <td>0.054315</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>-0.026724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_adjusted</th>\n",
       "      <td>-0.046735</td>\n",
       "      <td>0.041834</td>\n",
       "      <td>-0.061836</td>\n",
       "      <td>0.021160</td>\n",
       "      <td>0.046803</td>\n",
       "      <td>-0.024107</td>\n",
       "      <td>-0.032979</td>\n",
       "      <td>0.053604</td>\n",
       "      <td>0.037605</td>\n",
       "      <td>-0.080140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083108</td>\n",
       "      <td>-0.026857</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>-0.040803</td>\n",
       "      <td>-0.038954</td>\n",
       "      <td>0.027903</td>\n",
       "      <td>0.024604</td>\n",
       "      <td>-0.021590</td>\n",
       "      <td>-0.051460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_after_est</th>\n",
       "      <td>-0.011185</td>\n",
       "      <td>0.017912</td>\n",
       "      <td>-0.080105</td>\n",
       "      <td>-0.030186</td>\n",
       "      <td>0.052851</td>\n",
       "      <td>-0.022680</td>\n",
       "      <td>-0.036077</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>-0.008886</td>\n",
       "      <td>-0.044048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066457</td>\n",
       "      <td>-0.011853</td>\n",
       "      <td>-0.007649</td>\n",
       "      <td>0.017082</td>\n",
       "      <td>-0.006570</td>\n",
       "      <td>-0.020994</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>0.006295</td>\n",
       "      <td>-0.008900</td>\n",
       "      <td>-0.035567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5    \\\n",
       "a_          -0.713271 -0.077981  0.010418 -0.697069 -2.598913 -1.080629   \n",
       "a__          0.013685 -0.000013 -0.025492 -0.012469 -0.009770 -0.009924   \n",
       "a_a          0.020415  0.156389 -0.060513 -0.028313 -0.116366 -0.042931   \n",
       "a_adjusted  -0.046735  0.041834 -0.061836  0.021160  0.046803 -0.024107   \n",
       "a_after_est -0.011185  0.017912 -0.080105 -0.030186  0.052851 -0.022680   \n",
       "\n",
       "                  6         7         8         9      ...          190  \\\n",
       "a_          -1.033209  0.885287  0.872299 -1.011196    ...     0.674927   \n",
       "a__         -0.004867  0.006001  0.038898 -0.027364    ...     0.053458   \n",
       "a_a         -0.130101 -0.006461  0.008261 -0.065245    ...    -0.067757   \n",
       "a_adjusted  -0.032979  0.053604  0.037605 -0.080140    ...     0.083108   \n",
       "a_after_est -0.036077  0.007863 -0.008886 -0.044048    ...     0.066457   \n",
       "\n",
       "                  191       192       193       194       195       196  \\\n",
       "a_           0.418498 -0.977101 -1.157536 -0.027437 -0.224027 -0.545397   \n",
       "a__          0.000878  0.002365 -0.048481 -0.054677 -0.030503 -0.012346   \n",
       "a_a          0.007425 -0.111685  0.160803  0.014258 -0.012060  0.078916   \n",
       "a_adjusted  -0.026857  0.024127  0.040530 -0.040803 -0.038954  0.027903   \n",
       "a_after_est -0.011853 -0.007649  0.017082 -0.006570 -0.020994  0.003839   \n",
       "\n",
       "                  197       198       199  \n",
       "a_          -0.176365  2.447978  0.182951  \n",
       "a__          0.029457  0.056696 -0.029260  \n",
       "a_a          0.054315  0.021641 -0.026724  \n",
       "a_adjusted   0.024604 -0.021590 -0.051460  \n",
       "a_after_est  0.006295 -0.008900 -0.035567  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word_vec_df[key].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100269, 200)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word_vec_df[key].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the vocab from tf_idf vectorizer for future reference.\n",
    "import csv\n",
    "fname = './vocab_dict/vocab_dict_question_answer_'+str(int(size))+\"_\"+str(int(window))+\"_\"+str(int(min_count))+'.csv'\n",
    "with open(fname, 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in vectorizer.vocabulary_.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100269"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # The number of nonzero elements in a row in X should equal to the number \n",
    "# # of unique words in the corresponding doc or word2vec_question_corpus. \n",
    "# # The funciton check is used to check this.\n",
    "\n",
    "# def check(key,i): # ith documents. doc_corpus a list of words\n",
    "#     doc_corpus = word2vec_question_corpus[i]\n",
    "#     df = model_word_vec_df[key]\n",
    "#     related_rows = df.loc[sorted(list(set(doc_corpus).intersection(set(vocab)))), :]  \n",
    "#     words_index = set(np.where(df.index.isin(set(doc_corpus).intersection(set(vocab))))[0])\n",
    "#     # print(related_rows.T)\n",
    "#     weights_index = []\n",
    "#     ind = sorted(X[i,:].nonzero()[1])\n",
    "#     for j in ind:\n",
    "#         weights_index.append(j)\n",
    "   \n",
    "#     if len(weights_index) - len(words_index) != 0:\n",
    "#         print(i)\n",
    "#         print(weights_index)\n",
    "#         print(words_index)\n",
    "#     if related_rows.shape[0] != len(weights_index):\n",
    "#         print(i)\n",
    "#         print(\"*********\")\n",
    "#     return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vector_model_question_answer_200_6_2\n"
     ]
    }
   ],
   "source": [
    "key = list(models.keys())[1]\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = Q_df.shape[0]\n",
    "# current = 0\n",
    "# for i in range(Q_df.shape[0]):\n",
    "#     check(key,i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #vectorizer.vocabulary_['a_']  # =0\n",
    "# model_word_vec_df[key].index[0] # = 'a_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('doc_vec/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('doc_vec/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = sorted(X[0,:].nonzero()[1])\n",
    "print(len(ind))\n",
    "for i in range(11):\n",
    "    print(ind[i]== vectorizer.vocabulary_[list(related_rows.index)[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ith documents. doc_corpus a list of words \n",
    "# only do this for questions\n",
    "def get_doc_weighted_vec(key,i, weighted = True): \n",
    "    doc_corpus = word2vec_question_corpus[i]\n",
    "    df = model_word_vec_df[key]\n",
    "    related_rows = df.loc[sorted(list(set(doc_corpus).intersection(set(vocab)))), :] \n",
    "    \n",
    "    if weighted:\n",
    "        weights = []\n",
    "        ind = sorted(X[i,:].nonzero()[1]) # words index \n",
    "# One should check this. But it is very slow. I checked half of all question posts and they are valid. \n",
    "#         if sum([vectorizer.vocabulary_[related_rows.index[j]] != ind[j] for j in range(len(ind))]) != 0:\n",
    "#             print(\"words position don't match\")\n",
    "#             return \n",
    "\n",
    "        weights = [X[i, ind[j]] for j in range(len(ind))]        \n",
    "        weights = np.array(weights)/sum(weights) # scale so the sum of weights is one.\n",
    "    else:\n",
    "        # to correct: some words may appear more than once. But since we are only interested in the weighted vec, I will correct this later.\n",
    "        weights = [1/related_rows.shape[0]] * related_rows.shape[0] \n",
    " \n",
    "\n",
    "    result = related_rows.T * weights\n",
    "    return result.sum(axis = 1) # weighted sum as the doc vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status:[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100.00%\n",
      "\n",
      "108954\n",
      "200\n",
      "108954\n",
      "200\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.274664 -0.197037  1.621728 -0.237445 -0.288944  0.843277  0.617973   \n",
      "1 -0.009377 -0.493120  2.030924 -0.442848 -0.290093  0.380474  1.293419   \n",
      "2  0.144012 -0.184814  0.698455 -0.249389 -0.030401 -0.010584  0.131361   \n",
      "3  0.119089  0.075075  1.400657 -0.792461 -0.304917  0.148512  0.428406   \n",
      "4  0.217803 -0.220348  0.532312 -0.145882 -0.036074  0.122394 -0.028780   \n",
      "\n",
      "        7         8         9      ...          190       191       192  \\\n",
      "0  1.499649  0.842379 -0.634372    ...    -0.057703 -0.040805  0.639898   \n",
      "1  1.768064  1.919836 -0.660174    ...     0.605748  0.176924  0.618500   \n",
      "2  0.616810  0.120255  0.454690    ...     0.412973  0.268411 -0.134549   \n",
      "3  1.037877  0.446540 -0.569850    ...     0.973830 -0.436764  0.745695   \n",
      "4  0.217096  0.266330  0.023324    ...     0.062220  0.167978  0.064264   \n",
      "\n",
      "        193       194       195       196       197       198       199  \n",
      "0  0.234292  0.024925  0.118979 -0.794148 -0.566620  0.138416 -0.548374  \n",
      "1 -0.351151 -0.435653  0.027308 -0.069966  0.080411  1.921239 -0.199846  \n",
      "2 -0.435909 -0.054599  0.377649  0.198194 -0.116156  0.011338 -0.133228  \n",
      "3 -0.057611  0.056279 -0.127634 -0.005717  0.302014  0.462430  0.046998  \n",
      "4 -0.032044 -0.126803  0.170066 -0.218061 -0.247630 -0.101045 -0.194075  \n",
      "\n",
      "[5 rows x 200 columns]\n",
      "(108954, 200)\n"
     ]
    }
   ],
   "source": [
    "questions_vec_weighted = []\n",
    "total = len(word2vec_question_corpus)\n",
    "current = 0\n",
    "for i in range(len(word2vec_question_corpus)):\n",
    "    show_work_status(1, total, current)\n",
    "    current += 1\n",
    "    questions_vec_weighted.append(get_doc_weighted_vec(key,i))\n",
    "\n",
    "\n",
    "print(len(questions_vec_weighted))\n",
    "print(len(questions_vec_weighted[0]))\n",
    "\n",
    "# save questions_vec_weighted\n",
    "save_obj(questions_vec_weighted, \"questions_weighted_vec_\"+key)\n",
    "questions_vec_weighted = load_obj(\"questions_weighted_vec_\"+key)\n",
    "\n",
    "print(len(questions_vec_weighted))\n",
    "print(len(questions_vec_weighted[0]))\n",
    "\n",
    "questions_vec_weighted_df = pd.DataFrame(questions_vec_weighted)\n",
    "print(questions_vec_weighted_df.head())\n",
    "print(questions_vec_weighted_df.shape)\n",
    "\n",
    "questions_vec_weighted_df.to_csv('doc_vec/questions_weighted_vec_'+key+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
