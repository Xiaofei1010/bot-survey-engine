{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import configparser\n",
    "import json\n",
    "import requests\n",
    "import xmltodict\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import gensim\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import smart_open\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve episode descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'feed.xml'\n",
    "url = 'http://dataskeptic.com/feed.rss'\n",
    "\n",
    "if not(os.path.isfile(fname)):\n",
    "    print('fetching')\n",
    "    r = requests.get(url)\n",
    "    f = open(fname, 'wb')\n",
    "    f.write(r.text.encode('utf-8'))\n",
    "    f.close()\n",
    "\n",
    "with open(fname) as fd:\n",
    "    xml = xmltodict.parse(fd.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm joined by Wes McKinney (@wesmckinn) and Hadley Wickham (@hadleywickham) on this episode to discuss their joint project Feather. Feather is a file format for storing data frames along with some metadata, to help with interoperability between languages. At the time of recording, libraries are available for R and Python, making it easy for data scientists working in these languages to quickly and effectively share datasets and collaborate.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = xml['rss']['channel']['item'][72]['description']\n",
    "a = BeautifulSoup(test,'lxml').text\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episodes = xml['rss']['channel']['item']\n",
    "descriptions = []\n",
    "descToTitle = {}\n",
    "descToLink = {}\n",
    "descToNum = {}\n",
    "l = len(episodes)\n",
    "for episode in episodes:\n",
    "    enclosure = episode['enclosure']\n",
    "    \n",
    "    desc = episode['description']\n",
    "    desc = desc.replace(u'\\xa0', u' ')\n",
    "    desc = desc.replace(u'\\n', u' ')\n",
    "    desc = desc.replace(u'\\xc2', u' ')\n",
    "\n",
    "    \n",
    "    desc = BeautifulSoup(desc, \"lxml\").text\n",
    "    descriptions.append(desc)\n",
    "    \n",
    "    descToTitle[desc] = episode['title']\n",
    "    descToLink[desc] = episode['link']\n",
    "    descToNum[desc] = l\n",
    "    l = l - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['title', 'pubDate', 'guid', 'link', 'itunes:image', 'description', 'content:encoded', 'enclosure', 'itunes:duration', 'itunes:explicit', 'itunes:keywords', 'itunes:subtitle', 'itunes:episodeType'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = {}\n",
    "for desc in descriptions:\n",
    "    info = {}\n",
    "    info[\"link\"] = descToLink[desc]\n",
    "    info[\"title\"] = descToTitle[desc]\n",
    "    info[\"num\"] = descToNum[desc]\n",
    "    result[desc] = info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./text/episodes_json.txt', 'w') as outfile:  \n",
    "    json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "Generative AI for Content Creation\n",
      "https://dataskeptic.com/blog/episodes/2017/generative-ai-for-content-creation\n",
      "178\n",
      "[MINI] One Shot Learning\n",
      "https://dataskeptic.com/blog/episodes/2017/one-shot-learning\n",
      "177\n",
      "Recommender Systems Live from FARCON 2017\n",
      "https://dataskeptic.com/blog/episodes/2017/recommender-systems-live-from-farcon\n",
      "176\n",
      "[MINI] Long Short Term Memory\n",
      "https://dataskeptic.com/blog/episodes/2017/long-short-term-memory\n",
      "175\n",
      "Zillow Zestimate\n",
      "https://dataskeptic.com/blog/episodes/2017/zillow-zestimate\n",
      "174\n",
      "Cardiologist Level Arrhythmia Detection with CNNs\n",
      "https://dataskeptic.com/blog/episodes/2017/cardiologist-level-arrhythmia-detection-with-cnns\n",
      "173\n",
      "[MINI] Recurrent Neural Networks\n",
      "https://dataskeptic.com/blog/episodes/2017/recurrent-neural-networks\n",
      "172\n",
      "Project Common Voice\n",
      "https://dataskeptic.com/blog/episodes/2017/project-common-voice\n",
      "171\n",
      "[MINI] Bayesian Belief Networks\n",
      "http://dataskeptic.com/blog/episodes/2017/bayesian-belief-networks\n",
      "170\n",
      "pix2code\n",
      "https://dataskeptic.com/blog/episodes/2017/pix2code\n",
      "169\n",
      "[MINI] Conditional Independence\n",
      "http://dataskeptic.com/blog/episodes/2017/conditional-independence\n",
      "168\n",
      "Estimating Sheep Pain with Facial Recognition\n",
      "http://dataskeptic.com/blog/episodes/2017/estimating-sheep-pain-with-facial-recognition\n",
      "167\n",
      "CosmosDB\n",
      "https://dataskeptic.com/blog/episodes/2017/cosmos-db\n",
      "166\n",
      "[MINI] The Vanishing Gradient\n",
      "http://dataskeptic.com/blog/episodes/2017/the-vanishing-gradient\n",
      "165\n",
      "Doctor AI\n",
      "https://dataskeptic.com/blog/episodes/2017/doctor-ai\n",
      "164\n",
      "[MINI] Activation Functions\n",
      "https://dataskeptic.com/blog/episodes/2017/activation-functions\n",
      "163\n",
      "MS Build 2017\n",
      "https://dataskeptic.com/blog/episodes/2017/ms-build-2017\n",
      "162\n",
      "[MINI] Max-pooling\n",
      "http://dataskeptic.com/blog/episodes/2017/max-pooling\n",
      "161\n",
      "Unsupervised Depth Perception\n",
      "https://dataskeptic.com/blog/episodes/2017/unsupervised-depth-perception\n",
      "160\n",
      "[MINI] Convolutional Neural Networks\n",
      "https://dataskeptic.com/blog/episodes/2017/convolutional-neural-networks\n",
      "159\n",
      "Multi-Agent Diverse Generative Adversarial Networks\n",
      "http://dataskeptic.com/blog/episodes/2017/multi-agent-diverse-generative-adversarial-networks\n",
      "158\n",
      "[MINI] Generative Adversarial Networks\n",
      "https://dataskeptic.com/blog/episodes/2017/generative-adversarial-networks\n",
      "157\n",
      "Opinion Polls for Presidential Elections\n",
      "https://dataskeptic.com/blog/episodes/2017/polling\n",
      "156\n",
      "OpenHouse\n",
      "https://dataskeptic.com/blog/episodes/2017/openhouse\n",
      "155\n",
      "[MINI] GPU CPU\n",
      "https://dataskeptic.com/blog/episodes/2017/gpu-cpu\n",
      "154\n",
      "[MINI] Backpropagation\n",
      "http://dataskeptic.com/blog/episodes/2017/backpropagation\n",
      "153\n",
      "Data Science at Patreon\n",
      "http://dataskeptic.com/blog/episodes/2017/data-science-at-patreon\n",
      "152\n",
      "[MINI] Feed Forward Neural Networks\n",
      "http://dataskeptic.com/blog/episodes/2017/feed-forward-neural-networks\n",
      "151\n",
      "Reinventing Sponsored Search Auctions\n",
      "https://dataskeptic.com/blog/episodes/2017/reinventing-sponsored-search-auctions\n",
      "150\n",
      "[MINI] The Perceptron\n",
      "https://dataskeptic.com/blog/episodes/2017/the-perceptron\n",
      "149\n",
      "The Data Refuge Project\n",
      "https://dataskeptic.com/blog/episodes/2017/the-data-refuge-project\n",
      "148\n",
      "[MINI] Automated Feature Engineering\n",
      "https://dataskeptic.com/blog/episodes/2017/automated-feature-engineering\n",
      "147\n",
      "Big Data Tools and Trends\n",
      "https://dataskeptic.com/blog/episodes/2017/big-data-tools-and-trends\n",
      "146\n",
      "[MINI] Primer on Deep Learning\n",
      "https://dataskeptic.com/blog/episodes/2017/primer-on-deep-learning\n",
      "145\n",
      "Data Provenance and Reproducibility with Pachyderm\n",
      "http://dataskeptic.com/blog/episodes/2017/data-provenance-and-reproducibility-with-pachyderm\n",
      "144\n",
      "[MINI] Logistic Regression on Audio Data\n",
      "http://dataskeptic.com/blog/episodes/2017/logistic-regression-on-audio-data\n",
      "143\n",
      "Studying Competition and Gender Through Chess\n",
      "http://dataskeptic.com/blog/episodes/2017/studying-competition-and-gender-through-chess\n",
      "142\n",
      "[MINI] Dropout\n",
      "http://dataskeptic.com/blog/episodes/2017/dropout\n",
      "141\n",
      "The Police Data and the Data Driven Justice Initiatives\n",
      "http://dataskeptic.com/blog/episodes/2017/the-police-data-initiative-and-the-data-driven-justice-initiative\n",
      "140\n",
      "The Library Problem\n",
      "http://dataskeptic.com/blog/episodes/2016/the-library-problem\n",
      "139\n",
      "2016 Holiday Special\n",
      "http://dataskeptic.com/blog/episodes/2016/holiday-special\n",
      "138\n",
      "[MINI] Entropy\n",
      "http://dataskeptic.com/blog/episodes/2016/entropy\n",
      "137\n",
      "MS Connect Conference\n",
      "http://dataskeptic.com/blog/episodes/2016/ms-connect-conference\n",
      "136\n",
      "Causal Impact\n",
      "http://dataskeptic.com/blog/episodes/2016/causal-impact\n",
      "135\n",
      "[MINI] The Bootstrap\n",
      "http://dataskeptic.com/blog/episodes/2016/the-bootstrap\n",
      "134\n",
      "[MINI] Gini Coefficients\n",
      "http://dataskeptic.com/blog/episodes/2016/gini-coefficient\n",
      "133\n",
      "Unstructured Data for Finance\n",
      "http://dataskeptic.com/epnotes/unstructured-data-for-finance.php\n",
      "132\n",
      "[MINI] AdaBoost\n",
      "http://dataskeptic.com/epnotes/adaboost.php\n",
      "131\n",
      "Stealing Models from the Cloud\n",
      "http://dataskeptic.com/epnotes/stealing-models-from-the-cloud.php\n",
      "130\n",
      "[MINI] Calculating Feature Importance\n",
      "http://dataskeptic.com/epnotes/calculating-feature-importance.php\n",
      "129\n",
      "NYC Bike Share Rebalancing\n",
      "http://dataskeptic.com/epnotes/nyc-bikeshare-rebalancing.php\n",
      "128\n",
      "[MINI] Random Forest\n",
      "http://dataskeptic.com/epnotes/random-forest.php\n",
      "127\n",
      "Election Predictions\n",
      "http://dataskeptic.com/epnotes/election-predictions.php\n",
      "126\n",
      "[MINI] F1 Score\n",
      "http://dataskeptic.com/epnotes/f1-score.php\n",
      "125\n",
      "Urban Congestion\n",
      "http://dataskeptic.com/epnotes/urban-congestion.php\n",
      "124\n",
      "[MINI] Heteroskedasticity\n",
      "http://dataskeptic.com/epnotes/heteroskedasticity.php\n",
      "123\n",
      "Music21\n",
      "http://dataskeptic.com/epnotes/music21.php\n",
      "122\n",
      "[MINI] Paxos\n",
      "http://dataskeptic.com/epnotes/paxos.php\n",
      "121\n",
      "Trusting Machine Learning Models with LIME\n",
      "http://dataskeptic.com/epnotes/trusting-machine-learning-models-with-lime.php\n",
      "120\n",
      "[MINI] ANOVA\n",
      "http://dataskeptic.com/epnotes/anova.php\n",
      "119\n",
      "Machine Learning on Images with Noisy Human-centric Labels\n",
      "http://dataskeptic.com/epnotes/machine-learning-on-images-with-noisy-human-centric-labels.php\n",
      "118\n",
      "[MINI] Survival Analysis\n",
      "http://dataskeptic.com/epnotes/survival-analysis.php\n",
      "117\n",
      "Predictive Models on Random Data\n",
      "http://dataskeptic.com/epnotes/predictive-models-on-random-data.php\n",
      "116\n",
      "[MINI] Receiver Operating Characteristic (ROC) Curve\n",
      "http://dataskeptic.com/epnotes/roc.php\n",
      "115\n",
      "Multiple Comparisons and Conversion Optimization\n",
      "http://dataskeptic.com/epnotes/multiple-comparisons.php\n",
      "114\n",
      "[MINI] Leakage\n",
      "http://dataskeptic.com/epnotes/leakage.php\n",
      "113\n",
      "Predictive Policing\n",
      "http://dataskeptic.com/epnotes/predictive-policing.php\n",
      "112\n",
      "[MINI] The CAP Theorem\n",
      "http://dataskeptic.com/epnotes/cap-theorem.php\n",
      "111\n",
      "Detecting Terrorists with Facial Recognition?\n",
      "http://dataskeptic.com/epnotes/detecting-terrorists-with-facial-recognition.php\n",
      "110\n",
      "[MINI] Goodhart's Law\n",
      "http://dataskeptic.com/epnotes/goodharts-law.php\n",
      "109\n",
      "Data Science at eHarmony\n",
      "http://dataskeptic.com/epnotes/data-science-at-eharmony.php\n",
      "108\n",
      "[MINI] Stationarity and Differencing\n",
      "http://dataskeptic.com/epnotes/stationarity-and-differencing.php\n",
      "107\n",
      "Feather\n",
      "http://dataskeptic.com/blog/episodes/2016/feather\n",
      "106\n",
      "[MINI] Bargaining\n",
      "http://dataskeptic.com/epnotes/bargaining.php\n",
      "105\n",
      "deepjazz\n",
      "http://dataskeptic.com/epnotes/deepjazz.php\n",
      "104\n",
      "[MINI] Auto-correlative functions and correlograms\n",
      "http://dataskeptic.com/epnotes/acf-correlograms.php\n",
      "103\n",
      "Early Identification of Violent Criminal Gang Members\n",
      "http://dataskeptic.com/epnotes/early-identification-of-violent-criminal-gang-members.php\n",
      "102\n",
      "[MINI] Fractional Factorial Design\n",
      "http://dataskeptic.com/epnotes/fractional-factorial-design.php\n",
      "101\n",
      "Machine Learning Done Wrong\n",
      "http://dataskeptic.com/epnotes/machine-learning-done-wrong.php\n",
      "100\n",
      "Potholes\n",
      "http://dataskeptic.com/epnotes/potholes.php\n",
      "99\n",
      "[MINI] The Elbow Method\n",
      "http://dataskeptic.com/epnotes/the-elbow-method.php\n",
      "98\n",
      "Too Good to be True\n",
      "http://dataskeptic.com/epnotes/too-good-to-be-true.php\n",
      "97\n",
      "[MINI] R-squared\n",
      "http://dataskeptic.com/epnotes/r-squared.php\n",
      "96\n",
      "Models of Mental Simulation\n",
      "http://dataskeptic.com/epnotes/models-of-mental-simulation.php\n",
      "95\n",
      "[MINI] Multiple Regression\n",
      "http://dataskeptic.com/epnotes/multiple-regression.php\n",
      "94\n",
      "Scientific Studies of People's Relationship to Music\n",
      "http://dataskeptic.com/epnotes/scientific-studies-of-peoples-relationship-to-music.php\n",
      "93\n",
      "[MINI] k-d trees\n",
      "http://dataskeptic.com/epnotes/k-d-trees.php\n",
      "92\n",
      "Auditing Algorithms\n",
      "http://dataskeptic.com/epnotes/auditing-algorithms.php\n",
      "91\n",
      "[MINI] The Bonferroni Correction\n",
      "http://dataskeptic.com/epnotes/bonferroni-correction.php\n",
      "90\n",
      "Detecting Pseudo-profound BS\n",
      "http://dataskeptic.com/epnotes/detecting-pseudo-profound-bs.php\n",
      "89\n",
      "[MINI] Gradient Descent\n",
      "http://dataskeptic.com/epnotes/gradient-descent.php\n",
      "88\n",
      "Let's Kill the Word Cloud\n",
      "http://dataskeptic.com/epnotes/kill-the-word-cloud.php\n",
      "87\n",
      "2015 Holiday Special\n",
      "http://dataskeptic.com/epnotes/2015-holiday-special.php\n",
      "86\n",
      "Wikipedia Revision Scoring as a Service\n",
      "http://dataskeptic.com/epnotes/wikipedia-revision-scoring-as-a-service.php\n",
      "85\n",
      "[MINI] Term Frequency - Inverse Document Frequency\n",
      "http://dataskeptic.com/epnotes/tf-idf.php\n",
      "84\n",
      "The Hunt for Vulcan\n",
      "http://dataskeptic.com/epnotes/the-hunt-for-vulcan.php\n",
      "83\n",
      "[MINI] The Accuracy Paradox\n",
      "http://dataskeptic.com/epnotes/the-accuracy-paradox.php\n",
      "82\n",
      "Neuroscience from a Data Scientist's Perspective\n",
      "http://dataskeptic.com/epnotes/neuroscience-from-a-data-scientists-perspective.php\n",
      "81\n",
      "[MINI] Bias Variance Tradeoff\n",
      "http://dataskeptic.com/epnotes/bias-variance-tradeoff.php\n",
      "80\n",
      "Big Data Doesn't Exist\n",
      "http://dataskeptic.com/epnotes/big-data-doesnt-exist.php\n",
      "79\n",
      "[MINI] Covariance and Correlation\n",
      "http://dataskeptic.com/epnotes/ep79_covariance-and-correlation.php\n",
      "78\n",
      "Bayesian A/B Testing\n",
      "http://dataskeptic.com/epnotes/ep78_bayesian-a-b-testing.php\n",
      "77\n",
      "[MINI] The Central Limit Theorem\n",
      "http://dataskeptic.com/epnotes/ep77_central-limit-theorem.php\n",
      "76\n",
      "Accessible Technology\n",
      "http://dataskeptic.com/epnotes/ep76_accessible-technology.php\n",
      "75\n",
      "[MINI] Multi-armed Bandit Problems\n",
      "http://dataskeptic.com/epnotes/ep75_multi-armed-bandit-problems.php\n",
      "74\n",
      "Shakespeare, Abiogenesis, and Exoplanets\n",
      "http://dataskeptic.com/epnotes/ep74_shakespeare-abiogenesis-and-exoplanets.php\n",
      "73\n",
      "[MINI] Sample Sizes\n",
      "http://dataskeptic.com/epnotes/ep73_small-sample-sizes.php\n",
      "72\n",
      "The Model Complexity Myth\n",
      "http://dataskeptic.com/epnotes/ep72_model-complexity-myth.php\n",
      "71\n",
      "[MINI] Distance Measures\n",
      "http://dataskeptic.com/epnotes/ep71_distance-measures.php\n",
      "70\n",
      "ContentMine\n",
      "http://dataskeptic.com/epnotes/ep70_contentmine.php\n",
      "69\n",
      "[MINI] Structured and Unstructured Data\n",
      "http://dataskeptic.com/epnotes/ep69_structured-and-unstructured.php\n",
      "68\n",
      "Measuring the Influence of Fashion Designers\n",
      "http://dataskeptic.com/epnotes/ep68_measuring-the-influence-of-fashion-designers.php\n",
      "67\n",
      "[MINI] PageRank\n",
      "http://dataskeptic.com/epnotes/ep67_pagerank.php\n",
      "66\n",
      "Data Science at Work in LA County\n",
      "http://dataskeptic.com/epnotes/ep66_data-science-at-work-in-la-county.php\n",
      "65\n",
      "[MINI] k-Nearest Neighbors\n",
      "http://dataskeptic.com/epnotes/ep65_k-nearest-neighbors.php\n",
      "64\n",
      "Crypto\n",
      "http://dataskeptic.com/blog/episodes/2015/crypto\n",
      "63\n",
      "[MINI] MapReduce\n",
      "http://dataskeptic.com/epnotes/ep63_map-reduce.php\n",
      "62\n",
      "Genetically Engineered Food and Trends in Herbicide Usage\n",
      "http://dataskeptic.com/epnotes/ep62_genetically-engineered-food-and-trends-in-herbicide-usage.php\n",
      "61\n",
      "[MINI] The Curse of Dimensionality\n",
      "http://dataskeptic.com/epnotes/ep61_the-curse-of-dimensionality.php\n",
      "60\n",
      "Video Game Analytics\n",
      "http://dataskeptic.com/epnotes/ep60_game-analytics.php\n",
      "59\n",
      "[MINI] Anscombe's Quartet\n",
      "http://dataskeptic.com/epnotes/ep59_anscombes_quartet.php\n",
      "58\n",
      "Proposing Annoyance Mining\n",
      "http://dataskeptic.com/epnotes/proposing-annoyance-mining.php\n",
      "57\n",
      "Preserving History at Cyark\n",
      "http://dataskeptic.com/epnotes/ep57_preserving-history-at-cyark.php\n",
      "56\n",
      "[MINI] A Critical Examination of a Study of Marriage by Political Affiliation\n",
      "http://dataskeptic.com/epnotes/ep56_a-critical-examination-of-a-study-of-marriage-by-political-affiliation.php\n",
      "55\n",
      "Detecting Cheating in Chess\n",
      "http://dataskeptic.com/epnotes/ep55_detecting-cheating-in-chess.php\n",
      "54\n",
      "[MINI] z-scores\n",
      "http://dataskeptic.com/epnotes/ep54_z-scores.php\n",
      "53\n",
      "Using Data to Help Those in Crisis\n",
      "http://dataskeptic.com/epnotes/ep53_using-data-to-help-those-in-crisis.php\n",
      "52\n",
      "The Ghost in the MP3\n",
      "http://dataskeptic.com/epnotes/ep52_the-ghost-in-the-mp3-with-Ryan-Maguire.php\n",
      "51\n",
      "Data Fest 2015\n",
      "http://dataskeptic.com/epnotes/ep51_data-fest-2015.php\n",
      "50\n",
      "[MINI] Cornbread and Overdispersion\n",
      "http://dataskeptic.com/epnotes/ep50_the-cornbread-episode-on-over-dispersion.php\n",
      "49\n",
      "[MINI] Natural Language Processing\n",
      "http://dataskeptic.com/epnotes/ep49_natural-language-processing.php\n",
      "48\n",
      "Computer-based Personality Judgments\n",
      "http://dataskeptic.com/epnotes/ep48_computer-based-personality-judgments-with-Youyou-Wu.php\n",
      "47\n",
      "[MINI] Markov Chain Monte Carlo\n",
      "http://dataskeptic.com/epnotes/ep47_Markov-chain-monte-carlo.php\n",
      "46\n",
      "[MINI] Markov Chains\n",
      "http://dataskeptic.com/epnotes/ep46_Markov-Chains.php\n",
      "45\n",
      "Oceanography and Data Science\n",
      "http://dataskeptic.com/epnotes/ep45_Oceanography-and-Data-Science.php\n",
      "44\n",
      "[MINI] Ordinary Least Squares Regression\n",
      "http://dataskeptic.com/epnotes/ordinary-least-squares.php\n",
      "43\n",
      "NYC Speed Camera Analysis with Tim Schmeier\n",
      "http://dataskeptic.com/epnotes/ep43_NYC-Speed-Camera-Analysis-with-Tim-Schmeier.php\n",
      "42\n",
      "[MINI] k-means clustering\n",
      "http://dataskeptic.com/epnotes/k-means-clustering.php\n",
      "41\n",
      "Shadow Profiles on Social Networks\n",
      "http://dataskeptic.com/epnotes/ep41_Shadow-Profiles-on-Social-Networks-with-Emre-Sarigol.php\n",
      "40\n",
      "[MINI] The Chi-Squared Test\n",
      "http://dataskeptic.com/epnotes/ep40_chi_sq_test.php\n",
      "39\n",
      "Mapping Reddit Topics with Randy Olson\n",
      "http://dataskeptic.com/epnotes/ep39_mapping-reddit-topics.php\n",
      "38\n",
      "[MINI] Partially Observable State Spaces\n",
      "http://dataskeptic.com/epnotes/partially-observable-state-spaces.php\n",
      "37\n",
      "Easily Fooling Deep Neural Networks\n",
      "http://dataskeptic.com/epnotes/ep37_easily-fooling-deep-neural-networks.php\n",
      "36\n",
      "[MINI] Data Provenance\n",
      "http://dataskeptic.com/epnotes/ep36_data-provenance.php\n",
      "35\n",
      "Doubtful News, Geology, Investigating Paranormal Groups, and Thinking Scientifically with Sharon Hill\n",
      "http://dataskeptic.com/epnotes/ep35_doubtful-news-geology-and-thinking-scientifically-with-Sharon-Hill.php\n",
      "34\n",
      "[MINI] Belief in Santa\n",
      "http://dataskeptic.com/epnotes/belief-in-santa.php\n",
      "33\n",
      "Economic Modeling and Prediction, Charitable Giving, and a Follow Up with Peter Backus\n",
      "http://dataskeptic.com/epnotes/ep33_Economic-Modeling-and-Prediction-with-Peter-Backus.php\n",
      "32\n",
      "[MINI] The Battle of the Sexes\n",
      "http://dataskeptic.com/epnotes/ep32_battle-of-the-sexes.php\n",
      "31\n",
      "The Science of Online Data at Plenty of Fish with Thomas Levi\n",
      "http://dataskeptic.com/epnotes/ep31_plenty-of-fish-data-science-approaches-with-thomas-levi.php\n",
      "30\n",
      "[MINI] The Girlfriend Equation\n",
      "http://dataskeptic.com/epnotes/ep30_the_girlfriend_equation.php\n",
      "29\n",
      "The Secret and the Global Consciousness Project with Alex Boklin\n",
      "http://dataskeptic.com/epnotes/ep29_the-secret-and-the-global-consciousness-project.php\n",
      "28\n",
      "[MINI] Monkeys on Typewriters\n",
      "http://dataskeptic.com/epnotes/ep28_random-numbers.php\n",
      "27\n",
      "Mining the Social Web with Matthew Russell\n",
      "http://dataskeptic.com/epnotes/ep27_mining-the-social-web.php\n",
      "26\n",
      "[MINI] Is the Internet Secure?\n",
      "http://dataskeptic.com/epnotes/is-the-internet-secure.php\n",
      "25\n",
      "Practicing and Communicating Data Science with Jeff Stanton\n",
      "http://dataskeptic.com/epnotes/practicing-and-communicating-data-science.php\n",
      "24\n",
      "[MINI] The T-Test\n",
      "http://dataskeptic.com/epnotes/t-test.php\n",
      "23\n",
      "Data Myths with Karl Mamer\n",
      "http://dataskeptic.com/epnotes/ep023.php\n",
      "22\n",
      "Contest Announcement\n",
      "http://dataskeptic.com/epnotes/ep022.php\n",
      "21\n",
      "[MINI] Selection Bias\n",
      "http://dataskeptic.com/epnotes/selection-bias.php\n",
      "20\n",
      "[MINI] Confidence Intervals\n",
      "http://dataskeptic.com/epnotes/confidence-intervals.php\n",
      "19\n",
      "[MINI] Value of Information\n",
      "http://dataskeptic.com/epnotes/value-of-information.php\n",
      "18\n",
      "Game Science Dice with Louis Zocchi\n",
      "http://dataskeptic.com/epnotes/ep017.php\n",
      "17\n",
      "Data Science at ZestFinance with Marick Sinay\n",
      "http://dataskeptic.com/epnotes/ep17_zest-finance-with-marick-sinay.php\n",
      "16\n",
      "[MINI] Decision Tree Learning\n",
      "http://dataskeptic.com/epnotes/decision-tree-learning.php\n",
      "15\n",
      "Jackson Pollock Authentication Analysis with Kate Jones-Smith\n",
      "http://dataskeptic.com/epnotes/ep014.php\n",
      "14\n",
      "[MINI] Noise!!\n",
      "http://dataskeptic.com/epnotes/noise.php\n",
      "13\n",
      "Guerilla Skepticism on Wikipedia with Susan Gerbic\n",
      "http://dataskeptic.com/epnotes/ep012.php\n",
      "12\n",
      "[MINI] Ant Colony Optimization\n",
      "http://dataskeptic.com/epnotes/ant-colony-optimization.php\n",
      "11\n",
      "Data in Healthcare IT with Shahid Shah\n",
      "http://dataskeptic.com/epnotes/ep010.php\n",
      "10\n",
      "[MINI] Cross Validation\n",
      "http://dataskeptic.com/epnotes/cross-validation.php\n",
      "9\n",
      "Streetlight Outage and Crime Rate Analysis with Zach Seeskin\n",
      "http://dataskeptic.com/epnotes/ep008.php\n",
      "8\n",
      "[MINI] Experimental Design\n",
      "http://dataskeptic.com/epnotes/experimental-design.php\n",
      "7\n",
      "The Right (big data) Tool for the Job with Jay Shankar\n",
      "http://dataskeptic.com/epnotes/ep006.php\n",
      "6\n",
      "[MINI] Bayesian Updating\n",
      "http://dataskeptic.com/blog/episodes/2014/bayesian-updating\n",
      "5\n",
      "Personalized Medicine with Niki Athanasiadou\n",
      "http://dataskeptic.com/blog/episodes/2014/personalized-medicine\n",
      "4\n",
      "[MINI] p-values\n",
      "http://dataskeptic.com/epnotes/p-values.php\n",
      "3\n",
      "Advertising Attribution with Nathan Janos\n",
      "http://dataskeptic.com/epnotes/ep002.php\n",
      "2\n",
      "[MINI] type i / type ii errors\n",
      "http://dataskeptic.com/epnotes/type_i_type_ii.php\n",
      "1\n",
      "Introduction\n",
      "http://dataskeptic.com/epnotes/ep001.php\n"
     ]
    }
   ],
   "source": [
    "for desc in descriptions:\n",
    "    print(descToNum[desc])\n",
    "    print(descToTitle[desc])\n",
    "    print(descToLink[desc])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save description in txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thefile = open('./text/episode_descs_titles.txt', 'w')\n",
    "\n",
    "# for i, desc in enumerate(descriptions):\n",
    "#     desc = desc.encode('utf-8').strip()\n",
    "    \n",
    "#     desc = \"*\"+ str(i)+str(desc).replace('\\n', \"\") \n",
    "#     thefile.write(\"%s\\n\" % desc)\n",
    "\n",
    "for i in range(len(descriptions)):\n",
    "    desc = descriptions[i]\n",
    "    title = descToTitle[desc]\n",
    "    \n",
    "    desc = desc.encode('utf-8').strip()\n",
    "    desc = str(desc).replace('\\n', \"\") \n",
    "    #print(desc)\n",
    "    title = title.replace('[MINI]', \"\")\n",
    "    title = title.encode('utf-8').strip()\n",
    "    title = \"*\"+ str(i)+str(title).replace('\\n', \"\") \n",
    "    #print(title)\n",
    "    thefile.write(\"%s\\n\" % str(title+\", \"+desc))\n",
    "    #print(title+\" \"+desc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "*0b'Generative AI for Content Creation',\n",
      "2\n",
      "*1b'One Shot Learning', b'One Shot Learn\n",
      "3\n",
      "*2b'Recommender Systems Live from FARCON\n",
      "4\n",
      "*3b'Long Short Term Memory', b'Thanks to\n",
      "5\n",
      "*4b'Zillow Zestimate', b'Zillow is a lea\n",
      "6\n",
      "*5b'Cardiologist Level Arrhythmia Detect\n",
      "7\n",
      "*6b'Recurrent Neural Networks', b'RNNs a\n",
      "8\n",
      "*7b'Project Common Voice', b\"Thanks to o\n",
      "9\n",
      "*8b'Bayesian Belief Networks', b\"A Bayes\n",
      "10\n",
      "*9b'pix2code', b'In this episode, Tony B\n",
      "11\n",
      "*10b'Conditional Independence', b\"In sta\n",
      "12\n",
      "*11b'Estimating Sheep Pain with Facial R\n",
      "13\n",
      "*12b'CosmosDB', b'This episode collects \n",
      "14\n",
      "*13b'The Vanishing Gradient', b'This epi\n",
      "15\n",
      "*14b'Doctor AI', b'hen faced with medica\n",
      "16\n",
      "*15b'Activation Functions', b'In a neura\n",
      "17\n",
      "*16b'MS Build 2017', b'This episode reca\n",
      "18\n",
      "*17b'Max-pooling', b\"Max-pooling is a pr\n",
      "19\n",
      "*18b'Unsupervised Depth Perception', b'T\n",
      "20\n",
      "*19b'Convolutional Neural Networks', b\"C\n",
      "21\n",
      "*20b'Multi-Agent Diverse Generative Adve\n",
      "22\n",
      "*21b'Generative Adversarial Networks', b\n",
      "23\n",
      "*22b'Opinion Polls for Presidential Elec\n",
      "24\n",
      "*23b'OpenHouse', b\"No reliable, complete\n",
      "25\n",
      "*24b'GPU CPU', b'There\\'s more than one \n",
      "26\n",
      "*25b'Backpropagation', b'Backpropagation\n",
      "27\n",
      "*26b'Data Science at Patreon', b\"In this\n",
      "28\n",
      "*27b'Feed Forward Neural Networks', b\"Fe\n",
      "29\n",
      "*28b'Reinventing Sponsored Search Auctio\n",
      "30\n",
      "*29b'The Perceptron', b\"Today's episode \n",
      "31\n",
      "*30b'The Data Refuge Project', b'DataRef\n",
      "32\n",
      "*31b'Automated Feature Engineering', b'I\n",
      "33\n",
      "*32b'Big Data Tools and Trends', b'In th\n",
      "34\n",
      "*33b'Primer on Deep Learning', b'In this\n",
      "35\n",
      "*34b'Data Provenance and Reproducibility\n",
      "36\n",
      "*35b'Logistic Regression on Audio Data',\n",
      "37\n",
      "*36b'Studying Competition and Gender Thr\n",
      "38\n",
      "*37b'Dropout', b\"Deep learning can be pr\n",
      "39\n",
      "*38b'The Police Data and the Data Driven\n",
      "40\n",
      "*39b'The Library Problem', b'We close ou\n",
      "41\n",
      "*40b'2016 Holiday Special', b\"Today's ep\n",
      "42\n",
      "*41b'Entropy', b\"Classically, entropy is\n",
      "43\n",
      "*42b'MS Connect Conference', b'Cloud ser\n",
      "44\n",
      "*43b'Causal Impact', b\"Today's episode i\n",
      "45\n",
      "*44b'The Bootstrap', b\"The Bootstrap is \n",
      "46\n",
      "*45b'Gini Coefficients', b'The Gini Coef\n",
      "47\n",
      "*46b'Unstructured Data for Finance', b\"F\n",
      "48\n",
      "*47b'AdaBoost', b'AdaBoost is a canonica\n",
      "49\n",
      "*48b'Stealing Models from the Cloud', b'\n",
      "50\n",
      "*49b'Calculating Feature Importance', b'\n",
      "51\n",
      "*50b'NYC Bike Share Rebalancing', b'As c\n",
      "52\n",
      "*51b'Random Forest', b'Random forest is \n",
      "53\n",
      "*52b'Election Predictions', b\"Jo Hardin \n",
      "54\n",
      "*53b'F1 Score', b'The F1 score is a mode\n",
      "55\n",
      "*54b'Urban Congestion', b\"Urban congesti\n",
      "56\n",
      "*55b'Heteroskedasticity', b\"Heteroskedas\n",
      "57\n",
      "*56b'Music21', b\"Our guest today is Mich\n",
      "58\n",
      "*57b'Paxos', b'Paxos is a protocol for a\n",
      "59\n",
      "*58b'Trusting Machine Learning Models wi\n",
      "60\n",
      "*59b'ANOVA', b'Analysis of variance is a\n",
      "61\n",
      "*60b'Machine Learning on Images with Noi\n",
      "62\n",
      "*61b'Survival Analysis', b'Survival anal\n",
      "63\n",
      "*62b'Predictive Models on Random Data', \n",
      "64\n",
      "*63b'Receiver Operating Characteristic (\n",
      "65\n",
      "*64b'Multiple Comparisons and Conversion\n",
      "66\n",
      "*65b'Leakage', b\"If you'd like to make a\n",
      "67\n",
      "*66b'Predictive Policing', b'Kristian Lu\n",
      "68\n",
      "*67b'The CAP Theorem', b'Distributed com\n",
      "69\n",
      "*68b'Detecting Terrorists with Facial Re\n",
      "70\n",
      "*69b\"Goodhart's Law\", b'Goodhart\\'s law \n",
      "71\n",
      "*70b'Data Science at eHarmony', b\"I'm jo\n",
      "72\n",
      "*71b'Stationarity and Differencing', b'M\n",
      "73\n",
      "*72b'Feather', b\"I'm joined by Wes McKin\n",
      "74\n",
      "*73b'Bargaining', b'Bargaining is the pr\n",
      "75\n",
      "*74b'deepjazz', b'Deepjazz is a project \n",
      "76\n",
      "*75b'Auto-correlative functions and corr\n",
      "77\n",
      "*76b'Early Identification of Violent Cri\n",
      "78\n",
      "*77b'Fractional Factorial Design', b'A d\n",
      "79\n",
      "*78b'Machine Learning Done Wrong', b'Che\n",
      "80\n",
      "*79b'Potholes', b\"Co-host Linh Da was in\n",
      "81\n",
      "*80b'The Elbow Method', b'Certain data m\n",
      "82\n",
      "*81b'Too Good to be True', b'Today on Da\n",
      "83\n",
      "*82b'R-squared', b\"How well does your mo\n",
      "84\n",
      "*83b'Models of Mental Simulation', b'Jes\n",
      "85\n",
      "*84b'Multiple Regression', b'This episod\n",
      "86\n",
      "*85b\"Scientific Studies of People's Rela\n",
      "87\n",
      "*86b'k-d trees', b'This episode reviews \n",
      "88\n",
      "*87b'Auditing Algorithms', b'Algorithms \n",
      "89\n",
      "*88b'The Bonferroni Correction', b\"Today\n",
      "90\n",
      "*89b'Detecting Pseudo-profound BS', b\"A \n",
      "91\n",
      "*90b'Gradient Descent', b\"Today's mini e\n",
      "92\n",
      "*91b\"Let's Kill the Word Cloud\", b\"This \n",
      "93\n",
      "*92b'2015 Holiday Special', b\"Today's ep\n",
      "94\n",
      "*93b'Wikipedia Revision Scoring as a Ser\n",
      "95\n",
      "*94b'Term Frequency - Inverse Document F\n",
      "96\n",
      "*95b'The Hunt for Vulcan', b'Early astro\n",
      "97\n",
      "*96b'The Accuracy Paradox', b\"Today's ep\n",
      "98\n",
      "*97b\"Neuroscience from a Data Scientist'\n",
      "99\n",
      "*98b'Bias Variance Tradeoff', b\"A discus\n",
      "100\n",
      "*99b\"Big Data Doesn't Exist\", b'The rece\n",
      "101\n",
      "*100b'Covariance and Correlation', b'The\n",
      "102\n",
      "*101b'Bayesian A/B Testing', b\"Today's g\n",
      "103\n",
      "*102b'The Central Limit Theorem', b'The \n",
      "104\n",
      "*103b'Accessible Technology', b\"Today's \n",
      "105\n",
      "*104b'Multi-armed Bandit Problems', b'Th\n",
      "106\n",
      "*105b'Shakespeare, Abiogenesis, and Exop\n",
      "107\n",
      "*106b'Sample Sizes', b\"There are several\n",
      "108\n",
      "*107b'The Model Complexity Myth', b\"Ther\n",
      "109\n",
      "*108b'Distance Measures', b'There are ma\n",
      "110\n",
      "*109b'ContentMine', b\"ContentMine is a p\n",
      "111\n",
      "*110b'Structured and Unstructured Data',\n",
      "112\n",
      "*111b'Measuring the Influence of Fashion\n",
      "113\n",
      "*112b'PageRank', b'PageRank is the algor\n",
      "114\n",
      "*113b'Data Science at Work in LA County'\n",
      "115\n",
      "*114b'k-Nearest Neighbors', b'This episo\n",
      "116\n",
      "*115b'Crypto', b'How do people think rat\n",
      "117\n",
      "*116b'MapReduce', b'This mini-episode is\n",
      "118\n",
      "*117b'Genetically Engineered Food and Tr\n",
      "119\n",
      "*118b'The Curse of Dimensionality', b\"Mo\n",
      "120\n",
      "*119b'Video Game Analytics', b'This epis\n",
      "121\n",
      "*120b\"Anscombe's Quartet\", b\"This mini-e\n",
      "122\n",
      "*121b'Proposing Annoyance Mining', b'A r\n",
      "123\n",
      "*122b'Preserving History at Cyark', b\"El\n",
      "124\n",
      "*123b'A Critical Examination of a Study \n",
      "125\n",
      "*124b'Detecting Cheating in Chess', b\"Wi\n",
      "126\n",
      "*125b'z-scores', b\"This week's episode d\n",
      "127\n",
      "*126b'Using Data to Help Those in Crisis\n",
      "128\n",
      "*127b'The Ghost in the MP3', b\"Have you \n",
      "129\n",
      "*128b'Data Fest 2015', b'This episode co\n",
      "130\n",
      "*129b'Cornbread and Overdispersion', b'F\n",
      "131\n",
      "*130b'Natural Language Processing', b'Th\n",
      "132\n",
      "*131b'Computer-based Personality Judgmen\n",
      "133\n",
      "*132b'Markov Chain Monte Carlo', b'This \n",
      "134\n",
      "*133b'Markov Chains', b\"This episode int\n",
      "135\n",
      "*134b'Oceanography and Data Science', b\"\n",
      "136\n",
      "*135b'Ordinary Least Squares Regression'\n",
      "137\n",
      "*136b'NYC Speed Camera Analysis with Tim\n",
      "138\n",
      "*137b'k-means clustering', b'The k-means\n",
      "139\n",
      "*138b'Shadow Profiles on Social Networks\n",
      "140\n",
      "*139b'The Chi-Squared Test', b'The \\xcf\\\n",
      "141\n",
      "*140b'Mapping Reddit Topics with Randy O\n",
      "142\n",
      "*141b'Partially Observable State Spaces'\n",
      "143\n",
      "*142b'Easily Fooling Deep Neural Network\n",
      "144\n",
      "*143b'Data Provenance', b'This episode i\n",
      "145\n",
      "*144b'Doubtful News, Geology, Investigat\n",
      "146\n",
      "*145b'Belief in Santa', b'In this quick \n",
      "147\n",
      "*146b'Economic Modeling and Prediction, \n",
      "148\n",
      "*147b'The Battle of the Sexes', b'Love a\n",
      "149\n",
      "*148b'The Science of Online Data at Plen\n",
      "150\n",
      "*149b'The Girlfriend Equation', b'Econom\n",
      "151\n",
      "*150b'The Secret and the Global Consciou\n",
      "152\n",
      "*151b'Monkeys on Typewriters', b'What is\n",
      "153\n",
      "*152b'Mining the Social Web with Matthew\n",
      "154\n",
      "*153b'Is the Internet Secure?', b'This e\n",
      "155\n",
      "*154b'Practicing and Communicating Data \n",
      "156\n",
      "*155b'The T-Test', b\"The t-test is this \n",
      "157\n",
      "*156b'Data Myths with Karl Mamer', b\"Thi\n",
      "158\n",
      "*157b'Contest Announcement', b'The Data \n",
      "159\n",
      "*158b'Selection Bias', b'A discussion ab\n",
      "160\n",
      "*159b'Confidence Intervals', b'Commute t\n",
      "161\n",
      "*160b'Value of Information', b'A discuss\n",
      "162\n",
      "*161b'Game Science Dice with Louis Zocch\n",
      "163\n",
      "*162b'Data Science at ZestFinance with M\n",
      "164\n",
      "*163b'Decision Tree Learning', b'Linhda \n",
      "165\n",
      "*164b'Jackson Pollock Authentication Ana\n",
      "166\n",
      "*165b'Noise!!', b'Our topic for this wee\n",
      "167\n",
      "*166b'Guerilla Skepticism on Wikipedia w\n",
      "168\n",
      "*167b'Ant Colony Optimization', b\"In thi\n",
      "169\n",
      "*168b'Data in Healthcare IT with Shahid \n",
      "170\n",
      "*169b'Cross Validation', b'This miniepis\n",
      "171\n",
      "*170b'Streetlight Outage and Crime Rate \n",
      "172\n",
      "*171b'Experimental Design', b'This episo\n",
      "173\n",
      "*172b'The Right (big data) Tool for the \n",
      "174\n",
      "*173b'Bayesian Updating', b\"In this mini\n",
      "175\n",
      "*174b'Personalized Medicine with Niki At\n",
      "176\n",
      "*175b'p-values', b'In this mini, we disc\n",
      "177\n",
      "*176b'Advertising Attribution with Natha\n",
      "178\n",
      "*177b'type i / type ii errors', b'In thi\n",
      "179\n",
      "*178b'Introduction', b'The Data Skeptic \n"
     ]
    }
   ],
   "source": [
    "with open('./text/episode_descs_titles.txt', 'r') as f:\n",
    "    i=0\n",
    "    for line in f:\n",
    "        i+=1\n",
    "        print(i)\n",
    "        print(line[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i # it should be 179 before 10/3/2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop here and the rest can be done by rcm.py.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the word vectors trained from SO to represent episode descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get word vectors trained from SO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a_</th>\n",
       "      <td>-0.502157</td>\n",
       "      <td>-1.231525</td>\n",
       "      <td>1.501073</td>\n",
       "      <td>-0.824721</td>\n",
       "      <td>0.591714</td>\n",
       "      <td>-0.488768</td>\n",
       "      <td>0.903779</td>\n",
       "      <td>-0.038645</td>\n",
       "      <td>0.459253</td>\n",
       "      <td>-0.485186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276586</td>\n",
       "      <td>-0.661979</td>\n",
       "      <td>0.297288</td>\n",
       "      <td>0.136056</td>\n",
       "      <td>-0.305163</td>\n",
       "      <td>1.396190</td>\n",
       "      <td>-0.239185</td>\n",
       "      <td>1.228788</td>\n",
       "      <td>0.333640</td>\n",
       "      <td>0.492632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a__</th>\n",
       "      <td>-0.044594</td>\n",
       "      <td>-0.024119</td>\n",
       "      <td>0.018766</td>\n",
       "      <td>0.024026</td>\n",
       "      <td>-0.028895</td>\n",
       "      <td>-0.022439</td>\n",
       "      <td>0.013539</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>-0.009192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024952</td>\n",
       "      <td>-0.038260</td>\n",
       "      <td>-0.004415</td>\n",
       "      <td>-0.019359</td>\n",
       "      <td>-0.053633</td>\n",
       "      <td>-0.015187</td>\n",
       "      <td>0.011295</td>\n",
       "      <td>-0.014313</td>\n",
       "      <td>0.038858</td>\n",
       "      <td>-0.030853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_a</th>\n",
       "      <td>0.013455</td>\n",
       "      <td>-0.012830</td>\n",
       "      <td>-0.017656</td>\n",
       "      <td>0.014753</td>\n",
       "      <td>0.062009</td>\n",
       "      <td>-0.132435</td>\n",
       "      <td>0.068699</td>\n",
       "      <td>0.045951</td>\n",
       "      <td>0.074507</td>\n",
       "      <td>-0.035419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028048</td>\n",
       "      <td>0.040826</td>\n",
       "      <td>0.069079</td>\n",
       "      <td>0.088605</td>\n",
       "      <td>0.082241</td>\n",
       "      <td>0.211491</td>\n",
       "      <td>0.126767</td>\n",
       "      <td>0.039316</td>\n",
       "      <td>-0.028075</td>\n",
       "      <td>-0.026345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_adjusted</th>\n",
       "      <td>0.028670</td>\n",
       "      <td>0.020240</td>\n",
       "      <td>0.025201</td>\n",
       "      <td>-0.070229</td>\n",
       "      <td>0.012619</td>\n",
       "      <td>-0.078576</td>\n",
       "      <td>0.051204</td>\n",
       "      <td>0.040304</td>\n",
       "      <td>0.027123</td>\n",
       "      <td>0.021348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>-0.060311</td>\n",
       "      <td>0.086649</td>\n",
       "      <td>0.062956</td>\n",
       "      <td>0.068462</td>\n",
       "      <td>0.053034</td>\n",
       "      <td>0.037799</td>\n",
       "      <td>-0.016378</td>\n",
       "      <td>0.058798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_and_b</th>\n",
       "      <td>0.114285</td>\n",
       "      <td>0.039049</td>\n",
       "      <td>0.213730</td>\n",
       "      <td>-0.080442</td>\n",
       "      <td>-0.088148</td>\n",
       "      <td>-0.197033</td>\n",
       "      <td>0.049060</td>\n",
       "      <td>0.106649</td>\n",
       "      <td>-0.017760</td>\n",
       "      <td>-0.002527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111197</td>\n",
       "      <td>0.030113</td>\n",
       "      <td>-0.044530</td>\n",
       "      <td>0.090094</td>\n",
       "      <td>0.085388</td>\n",
       "      <td>0.165226</td>\n",
       "      <td>0.098310</td>\n",
       "      <td>0.086020</td>\n",
       "      <td>-0.143789</td>\n",
       "      <td>-0.047890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5  \\\n",
       "a_         -0.502157 -1.231525  1.501073 -0.824721  0.591714 -0.488768   \n",
       "a__        -0.044594 -0.024119  0.018766  0.024026 -0.028895 -0.022439   \n",
       "a_a         0.013455 -0.012830 -0.017656  0.014753  0.062009 -0.132435   \n",
       "a_adjusted  0.028670  0.020240  0.025201 -0.070229  0.012619 -0.078576   \n",
       "a_and_b     0.114285  0.039049  0.213730 -0.080442 -0.088148 -0.197033   \n",
       "\n",
       "                   6         7         8         9    ...          190  \\\n",
       "a_          0.903779 -0.038645  0.459253 -0.485186    ...     0.276586   \n",
       "a__         0.013539  0.007449  0.006484 -0.009192    ...     0.024952   \n",
       "a_a         0.068699  0.045951  0.074507 -0.035419    ...    -0.028048   \n",
       "a_adjusted  0.051204  0.040304  0.027123  0.021348    ...    -0.016276   \n",
       "a_and_b     0.049060  0.106649 -0.017760 -0.002527    ...    -0.111197   \n",
       "\n",
       "                 191       192       193       194       195       196  \\\n",
       "a_         -0.661979  0.297288  0.136056 -0.305163  1.396190 -0.239185   \n",
       "a__        -0.038260 -0.004415 -0.019359 -0.053633 -0.015187  0.011295   \n",
       "a_a         0.040826  0.069079  0.088605  0.082241  0.211491  0.126767   \n",
       "a_adjusted  0.008707 -0.060311  0.086649  0.062956  0.068462  0.053034   \n",
       "a_and_b     0.030113 -0.044530  0.090094  0.085388  0.165226  0.098310   \n",
       "\n",
       "                 197       198       199  \n",
       "a_          1.228788  0.333640  0.492632  \n",
       "a__        -0.014313  0.038858 -0.030853  \n",
       "a_a         0.039316 -0.028075 -0.026345  \n",
       "a_adjusted  0.037799 -0.016378  0.058798  \n",
       "a_and_b     0.086020 -0.143789 -0.047890  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 'word2vector_model_question_answer_200_6_2'\n",
    "fname = './word_vec/'+key+\".csv\"\n",
    "word_vecs_df = pd.read_csv(fname,index_col=0)\n",
    "word_vecs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87750"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = word_vecs_df.index\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = './vocab_dict/vocab_dict_question_answer_200_6_2.csv'\n",
    "with open(fname, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    vocab_dic = dict(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, value in vocab_dic.items():\n",
    "    vocab_dic[k] = int(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dic['a_a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the text in episode descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                \n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "                #This lowercases, tokenizes, de-accents (optional). – the output are final tokens = unicode strings, that won’t be processed any further.\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = './text/episode_descs_titles.txt'\n",
    "episode_desc_title_corpus = list(read_corpus(fname, tokens_only= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for desc in episode_desc_title_corpus:\n",
    "    corpus.append(\" \".join(desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generative ai for content creation last year the film development and production company end cue produced short film called sunspring that was entirely written by an artificial intelligence using neural networks more specifically it was authored by recurrent neural network rnn called long short term memory lstm according to end cue xe chief technical officer deb ray the company has come long way in improving the generative ai aspect of the bot in this episode deb ray joins host kyle polich to discuss how generative ai models are being applied in creative processes such as screenwriting their discussion also explores how data science for analyzing development projects such as financing and selecting scripts as well as optimizing the content production process'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = './text/episode_corpus.txt'\n",
    "with open(fname, 'w') as f:\n",
    "    for c in corpus:\n",
    "        f.write(\"%s\\n\" % c)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tf_idf features of episode descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1,vocabulary = vocab_dic)\n",
    "X = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: [vocabulary] Mapping or iterable, optional\n",
    "Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words are in episode descriptions but not in the vocab of SO?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************\n",
      "23\n",
      "{'iamzareenf', 'joytafty', 'dashboarding', 'aglpjrmp', 'periscopedata', 'rosevere', 'blueplastic', 'zareen', 'openhouse', 'zehr', 'jawbone', 'cataloging'}\n",
      "No reliable, complete database cataloging home sales data at a transaction level is available for the average person to access. To a data scientist interesting in studying this data, our hands are complete tied. Opportunities like testing sociological theories, exploring economic impacts, study market forces, or simply research the value of an investment when buying a home are all blocked by the lack of easy access to this dataset. OpenHouse seeks to correct that by centralizing and standardizing all publicly available home sales transactional data. In this episode, we discuss the achievements of OpenHouse to date, and what plans exist for the future.     Check out the OpenHouse gallery.    I also encourage everyone to check out the project Zareen mentioned which was her Harry Potter word2vec webapp and Joy's project doing data visualization on Jawbone data. Guests Thanks again to @iamzareenf, @blueplastic, and @joytafty for coming on the show. Thanks to the numerous other volunteers who have helped with the project as well! Announcements and details   If you're interested in getting involved in OpenHouse, check out the OpenHouse contributor's quickstart page.   Kyle is giving a  machine learning talk in Los Angeles on May 25th, 2017 at Zehr.   Sponsor Thanks to our sponsor for this episode Periscope Data. The blog post demoing their maps option is on our blog titled Periscope Data Maps.  To start a free trial of their dashboarding too, visit http://periscopedata.com/skeptics Kyle recently did a youtube video exploring the Data Skeptic podcast download numbers using Periscope Data. Check it out at https://youtu.be/aglpJrMp0M4. Supplemental music is Lee Rosevere's Let's Start at the Beginning.  \n",
      "*******************************************************\n",
      "26\n",
      "{'maura', 'meneses', 'benefactors', 'omr', 'patreon', 'polich'}\n",
      "  In this week's episode of Data Skeptic, host Kyle Polich talks with guest Maura Church, Patreon's data science manager. Patreon is a fast-growing crowdfunding platform that allows artists and creators of all kinds build their own subscription content service. The platform allows fans to become patrons of their favorite artists- an idea similar the Renaissance times, when musicians would rely on benefactors to become their patrons so they could make more art. At Patreon, Maura's data science team strives to provide creators with insight, information, and tools, so that creators can focus on what they do best-- making art. On the show, Maura talks about some of her projects with the data science team at Patreon. Among the several topics discussed during the episode include: optical music recognition (OMR) to translate musical scores to electronic format, network analysis to understand the connection between creators and patrons, growth forecasting and modeling in a new market, and churn modeling to determine predictors of long time support. A more detailed explanation of Patreon's A/B testing framework can be found  here Other useful links to topics mentioned during the show:  OMR research Patreon blog Patreon HQ blog Amanda Palmer Fran Meneses\n",
      "*******************************************************\n",
      "34\n",
      "{'gopher', 'periscopedata', 'whitenack', 'rosevere', 'pachyderm', 'containerized'}\n",
      "Versioning isn't just for source code. Being able to track changes to data is critical for answering questions about data provenance, quality, and reproducibility. Daniel Whitenack joins me this week to talk about these concepts and share his work on Pachyderm. Pachyderm is an open source containerized data lake. During the show, Daniel mentioned the Gopher Data Science github repo as a great resource for any data scientists interested in the Go language. Although we didn't mention it, Daniel also did an interesting analysis on the 2016 world chess championship that complements our recent episode on chess well. You can find that post  here Supplemental music is Lee Rosevere's Let's Start at the Beginning.   Thanks to Periscope Data for sponsoring this episode. More about them at periscopedata.com/skeptics       \n",
      "*******************************************************\n",
      "76\n",
      "{'shaabani', 'shakarian', 'pauloshakasu', 'elham', 'onarxiv', 'cysis'}\n",
      "This week I spoke with Elham Shaabani and Paulo Shakarian (@PauloShakASU) about their recent paper Early Identification of Violent Criminal Gang Members (also available onarXiv). In this paper, they use social network analysis techniques and machine learning to provide early detection of known criminal offenders who are in a high risk group for committing violent crimes in the future. Their techniques outperform existing techniques used by the police. Elham and Paulo are part of the Cyber-Socio Intelligent Systems (CySIS) Lab.\n",
      "*******************************************************\n",
      "89\n",
      "{'profundity', 'pseudoprofound', 'receptivity', 'gordonpennycook', 'pennycook', 'nonesense', 'deepak', 'bsr'}\n",
      " A recent paper in the journal of Judgment and Decision Making titled On the reception and detection of pseudo-profound bullshit explores empirical questions around a reader's ability to detect statements which may sound profound but are actually a collection of buzzwords that fail to contain adequate meaning or truth. These statements are definitively different from lies and nonesense, as we discuss in the episode. This paper proposes the Bullshit Receptivity scale (BSR) and empirically demonstrates that it correlates with existing metrics like the Cognitive Reflection Test, building confidence that this can be a useful, repeatable, empirical measure of a person's ability to detect pseudo-profound statements as being different from genuinely profound statements. Additionally, the correlative results provide some insight into possible root causes for why individuals might find great profundity in these statements based on other beliefs or cognitive measures. The paper's lead author Gordon Pennycook joins me to discuss this study's results. If you'd like some examples of pseudo-profound bullshit, you can randomly generate some based on Deepak Chopra's twitter feed. To read other work from Gordon, check out his Google Scholar page and find him on twitter via @GordonPennycook. And just for fun, if you think you've dreamed up a Data Skeptic related pseudo-profound bullshit statement, tweet it with hashtag #pseudoprofound. If I see an especially clever or humorous one, I might want to send you a free Data Skeptic sticker.   \n",
      "*******************************************************\n",
      "95\n",
      "{'documentaries', 'xac', 'vulcan', 'tomlevenson', 'urbain', 'athttps', 'inversesquare', 'verrier', 'neptune', 'levenson', 'cosmos', 'counterfeiter'}\n",
      "Early astronomers could see several of the planets with the naked eye. The invention of the telescope allowed for further understanding of our solar system. The work of Isaac Newton allowed later scientists to accurately predict Neptune, which was later observationally confirmed exactly where predicted. It seemed only natural that a similar unknown body might explain anomalies in the orbit of Mercury, and thus began the search for the hypothesized planet Vulcan. Thomas Levenson's book \"The Hunt for Vulcan\" is a narrative of the key scientific minds involved in the search and eventual refutation of an unobserved planet between Mercury and the sun. Thomas joins me in this episode to discuss his book and the fascinating story of the quest to find this planet. During the discussion, we mention one of the contributions made by Urbain-Jean-Joseph Le Verrier which involved some complex calculations which enabled him to predict where to find the planet that would eventually be called Neptune. The calculus behind this work is difficult, and some of that work is demonstrated in a Jupyter notebook I recently discovered from Paulo Marques titled The-Body Problem.      Thomas Levenson is a professor at MIT and head of its science writing program. He is the author of several books, including Einstein in Berlin and Newton and the Counterfeiter: The Unknown Detective Career of the Worldâ€™s Greatest Scientist. He has also made ten feature-length documentaries (including a two-hour Nova program on Einstein) for which he has won numerous awards. In his most recent book \"The Hunt for Vulcan\", explores the century spanning quest to explain the movement of the cosmos via theory and the role the hypothesized planet Vulcan played in the story. Follow Thomas on twitter @tomlevenson and check out his blog athttps://inversesquare.wordpress.com/. Pick up your copy of The Hunt for Vulcan at your local bookstore, preferred book buying place, or at the Penguin Random House site.    \n",
      "*******************************************************\n",
      "103\n",
      "{'skeptibility', 'activist', 'braille', 'chrishofstader', 'skepchick', 'dqtech', 'hofstader', 'bandcamp', 'atheist', 'refreshable', 'nomads', 'shelley', 'tenon', 'gonz_blinko'}\n",
      "Today's guest is Chris Hofstader (@gonz_blinko), an accessibility researcher and advocate, as well as an activist for causes such as improving access to information for blind and vision impaired people. His background in computer programming enabled him to be the leader of JAWS, a Windows program that allowed people with a visual impairment to read their screen either through text-to-speech or a refreshable braille display. He's the Managing Member of 3 Mouse Technology. He's also a frequent blogger primarily at chrishofstader.com. For web developers and site owners, Chris recommends two tools to help test for accessibility issues: tenon.io and dqtech.co. A guest post from Chris appeared on the Skepchick blogged titled Skepticism and Disability which lead to the formation of the sister site Skeptibility. In a discussion of skepticism and favorite podcasts, Chris mentioned a number of great shows, most notably The Pod Delusion to which he was a contributor. Additionally, Chris has also appeared on The Atheist Nomads. Lastly, a shout out from Chris to musician Shelley Segal whom he hosted just before the date of recording of this episode. Her music can be found on her site or via bandcamp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************\n",
      "148\n",
      "{'matchmaking', 'caretlibrary', 'withthomas', 'levi', 'pof', 'tslevi'}\n",
      "Can algorithms help you find love? Many happy couples successfully brought together via online dating websites show us that data science can help you find love. I'm joined this week by Thomas Levi, Senior Data Scientist at Plenty of Fish, to discuss some of his work which helps people find one another as efficiently as possible. Matchmaking is a truly non-trivial problem, and one that's dynamically changing all the time as new users join and leave the \"pool of fish\". This episode explores the aspects of what makes this a tough problem and some of the ways POF has been successfully using data science to solve it, and continues to try to innovate with new techniques like interest matching. For his benevolent references, Thomas suggests readers check out All of Statistics as well as the caretlibrary for R. And for a self serving recommendation, follow him on twitter (@tslevi) or connect withThomas Levi on Linkedin.\n",
      "*******************************************************\n",
      "161\n",
      "{'satanic', 'dicecollector', 'outro', 'swears', 'zocchi', 'awesomedice', 'unnoticably'}\n",
      "In this bonus episode, guest Louis Zocchi discusses his background in the gaming industry, specifically, how he became a manufacturer of dice designed to produce statistically uniform outcomes. During the show Louis mentioned a two part video listeners might enjoy: part 1 and part 2 can both be found on youtube. Kyle mentioned a robot capable of unnoticably cheating at Rock Paper Scissors / Ro Sham Bo. More details can be found here. Louis mentioned dice collector Kevin Cook whose website is DiceCollector.com While we're on the subject of table top role playing games, Kyle recommends these two related podcasts listeners might enjoy: The Conspiracy Skeptic podcast (on which host Kyle was recently a guest) had a great episode \"Dungeons and Dragons - The Devil's Game?\" which explores claims of D&Ds alleged ties to skepticism. Also, Kyle swears there's a great Monster Talk episode discussing claims of a satanic connection to Dungeons and Dragons, but despite mild efforts to locate it, he came up empty. Regardless, listeners of the Data Skeptic Podcast are encouraged to explore the back catalog to try and find the aforementioned episode of this great podcast. Last but not least, as mentioned in the outro, awesomedice.com did some great independent empirical testing that confirms Game Science dice are much closer to the desired uniform distribution over possible outcomes when compared to one leading manufacturer.\n",
      "*******************************************************\n",
      "164\n",
      "{'hockney', 'micolich', 'drip', 'falco', 'gogh', 'quasicrystal', 'mathur', 'brushstroke'}\n",
      "Our guest this week is Hamilton physics professor Kate Jones-Smith who joins us to discuss the evidence for the claim that drip paintings of Jackson Pollock contain fractal patterns. This hypothesis originates in a paper by Taylor, Micolich, and Jonas titled Fractal analysis of Pollock's drip paintings which appeared in Nature. Kate and co-author Harsh Mathur wrote a paper titled Revisiting Pollock's Drip Paintings which also appeared in Nature. A full text PDF can be found here, but lacks the helpful figures which can be found here, although two images are blurred behind a paywall. Their paper was covered in the New York Times as well as in USA Today (albeit with with a much more delightful headline: Never mind the Pollock's [sic]). While discussing the intersection of science and art, the conversation also touched briefly on a few other intersting topics. For example, Penrose Tiles appearing in islamic art (pre-dating Roger Penrose's investigation of the interesting properties of these tiling processes), Quasicrystal designs in art, Automated brushstroke analysis of the works of Vincent van Gogh, and attempts to authenticate a possible work of Leonardo Da Vinci of uncertain provenance. Last but not least, the conversation touches on the particularly compellingHockney-Falco Thesis which is also covered in David Hockney's book Secret Knowledge. For those interested in reading some of Kate's other publications, many Katherine Jones-Smith articles can be found at the given link, all of which have downloadable PDFs.\n",
      "*******************************************************\n",
      "168\n",
      "{'supersite', 'chairperson', 'shahid', 'netspective', 'cio', 'hitsphere', 'osehra'}\n",
      "Our guest this week is Shahid Shah. Shahid is CEO at Netspective, and writes three blogs: Health Care Guy, Shahid Shah, and HitSphere - the Healthcare IT Supersite.  During the program, Kyle recommended a talk from the 2014 MIT Sloan CIO Symposium entitled  Transforming \"Digital Silos\" to \"Digital Care Enterprise\"  which was hosted by our guest Shahid Shah.  In addition to his work in Healthcare IT, he also the chairperson for Open Source Electronic Health Record Alliance, an non-profit organization that, amongst other activities, is hosting an upcoming conference. The 3rd annual  OSEHRA Open Source Summit: Global Collaboration in Healthcare IT , which will be taking place September 3-5, 2014 in Washington DC.  For our benevolent recommendation, Shahid suggested listeners may benefit from taking the time to read  books on leadership for the insights they provide. For our self-serving recommendation, Shahid recommended listeners check out his company Netspective , if you are working with a company looking for help getting started building software utilizing next generation technologies.\n"
     ]
    }
   ],
   "source": [
    "# What words are not in SO?\n",
    "for i in range(179):\n",
    "#     print(len(X[i,:].nonzero()[1]))\n",
    "#     print(len(set(corpus[i].split(' '))))\n",
    "    diff = set(corpus[i].split(' ')).difference(set(vocab))\n",
    "    if len(diff) >5:\n",
    "        print('*******************************************************')\n",
    "        print(i)\n",
    "        print(diff)  \n",
    "        print(descriptions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those words are either people's names, website names and they don't affect the main ideas and I feel it is ok to filter them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 87750)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get weighted doc vectors for all episode description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generative',\n",
       " 'ai',\n",
       " 'for',\n",
       " 'content',\n",
       " 'creation',\n",
       " 'last',\n",
       " 'year',\n",
       " 'the',\n",
       " 'film',\n",
       " 'development',\n",
       " 'and',\n",
       " 'production',\n",
       " 'company',\n",
       " 'end',\n",
       " 'cue',\n",
       " 'produced',\n",
       " 'short',\n",
       " 'film',\n",
       " 'called',\n",
       " 'sunspring',\n",
       " 'that',\n",
       " 'was',\n",
       " 'entirely',\n",
       " 'written',\n",
       " 'by',\n",
       " 'an',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'using',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'more',\n",
       " 'specifically',\n",
       " 'it',\n",
       " 'was',\n",
       " 'authored',\n",
       " 'by',\n",
       " 'recurrent',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'rnn',\n",
       " 'called',\n",
       " 'long',\n",
       " 'short',\n",
       " 'term',\n",
       " 'memory',\n",
       " 'lstm',\n",
       " 'according',\n",
       " 'to',\n",
       " 'end',\n",
       " 'cue',\n",
       " 'xe',\n",
       " 'chief',\n",
       " 'technical',\n",
       " 'officer',\n",
       " 'deb',\n",
       " 'ray',\n",
       " 'the',\n",
       " 'company',\n",
       " 'has',\n",
       " 'come',\n",
       " 'long',\n",
       " 'way',\n",
       " 'in',\n",
       " 'improving',\n",
       " 'the',\n",
       " 'generative',\n",
       " 'ai',\n",
       " 'aspect',\n",
       " 'of',\n",
       " 'the',\n",
       " 'bot',\n",
       " 'in',\n",
       " 'this',\n",
       " 'episode',\n",
       " 'deb',\n",
       " 'ray',\n",
       " 'joins',\n",
       " 'host',\n",
       " 'kyle',\n",
       " 'polich',\n",
       " 'to',\n",
       " 'discuss',\n",
       " 'how',\n",
       " 'generative',\n",
       " 'ai',\n",
       " 'models',\n",
       " 'are',\n",
       " 'being',\n",
       " 'applied',\n",
       " 'in',\n",
       " 'creative',\n",
       " 'processes',\n",
       " 'such',\n",
       " 'as',\n",
       " 'screenwriting',\n",
       " 'their',\n",
       " 'discussion',\n",
       " 'also',\n",
       " 'explores',\n",
       " 'how',\n",
       " 'data',\n",
       " 'science',\n",
       " 'for',\n",
       " 'analyzing',\n",
       " 'development',\n",
       " 'projects',\n",
       " 'such',\n",
       " 'as',\n",
       " 'financing',\n",
       " 'and',\n",
       " 'selecting',\n",
       " 'scripts',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'optimizing',\n",
       " 'the',\n",
       " 'content',\n",
       " 'production',\n",
       " 'process']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "episode_desc_title_corpus[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get the weighted vectors of the episode descriptions?\n",
    "\n",
    "....\n",
    "\n",
    "\n",
    "<img src=\"pictures/tf_idf_matrix.png\">\n",
    "<img src=\"pictures/word_vec_df.png\">\n",
    "\n",
    "\n",
    "- For example, doc has three words: doc = [word1, word2, word3].\n",
    "- vec_word_i = [d1, d2, ..., dn] \n",
    "- n = size in hidden layer.\n",
    "\n",
    "- tf_idf_ji = tf_idf of word i in doc_j; Scale them such that sum_i tf_idf_ji = 1. \n",
    "\n",
    "- Then the vector of doc_j = sum_i (vec_word_i * tf_dif_ji) which is a vector with the same len as vec_word_i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doc_weighted_vec(i, doc_corpus , tf_idf = X, weighted = True): # ith documents. doc_corpus a list of words\n",
    "    \n",
    "    df = word_vecs_df \n",
    "    related_rows = df.loc[sorted(list(set(doc_corpus).intersection(set(vocab)))), :] \n",
    "    \n",
    "    if weighted:\n",
    "        weights = []\n",
    "        ind = sorted(tf_idf[i,:].nonzero()[1])\n",
    "        if sum([vectorizer.vocabulary_[related_rows.index[j]] != ind[j] for j in range(len(ind))]) != 0:\n",
    "            print(\"words position don't match\")\n",
    "            return \n",
    "        for j in ind:\n",
    "            weights.append(tf_idf[i,j])\n",
    "        weights = np.array(weights)/sum(weights)\n",
    "    else:\n",
    "        weights = [1/related_rows.shape[0]] * related_rows.shape[0]\n",
    "    \n",
    "    if related_rows.shape[0] != len(weights):\n",
    "        print(i)\n",
    "        print(related_rows.shape[0])\n",
    "        print(len(weights))\n",
    "    \n",
    "    result = related_rows.T * weights\n",
    "    return result.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_vec_weighted = []\n",
    "total = len(descriptions)\n",
    "for i in range(total):\n",
    "    doc_corpus = episode_desc_title_corpus[i]\n",
    "    episode_vec_weighted.append(get_doc_weighted_vec(i,doc_corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the episode weighted vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('episode_vec/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('episode_vec/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(episode_vec_weighted, \"episode_vec_weighted_with_title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(episode_vec_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500611</td>\n",
       "      <td>-0.392125</td>\n",
       "      <td>0.031091</td>\n",
       "      <td>0.058523</td>\n",
       "      <td>0.132875</td>\n",
       "      <td>0.498406</td>\n",
       "      <td>0.123527</td>\n",
       "      <td>-0.011573</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>-0.244095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056908</td>\n",
       "      <td>-0.452368</td>\n",
       "      <td>-0.174699</td>\n",
       "      <td>-0.078086</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>0.155416</td>\n",
       "      <td>-0.315453</td>\n",
       "      <td>0.723233</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.089709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.525857</td>\n",
       "      <td>0.068335</td>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.311522</td>\n",
       "      <td>-0.015205</td>\n",
       "      <td>-0.002165</td>\n",
       "      <td>-0.055992</td>\n",
       "      <td>0.214757</td>\n",
       "      <td>-0.023833</td>\n",
       "      <td>-0.017694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137174</td>\n",
       "      <td>-0.529182</td>\n",
       "      <td>-0.240740</td>\n",
       "      <td>-0.217705</td>\n",
       "      <td>0.237649</td>\n",
       "      <td>-0.538784</td>\n",
       "      <td>-0.306210</td>\n",
       "      <td>0.253044</td>\n",
       "      <td>0.062750</td>\n",
       "      <td>0.049728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.523980</td>\n",
       "      <td>0.019395</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.091436</td>\n",
       "      <td>0.262884</td>\n",
       "      <td>0.274046</td>\n",
       "      <td>0.499739</td>\n",
       "      <td>-0.304788</td>\n",
       "      <td>0.094896</td>\n",
       "      <td>-0.039237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205848</td>\n",
       "      <td>-0.504070</td>\n",
       "      <td>-0.175757</td>\n",
       "      <td>-0.150721</td>\n",
       "      <td>-0.140069</td>\n",
       "      <td>0.166488</td>\n",
       "      <td>-0.278629</td>\n",
       "      <td>0.642518</td>\n",
       "      <td>0.405776</td>\n",
       "      <td>-0.229477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.680434</td>\n",
       "      <td>-0.518742</td>\n",
       "      <td>0.615582</td>\n",
       "      <td>-0.139015</td>\n",
       "      <td>-0.009863</td>\n",
       "      <td>0.199846</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.288420</td>\n",
       "      <td>0.227574</td>\n",
       "      <td>-0.332820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.411431</td>\n",
       "      <td>-0.327884</td>\n",
       "      <td>-0.859069</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.110672</td>\n",
       "      <td>-0.295466</td>\n",
       "      <td>-0.401121</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>-0.208328</td>\n",
       "      <td>-0.124752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.059128</td>\n",
       "      <td>-0.002912</td>\n",
       "      <td>0.022362</td>\n",
       "      <td>0.096436</td>\n",
       "      <td>-0.132251</td>\n",
       "      <td>0.503611</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>0.227001</td>\n",
       "      <td>-0.084239</td>\n",
       "      <td>-0.208569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007823</td>\n",
       "      <td>-0.148612</td>\n",
       "      <td>-0.557120</td>\n",
       "      <td>-0.281385</td>\n",
       "      <td>-0.041186</td>\n",
       "      <td>-0.312629</td>\n",
       "      <td>-0.352170</td>\n",
       "      <td>0.191202</td>\n",
       "      <td>-0.093452</td>\n",
       "      <td>-0.047086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.500611 -0.392125  0.031091  0.058523  0.132875  0.498406  0.123527   \n",
       "1  0.525857  0.068335  0.090763  0.311522 -0.015205 -0.002165 -0.055992   \n",
       "2  0.523980  0.019395  0.003978  0.091436  0.262884  0.274046  0.499739   \n",
       "3  0.680434 -0.518742  0.615582 -0.139015 -0.009863  0.199846  0.092300   \n",
       "4  0.059128 -0.002912  0.022362  0.096436 -0.132251  0.503611 -0.005181   \n",
       "\n",
       "          7         8         9    ...          190       191       192  \\\n",
       "0 -0.011573  0.008225 -0.244095    ...    -0.056908 -0.452368 -0.174699   \n",
       "1  0.214757 -0.023833 -0.017694    ...    -0.137174 -0.529182 -0.240740   \n",
       "2 -0.304788  0.094896 -0.039237    ...    -0.205848 -0.504070 -0.175757   \n",
       "3  0.288420  0.227574 -0.332820    ...    -0.411431 -0.327884 -0.859069   \n",
       "4  0.227001 -0.084239 -0.208569    ...     0.007823 -0.148612 -0.557120   \n",
       "\n",
       "        193       194       195       196       197       198       199  \n",
       "0 -0.078086  0.015759  0.155416 -0.315453  0.723233 -0.093518 -0.089709  \n",
       "1 -0.217705  0.237649 -0.538784 -0.306210  0.253044  0.062750  0.049728  \n",
       "2 -0.150721 -0.140069  0.166488 -0.278629  0.642518  0.405776 -0.229477  \n",
       "3 -0.106659  0.110672 -0.295466 -0.401121  0.459016 -0.208328 -0.124752  \n",
       "4 -0.281385 -0.041186 -0.312629 -0.352170  0.191202 -0.093452 -0.047086  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_vec_weighted_df = pd.DataFrame(episode_vec_weighted)\n",
    "episode_vec_weighted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 200)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_vec_weighted_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_vec_weighted_df.to_csv(\"./episode_vec/episode_vec_weighted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500611</td>\n",
       "      <td>-0.392125</td>\n",
       "      <td>0.031091</td>\n",
       "      <td>0.058523</td>\n",
       "      <td>0.132875</td>\n",
       "      <td>0.498406</td>\n",
       "      <td>0.123527</td>\n",
       "      <td>-0.011573</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>-0.244095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056908</td>\n",
       "      <td>-0.452368</td>\n",
       "      <td>-0.174699</td>\n",
       "      <td>-0.078086</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>0.155416</td>\n",
       "      <td>-0.315453</td>\n",
       "      <td>0.723233</td>\n",
       "      <td>-0.093518</td>\n",
       "      <td>-0.089709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.525857</td>\n",
       "      <td>0.068335</td>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.311522</td>\n",
       "      <td>-0.015205</td>\n",
       "      <td>-0.002165</td>\n",
       "      <td>-0.055992</td>\n",
       "      <td>0.214757</td>\n",
       "      <td>-0.023833</td>\n",
       "      <td>-0.017694</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137174</td>\n",
       "      <td>-0.529182</td>\n",
       "      <td>-0.240740</td>\n",
       "      <td>-0.217705</td>\n",
       "      <td>0.237649</td>\n",
       "      <td>-0.538784</td>\n",
       "      <td>-0.306210</td>\n",
       "      <td>0.253044</td>\n",
       "      <td>0.062750</td>\n",
       "      <td>0.049728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.523980</td>\n",
       "      <td>0.019395</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.091436</td>\n",
       "      <td>0.262884</td>\n",
       "      <td>0.274046</td>\n",
       "      <td>0.499739</td>\n",
       "      <td>-0.304788</td>\n",
       "      <td>0.094896</td>\n",
       "      <td>-0.039237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205848</td>\n",
       "      <td>-0.504070</td>\n",
       "      <td>-0.175757</td>\n",
       "      <td>-0.150721</td>\n",
       "      <td>-0.140069</td>\n",
       "      <td>0.166488</td>\n",
       "      <td>-0.278629</td>\n",
       "      <td>0.642518</td>\n",
       "      <td>0.405776</td>\n",
       "      <td>-0.229477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.680434</td>\n",
       "      <td>-0.518742</td>\n",
       "      <td>0.615582</td>\n",
       "      <td>-0.139015</td>\n",
       "      <td>-0.009863</td>\n",
       "      <td>0.199846</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.288420</td>\n",
       "      <td>0.227574</td>\n",
       "      <td>-0.332820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.411431</td>\n",
       "      <td>-0.327884</td>\n",
       "      <td>-0.859069</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.110672</td>\n",
       "      <td>-0.295466</td>\n",
       "      <td>-0.401121</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>-0.208328</td>\n",
       "      <td>-0.124752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.059128</td>\n",
       "      <td>-0.002912</td>\n",
       "      <td>0.022362</td>\n",
       "      <td>0.096436</td>\n",
       "      <td>-0.132251</td>\n",
       "      <td>0.503611</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>0.227001</td>\n",
       "      <td>-0.084239</td>\n",
       "      <td>-0.208569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007823</td>\n",
       "      <td>-0.148612</td>\n",
       "      <td>-0.557120</td>\n",
       "      <td>-0.281385</td>\n",
       "      <td>-0.041186</td>\n",
       "      <td>-0.312629</td>\n",
       "      <td>-0.352170</td>\n",
       "      <td>0.191202</td>\n",
       "      <td>-0.093452</td>\n",
       "      <td>-0.047086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.500611 -0.392125  0.031091  0.058523  0.132875  0.498406  0.123527   \n",
       "1  0.525857  0.068335  0.090763  0.311522 -0.015205 -0.002165 -0.055992   \n",
       "2  0.523980  0.019395  0.003978  0.091436  0.262884  0.274046  0.499739   \n",
       "3  0.680434 -0.518742  0.615582 -0.139015 -0.009863  0.199846  0.092300   \n",
       "4  0.059128 -0.002912  0.022362  0.096436 -0.132251  0.503611 -0.005181   \n",
       "\n",
       "          7         8         9    ...          190       191       192  \\\n",
       "0 -0.011573  0.008225 -0.244095    ...    -0.056908 -0.452368 -0.174699   \n",
       "1  0.214757 -0.023833 -0.017694    ...    -0.137174 -0.529182 -0.240740   \n",
       "2 -0.304788  0.094896 -0.039237    ...    -0.205848 -0.504070 -0.175757   \n",
       "3  0.288420  0.227574 -0.332820    ...    -0.411431 -0.327884 -0.859069   \n",
       "4  0.227001 -0.084239 -0.208569    ...     0.007823 -0.148612 -0.557120   \n",
       "\n",
       "        193       194       195       196       197       198       199  \n",
       "0 -0.078086  0.015759  0.155416 -0.315453  0.723233 -0.093518 -0.089709  \n",
       "1 -0.217705  0.237649 -0.538784 -0.306210  0.253044  0.062750  0.049728  \n",
       "2 -0.150721 -0.140069  0.166488 -0.278629  0.642518  0.405776 -0.229477  \n",
       "3 -0.106659  0.110672 -0.295466 -0.401121  0.459016 -0.208328 -0.124752  \n",
       "4 -0.281385 -0.041186 -0.312629 -0.352170  0.191202 -0.093452 -0.047086  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"./episode_vec/episode_vec_weighted.csv\", index_col=0)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-73bbe247de15>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-73bbe247de15>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Stop here.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Stop here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make recomendation: find related episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user's strings: \n",
    "\n",
    "some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# user_requests = [\n",
    "#     \"\",\n",
    "#     \"Are there any episodes on Facial Recognition?\",\n",
    "#     \"How to know whether the data I am using is valid for my purpose?\",\n",
    "#     \"Could you recommend some episodes on decision tree and random forests?\",\n",
    "#     \"Can you talk about Convolutional neural network and recurrent neural network?\",\n",
    "#     \"Could you recommend some episodes on data science projects for beginners?\",\n",
    "#     \"artificial intelligence\",\n",
    "#     \"How can beginners in machine learning, who have finished their MOOCs in machine learning and deep learning, take it to the next level and get to the point of being able to read research papers & productively contribute in an industry?\",\n",
    "#     \"What can artificial intelligence do for human beings? What is the future of artificial intelligence?\",\n",
    "#     \"What is natural language processing? \",\n",
    "#     \"The error percentage of regression changes with change in the train and test data which I am deciding randomly. Cross validation can overcome this but how do I apply it for my regression model?\",\n",
    "#     \"I have a precision recall curve for two separate algorithms. If I want to calculate the F-Measure I have to use the precision and recall values at a particular point on each curve. How is this point decided? For example on curve one there is a point where recall is 0.9 and precision is 0.87 and the other curve there is a point of recall at 0.95 and precision at 0.84. Alternatively, should I plot a F-measure curve for every precision recall value?\",\n",
    "#     \"Suppose I want to make predictions of a response from predictors but I have some autocorrelation in the response variable. Under OLS this would be a problem as the residuals would have autocorrelation. What if I just want to predict the response and I use regularized least squares, like lasso or ridge or elastic net? I don't care about variances of the coefficients or anything of that nature as I'm not testing any hypotheses but I feel like I might be missing something.\",\n",
    "#     \"Evaluating the quality of data.\",\n",
    "#     \"I am interested in knowing musical stuff.\",\n",
    "#     \"Is there any podcast on musical data and musical projects?\",\n",
    "#     \"What is the trend of big data? What is big data? How to learn big data?\",\n",
    "#     \"How to learn machine learning? What books or website do you recommend?\",\n",
    "#     \"Looking for projects on criminal analysis? \",\n",
    "#     \"How to take advantage of Internet, computer,  cloud and other  platform in an effective way?\",\n",
    "#     \"What are the most important knowledge in statistics or probability when doing machine learning?\"\n",
    "# ]  \n",
    "\n",
    "\n",
    "user_requests = [\"Are there any episodes on Facial Recognition?\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_episode = episode_vec_weighted_df.values\n",
    "with open('some examples with titles.txt', 'w') as f:\n",
    "    for j in range(len(user_requests)):\n",
    "        f.write(\"*****************************************************\" + \"\\n\")\n",
    "        user_request = user_requests[j]\n",
    "        user_request_corpus = gensim.utils.simple_preprocess(user_request)\n",
    "        X_user = vectorizer.fit_transform([\" \".join(user_request_corpus)])\n",
    "        f.write(str(X_user.shape) + \"\\n\")\n",
    "        f.write('tf_idf is ' + str(X_user))\n",
    "        user_weighted_vec = get_doc_weighted_vec(0, user_request_corpus, tf_idf = X_user, weighted = True)\n",
    "        f.write(\"user_weighted_vec is \\n\" + str(user_weighted_vec[0:10]))\n",
    "        cos_similarities = cosine_similarity(X=user_weighted_vec, Y=all_episode)\n",
    "\n",
    "        cos_similarities = cos_similarities[0]\n",
    "        cos_similarities.shape\n",
    "\n",
    "\n",
    "        most_similar = cos_similarities.argsort()[-4:][::-1]\n",
    "        f.write(str(most_similar) + \"\\n\")\n",
    "\n",
    "        threshold = 0.60\n",
    "        f.write(\"User's request is: \" + user_request + \"\\n\" )\n",
    "        for i in most_similar:\n",
    "\n",
    "            if cos_similarities[i] > threshold:\n",
    "                f.write(\"--------------------------\"+str(cos_similarities[i])+\"-----------------------------------\\n\")\n",
    "                f.write( \"\\n\")\n",
    "                f.write(str(descToTitle[descriptions[i]]) + \"\\n\")\n",
    "                f.write(str(descToLink[descriptions[i]]) + \"\\n\")\n",
    "                f.write(str(descriptions[i].encode('utf-8')) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the result at some example.txt.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of all episode is handlable, let's have a look at the similarity between all episodes. By this, I also want to know the levels of the cosine similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = cosine_similarity(X=all_episode)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = plt.imshow(A[20:40,20:40])\n",
    "plt.colorbar(im)\n",
    "plt.show()\n",
    "\n",
    "# very diversity. so it is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_request = input('what topics are interesting to you? ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Hello.\", user_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to-do: reorganize the code and write a function recommend_episode.\n",
    "\n",
    "def recommend_episode(string):\n",
    "    all_episode = episode_vec_weighted_df.values\n",
    "\n",
    "    \n",
    "    print(\"*****************************************************\" + \"\\n\")\n",
    "    user_request = string\n",
    "    user_request_corpus = gensim.utils.simple_preprocess(user_request)\n",
    "    X_user = vectorizer.fit_transform([\" \".join(user_request_corpus)])\n",
    "    #print(str(X_user.shape) + \"\\n\")\n",
    "    user_weighted_vec = get_doc_weighted_vec(0, user_request_corpus , tf_idf = X_user, weighted = True)\n",
    "    cos_similarities = cosine_similarity(X=user_weighted_vec, Y=all_episode)\n",
    "\n",
    "    cos_similarities = cos_similarities[0]\n",
    "    cos_similarities.shape\n",
    "\n",
    "\n",
    "    most_similar = cos_similarities.argsort()[-4:][::-1]\n",
    "    #print(str(most_similar) + \"\\n\")\n",
    "\n",
    "    threshold = 0.60\n",
    "    print(\"User's request is: \" + user_request + \"\\n\" )\n",
    "\n",
    "    for i in most_similar:\n",
    "\n",
    "        if cos_similarities[i] > threshold:\n",
    "            print(\"--------------------The episode has cosine similarity is \"+str(cos_similarities[i])+\" with user's request-------------------------\\n\")\n",
    "            print( \"\\n\")\n",
    "            print(str(descToTitle[descriptions[i]]) + \"\\n\")\n",
    "            print(str(descToLink[descriptions[i]]) + \"\\n\")\n",
    "            print(str(descriptions[i].encode('utf-8')) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommend_episode(user_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'adboost' in vocab # so no matter how many times the word 'adboost' is in the string, it won't find "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
