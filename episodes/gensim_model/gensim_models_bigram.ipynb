{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import smart_open\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import heapq\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from gensim.models import Phrases\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_df = pd.read_csv('all_posts.csv', sep = \"\\t\")\n",
    "Q_df = post_df.loc[post_df['PostTypeId'] == 1][['Id', 'Title','Body']]\n",
    "A_df =  post_df.loc[post_df['PostTypeId'] != 1][['Id', 'Body']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_q_posts = Q_df['Body']\n",
    "original_a_posts = A_df['Body']\n",
    "original_q_titles = Q_df['Title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./text/'):\n",
    "            os.makedirs('./text/')\n",
    "with open('./text/questions_body.txt', 'w') as thefile:\n",
    "    for post in original_q_posts:\n",
    "        post = str(post).replace('\\n', ' ')\n",
    "        post = re.sub(r\"\\$.*\\$\", \"\", post)\n",
    "        thefile.write(\"%s\\n\" % post)\n",
    "\n",
    "with open('./text/answers_body.txt', 'w') as thefile:\n",
    "    for post in original_a_posts:\n",
    "        post = str(post).replace('\\n', ' ')\n",
    "        post = re.sub(r\"\\$.*\\$\", \"\", post)\n",
    "        thefile.write(\"%s\\n\" % post)\n",
    "\n",
    "with open('./text/questions_title.txt', 'w') as thefile:\n",
    "    for i, post in enumerate(original_q_titles):\n",
    "        post = str(post).replace('\\n', \"\") \n",
    "        post = re.sub(r\"\\$.*\\$\", \"\", post)\n",
    "        thefile.write(\"%s\\n\" % post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./text/all_posts_body.txt','w') as thefile:\n",
    "    for i, post in enumerate(original_q_posts):\n",
    "        post = str(post).replace('\\n', \"\") \n",
    "        post = re.sub(r\"\\$.*\\$\", \"\", post)\n",
    "        thefile.write(\"%s\\n\" % post)\n",
    "    for i, post in enumerate(original_a_posts):\n",
    "        post = str(post).replace('\\n', \"\") \n",
    "        post = re.sub(r\"\\$.*\\$\", \"\", post)\n",
    "        thefile.write(\"%s\\n\" % post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "215962\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open('./text/all_posts_body.txt','r') as thefile:\n",
    "    for line in thefile:\n",
    "        i += 1\n",
    "print(i==post_df.shape[0])\n",
    "print(post_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def is_number(s):\n",
    "#     try:\n",
    "#         float(s)\n",
    "#         return True\n",
    "#     except ValueError:\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def show_work_status(singleCount, totalCount, currentCount = 0):\n",
    "#     currentCount += singleCount\n",
    "#     percentage = currentCount/totalCount *100\n",
    "#     status = \">\" * int(percentage) +  \" \" * (100-int(percentage))\n",
    "#     sys.stdout.write('\\rStatus:[{0}] {1:.2f}%'.format(status, percentage))\n",
    "#     sys.stdout.flush()\n",
    "#     if percentage >= 100:\n",
    "#     \tprint('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# translation = str.maketrans(string.punctuation,' '*len(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess: \n",
    "# 1. lower case\n",
    "# 2. tokenize\n",
    "# 3. add to vocab (include bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(fname):\n",
    "    sentences = []\n",
    "    bigram = Phrases(min_count=5)\n",
    "    with open(fname, \"r\") as f:\n",
    "        for line in f:\n",
    "            sentence = gensim.utils.simple_preprocess(line) \n",
    "#             This lowercases, tokenizes, de-accents (optional). –  \n",
    "#             the output are final tokens = unicode strings, that won’t be processed any further.\n",
    "#             sentence is a list of words. only lowercases and de-accents, no use lemmatize or stopwords.\n",
    "            sentences.append(sentence)\n",
    "            bigram.add_vocab([sentence])\n",
    "    if not os.path.exists('../SO_bigram/'):\n",
    "            os.makedirs('../SO_bigram/')\n",
    "    with open(\"../SO_bigram/SO_bigram.pkl\", 'wb') as f:\n",
    "        pickle.dump(bigram,f)\n",
    "    return bigram, sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bigram, all_sentences = preprocess('./text/all_posts_body.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../SO_bigram/SO_bigram.pkl\", 'rb') as f:\n",
    "     new_bigram =  pickle.load(f)\n",
    "len(new_bigram.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3386232\n",
      "[b'how', b'how_should', b'should', b'should_elicit', b'elicit', b'elicit_prior', b'prior', b'prior_distributions', b'distributions', b'distributions_from', b'from', b'from_experts', b'experts', b'experts_when', b'when', b'when_fitting', b'fitting', b'fitting_bayesian', b'bayesian', b'bayesian_model', b'model', b'in', b'in_many', b'many', b'many_different', b'different', b'different_statistical', b'statistical', b'statistical_methods', b'methods', b'methods_there', b'there', b'there_is', b'is', b'is_an', b'an', b'an_assumption', b'assumption', b'assumption_of', b'of', b'of_normality', b'normality', b'normality_what', b'what', b'what_is', b'is_normality', b'normality_and', b'and', b'and_how', b'how_do']\n",
      "[['how', 'should', 'elicit', 'prior', 'distributions', 'from', 'experts', 'when', 'fitting', 'bayesian', 'model'], ['in', 'many', 'different', 'statistical', 'methods', 'there', 'is', 'an', 'assumption', 'of', 'normality', 'what', 'is', 'normality', 'and', 'how', 'do', 'know', 'if', 'there', 'is', 'normality'], ['what', 'are', 'some', 'valuable', 'statistical', 'analysis', 'open_source', 'projects', 'available', 'right', 'now', 'edit', 'as', 'pointed_out', 'by', 'sharpie', 'valuable', 'could', 'mean', 'helping', 'you', 'get', 'things', 'done', 'faster', 'or', 'more', 'cheaply'], ['have', 'two', 'groups', 'of', 'data', 'each', 'with', 'different', 'distribution', 'of', 'multiple', 'variables', 'trying', 'to', 'determine', 'if', 'these', 'two', 'groups', 'distributions', 'are', 'different', 'in', 'statistically_significant', 'way', 'have', 'the', 'data', 'in', 'both', 'raw', 'form', 'and', 'binned', 'up', 'in', 'easier', 'to', 'deal_with', 'discrete', 'categories', 'with', 'frequency', 'counts', 'in', 'each', 'what', 'tests', 'procedures', 'methods', 'should', 'use', 'to', 'determine_whether', 'or', 'not', 'these', 'two', 'groups', 'are', 'significantly_different', 'and', 'how', 'do', 'do', 'that', 'in', 'sas', 'or', 'or', 'orange'], ['last_year', 'read', 'blog_post', 'from', 'brendan', 'connor', 'entitled', 'statistics', 'vs', 'machine_learning', 'fight', 'that', 'discussed', 'some', 'of', 'the', 'differences_between', 'the', 'two', 'fields', 'andrew_gelman', 'responded', 'favorably', 'to', 'this', 'simon', 'blomberg', 'from', 'fortunes', 'package', 'to', 'paraphrase', 'provocatively', 'machine_learning', 'is', 'statistics', 'minus', 'any', 'checking', 'of', 'models', 'and', 'assumptions', 'brian_ripley', 'about', 'the', 'difference_between', 'machine_learning', 'and', 'statistics', 'user', 'vienna', 'may', 'season', 'greetings', 'andrew_gelman', 'in', 'that', 'case', 'maybe', 'we', 'should', 'get_rid', 'of', 'checking', 'of', 'models', 'and', 'assumptions', 'more', 'often', 'then', 'maybe', 'we', 'be_able', 'to', 'solve', 'some', 'of', 'the', 'problems', 'that', 'the', 'machine_learning', 'people', 'can', 'solve', 'but', 'we', 'can', 'there', 'was', 'also', 'the', 'statistical', 'modeling', 'the', 'two_cultures', 'paper', 'by', 'leo_breiman', 'in', 'which', 'argued', 'that', 'statisticians', 'rely', 'too', 'heavily', 'on', 'data', 'modeling', 'and', 'that', 'machine_learning', 'techniques', 'are', 'making', 'progress', 'by', 'instead', 'relying_on', 'the', 'predictive_accuracy', 'of', 'models', 'has', 'the', 'statistics', 'field', 'changed', 'over', 'the', 'last_decade', 'in', 'response', 'to', 'these', 'critiques', 'do', 'the', 'two_cultures', 'still', 'exist', 'or', 'has', 'statistics', 'grown', 'to', 'embrace', 'machine_learning', 'techniques', 'such', 'as', 'neural_networks', 'and', 'support_vector', 'machines']]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_bigram.vocab))\n",
    "print(list(all_bigram.vocab.keys())[0:50])\n",
    "print(list(all_bigram[all_sentences])[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(bigram, sentence):\n",
    "    temp = bigram[sentence]# temp is a list of words(unigram and bigram)\n",
    "    result = []\n",
    "    for element in temp:\n",
    "        key = element.split(\"_\")\n",
    "        if len(key) == 1:\n",
    "            result.append(element)\n",
    "        if len(key) > 1 and not any([word in stopwords.words(\"english\") for word in key]):\n",
    "            result.append(element)\n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_sentences_nonstopword = []\n",
    "for sentence in all_sentences:\n",
    "    all_sentences_nonstopword.append(remove_stopwords(all_bigram, sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "215962\n"
     ]
    }
   ],
   "source": [
    "print(len(all_sentences_nonstopword) == len(all_sentences))\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_sentences_nonstopword.pickle', 'wb') as f:\n",
    "    pickle.dump(all_sentences_nonstopword, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58999"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(all_sentences[i])- len(all_sentences_nonstopword[i]) > 10 for i in range(len(all_sentences))]) # so we filter out  a lot of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def count_bigram(bigram):\n",
    "#     bigram_counter = Counter()\n",
    "#     for key in bigram.vocab.keys():\n",
    "#         if key not in stopwords.words(\"english\"):\n",
    "#             if len(key.decode('utf-8').split(\"_\")) > 1 and not any([word in stopwords.words(\"english\") for word in key.decode('utf-8').split(\"_\")]):\n",
    "#                 bigram_counter[key] = bigram.vocab[key]\n",
    "#     for key, counts in bigram_counter.most_common(20):\n",
    "#         print(key,counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count_bigram(all_bigram) # the first 20 common bigram without stopwords in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most of them are out of interest. So we have to move the bigram with stopword out of the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def vocab(bigram):\n",
    "#     vocab_dict = Counter()\n",
    "#     for key,value in bigram.vocab.items():\n",
    "#         words = key.decode(\"utf-8\").split(\"_\")\n",
    "#         if len(words) == 1:\n",
    "#             if key.decode(\"utf-8\") not in stopwords.words(\"english\"):\n",
    "#                 vocab_dict[key.decode('utf-8')] = value    \n",
    "#         else:\n",
    "#             if not any([word in stopwords.words('english') for word in words]):\n",
    "#                 vocab_dict[key.decode('utf-8').replace(\"_\", \" \")] = value\n",
    "#     vocab = list(vocab_dict.keys())\n",
    "#     return vocab, vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# title_vocab,title_vocab_dict = vocab(title_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for key, counts in title_vocab_dict.most_common(200):\n",
    "#     print(key, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # It takes a while.\n",
    "# title_vocab,title_vocab_dict = vocab(title_bigram)\n",
    "# question_vocab,question_vocab_dict = vocab(question_bigram)\n",
    "# answer_vocab,answer_vocab_dict = vocab(answer_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(len(title_vocab))\n",
    "# print(title_vocab_dict.most_common(20))\n",
    "\n",
    "# print(len(question_vocab))\n",
    "# print(question_vocab_dict.most_common(20))\n",
    "\n",
    "# print(len(answer_vocab))\n",
    "# print(answer_vocab_dict.most_common(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for word2vec models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #parameters\n",
    "# sizes = np.linspace(100, 200, num = 1)\n",
    "# print(sizes)\n",
    "# windows = np.linspace(6, 6,num = 1 )\n",
    "# print(windows)\n",
    "# min_counts = [2]\n",
    "# print(min_counts )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Later make it more dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "200\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# After tuning the model and find appropriate parameters, save them in the config file and read from the config file. \n",
    "\n",
    "\n",
    "with open('../../config/config.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    paras = data['model_paras']\n",
    "    \n",
    "min_count = 1\n",
    "size = paras['size']\n",
    "window = paras['window']\n",
    "# workers = paras['workers']\n",
    "# sg = paras['sg']\n",
    "# alpha = paras[\"alpha\"]\n",
    "# hs = paras[\"hs\"]\n",
    "# negative = paras[\"negative\"]\n",
    "print(min_count)\n",
    "print(size)\n",
    "print(window)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./model_bigram/'):\n",
    "            os.makedirs('./model_bigram/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: sentence: a list of lists of words.\n",
    "# paras is a dictionary{ para: value, para: value}\n",
    "\n",
    "def train_word_model(**paras):\n",
    "    min_count = paras['min_count']\n",
    "    size = paras['size']\n",
    "    window = paras['window']\n",
    "    bigram_model = gensim.models.Word2Vec(all_sentences_nonstopword, min_count = min_count, size = size, window = window, workers = 4)\n",
    "    pickle.dump(bigram_model, \"./model_bigram/bigram_model.pkl\")\n",
    "    return bigram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training  models or loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_count': 1, 'size': 200, 'window': 6}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paras = {'size':int(size), 'window' : int(window), \"min_count\" :int(min_count)}\n",
    "all_post_model = train_word_model( **paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220018"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_post_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['new_york' in all_post_model.wv.vocab, 'random_walk' in all_post_model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glance_word(model):\n",
    "    print(model)\n",
    "    print('*************most similar words to \\'vector\\'***************')\n",
    "    print(model.most_similar('vector'))\n",
    "    print('\\n')\n",
    "    print(\"**************Similarity of \\'probability\\' and \\'distribution\\'******************\")\n",
    "    print(model.similarity('probability','distribution'))\n",
    "    print('\\n')\n",
    "    print(\"**************Similarity of \\'gaussian\\' and \\'normal\\'******************\")\n",
    "    print(model.similarity('gaussian','normal'))\n",
    "    print('\\n')\n",
    "    print(\"************** most_similar(positive=['neural_network']) ******************\")\n",
    "    print(model.most_similar(positive=['neural_network']))\n",
    "    print('\\n')\n",
    "    print(\"************** most_similar(positive=['p_value']) ******************\")\n",
    "    print(model.most_similar(positive=['p_value']))\n",
    "    print('\\n')\n",
    "    print(\"************** most_similar(positive=['gaussian']) ******************\")\n",
    "    print(model.most_similar(positive=['gaussian']))\n",
    "    print('\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=220018, size=200, alpha=0.025)\n",
      "*************most similar words to 'vector'***************\n",
      "[('vectors', 0.702854573726654), ('feature_vector', 0.6546720266342163), ('scalar', 0.615821123123169), ('array', 0.6127488613128662), ('matrix', 0.6100714206695557), ('element', 0.6082791686058044), ('dot_product', 0.5576158761978149), ('input', 0.5553973317146301), ('sequence', 0.546273946762085), ('tuple', 0.5442144870758057)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.408359437493\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.654213587982\n",
      "\n",
      "\n",
      "************** most_similar(positive=['neural_network']) ******************\n",
      "[('neural_net', 0.8487250804901123), ('network', 0.8000389933586121), ('rnn', 0.787010133266449), ('cnn', 0.7851302027702332), ('neural_networks', 0.7765765190124512), ('architecture', 0.755818247795105), ('lstm', 0.7551956176757812), ('mlp', 0.7498098611831665), ('feed_forward', 0.7438141107559204), ('perceptron', 0.7430264949798584)]\n",
      "\n",
      "\n",
      "************** most_similar(positive=['p_value']) ******************\n",
      "[('max_abs', 0.7978663444519043), ('pval', 0.7805851101875305), ('pvals', 0.7801036834716797), ('sum_abs', 0.7784078121185303), ('pnorm_abs', 0.7761155366897583), ('np_abs', 0.7738924026489258), ('xc', 0.7646868228912354), ('np_diag', 0.7635307312011719), ('ret', 0.7629741430282593), ('np_sqrt', 0.7567249536514282)]\n",
      "\n",
      "\n",
      "************** most_similar(positive=['gaussian']) ******************\n",
      "[('multivariate_normal', 0.7013981342315674), ('multivariate_gaussian', 0.6921984553337097), ('cauchy', 0.6588020324707031), ('normal', 0.6542136669158936), ('unimodal', 0.6540223956108093), ('spherical', 0.644368588924408), ('bivariate_normal', 0.6384446620941162), ('mixture', 0.6381934881210327), ('gaussians', 0.6309820413589478), ('truncated_normal', 0.6285369396209717)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glance_word(all_post_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it seems that the word vectors are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_vectors(model):\n",
    "    word_vec_dic = {}\n",
    "    vocab = model.wv.vocab.keys()\n",
    "    for word in vocab:\n",
    "        word_vec_dic[word] = model[word]\n",
    "    return word_vec_dic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save word vectors for all models in model_word_vec_dic, model_word_vec_df and also in csv form.\n",
    "\n",
    "if not os.path.exists('../word_vec_bigram/'):\n",
    "            os.makedirs('../word_vec_bigram/')\n",
    "\n",
    "model_word_vec_dic = save_vectors(all_post_model)\n",
    "model_word_vec_df = pd.DataFrame(model_word_vec_dic).T\n",
    "model_word_vec_df.to_csv('../word_vec_bigram/all_posts_word_vec.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(model_word_vec_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6    \\\n",
      "aa     0.664908 -1.551020 -0.329074 -0.618662  0.592144  0.534153 -0.028235   \n",
      "aa_    0.004260 -0.021642  0.010587 -0.031627  0.015227  0.014982  0.016942   \n",
      "aa_aa  0.131537 -0.327651 -0.197247 -0.338115  0.105150 -0.035781  0.119896   \n",
      "aa_ab  0.199243 -0.373871 -0.239974 -0.477379  0.192277 -0.248665 -0.028177   \n",
      "aa_bb  0.054158 -0.191643 -0.183025 -0.343656  0.176290 -0.022939  0.036259   \n",
      "\n",
      "            7         8         9      ...          190       191       192  \\\n",
      "aa    -0.392439  0.523166 -0.038182    ...     0.347881 -0.551577  0.804758   \n",
      "aa_    0.013478  0.030222 -0.016702    ...     0.007476 -0.012960  0.011283   \n",
      "aa_aa  0.178391  0.416138 -0.348326    ...     0.157932  0.358710  0.291022   \n",
      "aa_ab  0.285890  0.159288 -0.312130    ...     0.177757  0.221543  0.220309   \n",
      "aa_bb  0.175700  0.146300 -0.233154    ...     0.259909  0.409138  0.228346   \n",
      "\n",
      "            193       194       195       196       197       198       199  \n",
      "aa    -0.465007  0.612584  0.926957 -1.029923 -0.350024  0.466421  0.846451  \n",
      "aa_    0.001390 -0.000649  0.012213 -0.007287 -0.021065  0.006808 -0.004187  \n",
      "aa_aa -0.001388  0.223877  0.337542 -0.642194  0.119298  0.306005  0.333847  \n",
      "aa_ab  0.015171  0.346442  0.468528 -0.637908  0.244656  0.278803  0.637655  \n",
      "aa_bb -0.076399  0.197799  0.322237 -0.784629  0.178722  0.194121  0.434548  \n",
      "\n",
      "[5 rows x 200 columns]\n",
      "                 0         1         2         3         4         5    \\\n",
      "ﬂexible    -0.063758 -0.008282 -0.021459 -0.001160  0.025204 -0.018077   \n",
      "ﬂinch       0.007480 -0.041477 -0.010010  0.012929  0.011354  0.018836   \n",
      "ﬂow         0.001338 -0.037817 -0.022407  0.012837  0.006081  0.001113   \n",
      "ﬂuctuate   -0.033252 -0.013890 -0.029999  0.006155 -0.039254  0.018723   \n",
      "ﬂuctuation  0.008166 -0.004589  0.012987  0.009995 -0.015987  0.005739   \n",
      "\n",
      "                 6         7         8         9      ...          190  \\\n",
      "ﬂexible     0.016215  0.055107  0.008136  0.002397    ...     0.015316   \n",
      "ﬂinch       0.034709 -0.005104  0.026178 -0.027838    ...    -0.003804   \n",
      "ﬂow         0.010572  0.007110  0.016324  0.035751    ...    -0.014307   \n",
      "ﬂuctuate    0.015861 -0.010486  0.039244  0.014483    ...    -0.000681   \n",
      "ﬂuctuation  0.026721  0.020252 -0.016565 -0.017637    ...    -0.009014   \n",
      "\n",
      "                 191       192       193       194       195       196  \\\n",
      "ﬂexible     0.019835 -0.000776 -0.032950 -0.002651 -0.028631 -0.020287   \n",
      "ﬂinch       0.043577 -0.009426  0.013372  0.024344 -0.007430 -0.022159   \n",
      "ﬂow         0.007358  0.029030 -0.014385  0.024317  0.013257 -0.033773   \n",
      "ﬂuctuate    0.023580  0.000493 -0.010333  0.011494  0.007932 -0.029029   \n",
      "ﬂuctuation -0.009871 -0.029857 -0.019317  0.036008 -0.009291  0.005954   \n",
      "\n",
      "                 197       198       199  \n",
      "ﬂexible     0.025000  0.027933  0.056432  \n",
      "ﬂinch       0.012533  0.024370 -0.002204  \n",
      "ﬂow         0.040410  0.022647  0.019212  \n",
      "ﬂuctuate    0.012934  0.023304  0.012644  \n",
      "ﬂuctuation -0.029560  0.020385 -0.012240  \n",
      "\n",
      "[5 rows x 200 columns]\n",
      "(220018, 200)\n",
      "['aa', 'aa_', 'aa_aa', 'aa_ab', 'aa_bb', 'aaa', 'aaa_aaa', 'aaa_bbb', 'aaa_target', 'aaaa']\n"
     ]
    }
   ],
   "source": [
    "print(model_word_vec_df.head())\n",
    "print(model_word_vec_df.tail())\n",
    "print(model_word_vec_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['об', 'образов', 'обратных', 'обучение', 'обшествоя', 'ожидаемых', 'определяют', 'опять', 'остатки', 'от', 'отклонений', 'паклин', 'последняя', 'поставленных', 'похожа', 'правило', 'предел', 'прикладная', 'приятно', 'проблема', 'процесса', 'проще', 'пуассона', 'распознавание', 'расстояние', 'расстоянии', 'рациональный', 'регрессии', 'регуляризации', 'реферативный', 'решении', 'россия', 'сrdist', 'сао', 'сил', 'случайной', 'спецификации', 'средних', 'ссср', 'стандартных', 'стьюд', 'сушку', 'так', 'тел', 'технологий', 'типичные', 'то', 'трех', 'ул', 'устойчивости', 'факторы', 'фот', 'части', 'шансов', 'этого', 'юрьев', 'التي', 'السلام', 'الله', 'المساحة', 'اليمين', 'بركاته', 'تعني', 'رحمة', 'على', 'عليكم', 'ـnote', '₁aₓ', 'ℓn', '上海', '将軍', '洪湖', '洪湖i', '洪湖松花皮蛋', 'ﬁeld', 'ﬁelds', 'ﬁlter', 'ﬁlteryields', 'ﬁnancial', 'ﬁnd', 'ﬁnding', 'ﬁnds', 'ﬁnite', 'ﬁrm', 'ﬁrms', 'ﬁrst', 'ﬁrst_outcome', 'ﬁt', 'ﬁtdistrplus', 'ﬁts', 'ﬁtted', 'ﬁtting', 'ﬁve', 'ﬁxed', 'ﬂawed', 'ﬂexible', 'ﬂinch', 'ﬂow', 'ﬂuctuate', 'ﬂuctuation']\n"
     ]
    }
   ],
   "source": [
    "print(vocab[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From vectors of words, how to get the vectors of docs? n_similarity uses the mean of the vectors of all words in the doc. We will use the tf-idf weighted vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting tf_idf of all words  in the vocab in all documents as weights  for all models.\n",
    "They are to be the weights of the words when we calculate the vector of docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'should', 'elicit', 'prior', 'distributions', 'from', 'experts', 'when', 'fitting', 'bayesian', 'model']\n",
      "['in', 'many', 'different', 'statistical', 'methods', 'there', 'is', 'an', 'assumption', 'of', 'normality', 'what', 'is', 'normality', 'and', 'how', 'do', 'know', 'if', 'there', 'is', 'normality']\n",
      "['what', 'are', 'some', 'valuable', 'statistical', 'analysis', 'open_source', 'projects', 'available', 'right', 'now', 'edit', 'as', 'by', 'sharpie', 'valuable', 'could', 'mean', 'helping', 'you', 'get', 'things', 'done', 'faster', 'or', 'more', 'cheaply']\n",
      "['have', 'two', 'groups', 'of', 'data', 'each', 'with', 'different', 'distribution', 'of', 'multiple', 'variables', 'trying', 'to', 'determine', 'if', 'these', 'two', 'groups', 'distributions', 'are', 'different', 'in', 'statistically_significant', 'way', 'have', 'the', 'data', 'in', 'both', 'raw', 'form', 'and', 'binned', 'up', 'in', 'easier', 'to', 'discrete', 'categories', 'with', 'frequency', 'counts', 'in', 'each', 'what', 'tests', 'procedures', 'methods', 'should', 'use', 'to', 'determine_whether', 'or', 'not', 'these', 'two', 'groups', 'are', 'significantly_different', 'and', 'how', 'do', 'do', 'that', 'in', 'sas', 'or', 'or', 'orange']\n",
      "['last_year', 'read', 'blog_post', 'from', 'brendan', 'connor', 'entitled', 'statistics', 'vs', 'machine_learning', 'fight', 'that', 'discussed', 'some', 'of', 'the', 'the', 'two', 'fields', 'andrew_gelman', 'responded', 'favorably', 'to', 'this', 'simon', 'blomberg', 'from', 'fortunes', 'package', 'to', 'paraphrase', 'provocatively', 'machine_learning', 'is', 'statistics', 'minus', 'any', 'checking', 'of', 'models', 'and', 'assumptions', 'brian_ripley', 'about', 'the', 'machine_learning', 'and', 'statistics', 'user', 'vienna', 'may', 'season', 'greetings', 'andrew_gelman', 'in', 'that', 'case', 'maybe', 'we', 'should', 'get_rid', 'of', 'checking', 'of', 'models', 'and', 'assumptions', 'more', 'often', 'then', 'maybe', 'we', 'to', 'solve', 'some', 'of', 'the', 'problems', 'that', 'the', 'machine_learning', 'people', 'can', 'solve', 'but', 'we', 'can', 'there', 'was', 'also', 'the', 'statistical', 'modeling', 'the', 'two_cultures', 'paper', 'by', 'leo_breiman', 'in', 'which', 'argued', 'that', 'statisticians', 'rely', 'too', 'heavily', 'on', 'data', 'modeling', 'and', 'that', 'machine_learning', 'techniques', 'are', 'making', 'progress', 'by', 'instead', 'the', 'predictive_accuracy', 'of', 'models', 'has', 'the', 'statistics', 'field', 'changed', 'over', 'the', 'last_decade', 'in', 'response', 'to', 'these', 'critiques', 'do', 'the', 'two_cultures', 'still', 'exist', 'or', 'has', 'statistics', 'grown', 'to', 'embrace', 'machine_learning', 'techniques', 'such', 'as', 'neural_networks', 'and', 'support_vector', 'machines']\n",
      "['working', 'on', 'new', 'method', 'for', 'analyzing', 'and', 'parsing', 'datasets', 'to', 'identify', 'and', 'isolate', 'subgroups', 'of', 'population', 'without', 'foreknowledge', 'of', 'any', 'subgroup', 'characteristics', 'while', 'the', 'method', 'works_well', 'enough', 'with', 'artificial', 'data', 'samples', 'datasets', 'created', 'specifically', 'for', 'the', 'purpose', 'of', 'identifying', 'and', 'segregating', 'subsets', 'of', 'the', 'population', 'like', 'to', 'try', 'testing', 'it', 'with', 'live', 'data', 'what', 'looking', 'for', 'is', 'freely_available', 'non', 'confidential', 'non', 'proprietary', 'data', 'source', 'preferably', 'one', 'containing', 'bimodal', 'or', 'multimodal_distributions', 'or', 'being', 'obviously', 'comprised', 'of', 'multiple', 'subsets', 'that', 'cannot', 'be', 'easily', 'pulled', 'apart', 'via', 'traditional', 'means', 'where', 'would', 'go', 'to', 'find', 'such', 'information']\n",
      "['sorry', 'but', 'the', 'emptyness', 'was', 'bit', 'overwhelming', 'and', 'this', 'stuck', 'in', 'since', 'it', 'got', 'asked', 'at', 'area']\n",
      "['many', 'studies', 'in', 'the', 'social_sciences', 'use', 'likert_scales', 'when', 'is', 'it', 'appropriate', 'to', 'use', 'likert', 'data', 'as', 'ordinal', 'and', 'when', 'is', 'it', 'appropriate', 'to', 'use', 'it', 'as', 'interval', 'data']\n",
      "['is', 'there', 'good', 'modern', 'treatment', 'covering', 'the', 'various', 'methods', 'of', 'multivariate', 'interpolation', 'including', 'which', 'methodologies', 'are', 'typically', 'best', 'for', 'particular', 'types', 'of', 'problems', 'interested', 'in', 'solid', 'statistical', 'treatment', 'including', 'error', 'estimates', 'under', 'various', 'model', 'assumptions', 'an', 'example', 'shepard', 'methodsay', 'we', 're', 'sampling', 'from', 'multivariate_normal', 'distribution', 'with', 'unknown', 'parameters', 'what', 'can', 'we', 'say', 'about', 'the', 'standard_error', 'of', 'the', 'interpolated', 'estimates', 'for', 'pointer', 'to', 'general', 'survey', 'addressing', 'similar', 'questions', 'for', 'the', 'various_types', 'of', 'multivariate', 'interpolations', 'in', 'common', 'use']\n",
      "['have', 'four', 'competing_models', 'which', 'use', 'to', 'predict', 'binary_outcome', 'variable', 'say', 'employment_status', 'after', 'graduating', 'employed', 'not', 'employed', 'for', 'subjects', 'natural', 'metric', 'of', 'model', 'performance', 'is', 'hit_rate', 'which', 'is', 'the', 'percentage', 'of', 'correct', 'predictions', 'for', 'each', 'one', 'of', 'the', 'models', 'it', 'seems', 'to', 'me', 'that', 'cannot', 'use', 'anova', 'in', 'this', 'setting', 'as', 'the', 'data', 'violates', 'the', 'assumptions', 'underlying', 'anova', 'is', 'there', 'an', 'equivalent', 'procedure', 'could', 'use', 'instead', 'of', 'anova', 'in', 'the', 'above', 'setting', 'to', 'test', 'for', 'the', 'hypothesis', 'that', 'all', 'four', 'models', 'are', 'equally', 'effective']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(all_sentences_nonstopword[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(all_sentences_nonstopword)):\n",
    "    temp = \" \".join(all_sentences_nonstopword[i])\n",
    "    corpus.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215962, 220018)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1, vocabulary=vocab)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the vocab from tf_idf vectorizer for future reference.\n",
    "import csv\n",
    "if not os.path.exists('../vocab_dic_bigram/'):\n",
    "            os.makedirs('../vocab_dic_bigram/')\n",
    "\n",
    "fname = '../vocab_dic_bigram/vocab_dict_question_answer.csv'\n",
    "with open(fname, 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in vectorizer.vocabulary_.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220018"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weighted vectors for all documents.\n",
    "\n",
    "This part is used for intrinsic evaluation. We have found that the result is satisfying, though one reason maybe that the evaluation is on the training dataset itself. This part is time consuming, so if only want to get word vectors, then there is no need to run this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # The number of nonzero elements in a row in X should equal to the number \n",
    "# # of unique words in the corresponding doc or word2vec_question_corpus. \n",
    "# # The funciton check is used to check this.\n",
    "\n",
    "# def check(key,i): # ith documents. doc_corpus a list of words\n",
    "#     doc_corpus = word2vec_question_corpus[i]\n",
    "#     df = model_word_vec_df[key]\n",
    "#     related_rows = df.loc[sorted(list(set(doc_corpus).intersection(set(vocab)))), :]  \n",
    "#     words_index = set(np.where(df.index.isin(set(doc_corpus).intersection(set(vocab))))[0])\n",
    "#     # print(related_rows.T)\n",
    "#     weights_index = []\n",
    "#     ind = sorted(X[i,:].nonzero()[1])\n",
    "#     for j in ind:\n",
    "#         weights_index.append(j)\n",
    "   \n",
    "#     if len(weights_index) - len(words_index) != 0:\n",
    "#         print(i)\n",
    "#         print(weights_index)\n",
    "#         print(words_index)\n",
    "#     if related_rows.shape[0] != len(weights_index):\n",
    "#         print(i)\n",
    "#         print(\"*********\")\n",
    "#     return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('doc_vec/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('doc_vec/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ith documents. doc_corpus a list of words \n",
    "# only do this for questions\n",
    "def get_doc_weighted_vec(key,i, weighted = True): \n",
    "    doc_corpus = word2vec_question_corpus[i]\n",
    "    df = model_word_vec_df[key]\n",
    "    related_rows = df.loc[sorted(list(set(doc_corpus).intersection(set(vocab)))), :] \n",
    "    \n",
    "    if weighted:\n",
    "        weights = []\n",
    "        ind = sorted(X[i,:].nonzero()[1]) # words index \n",
    "# One should check this. But it is very slow. I checked half of all question posts and they are valid. \n",
    "#         if sum([vectorizer.vocabulary_[related_rows.index[j]] != ind[j] for j in range(len(ind))]) != 0:\n",
    "#             print(\"words position don't match\")\n",
    "#             return \n",
    "\n",
    "        weights = [X[i, ind[j]] for j in range(len(ind))]        \n",
    "        weights = np.array(weights)/sum(weights) # scale so the sum of weights is one.\n",
    "    else:\n",
    "        # to correct: some words may appear more than once. But since we are only interested in the weighted vec, I will correct this later.\n",
    "        weights = [1/related_rows.shape[0]] * related_rows.shape[0] \n",
    " \n",
    "\n",
    "    result = related_rows.T * weights\n",
    "    return result.sum(axis = 1) # weighted sum as the doc vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status:[>>>>>                                                                                               ] 5.74%"
     ]
    }
   ],
   "source": [
    "questions_vec_weighted = []\n",
    "total = len(word2vec_question_corpus)\n",
    "current = 0\n",
    "for i in range(len(word2vec_question_corpus)):\n",
    "    show_work_status(1, total, current)\n",
    "    current += 1\n",
    "    questions_vec_weighted.append(get_doc_weighted_vec(key,i))\n",
    "\n",
    "\n",
    "print(len(questions_vec_weighted))\n",
    "print(len(questions_vec_weighted[0]))\n",
    "\n",
    "# save questions_vec_weighted\n",
    "save_obj(questions_vec_weighted, \"questions_weighted_vec_\"+key)\n",
    "questions_vec_weighted = load_obj(\"questions_weighted_vec_\"+key)\n",
    "\n",
    "print(len(questions_vec_weighted))\n",
    "print(len(questions_vec_weighted[0]))\n",
    "\n",
    "questions_vec_weighted_df = pd.DataFrame(questions_vec_weighted)\n",
    "print(questions_vec_weighted_df.head())\n",
    "print(questions_vec_weighted_df.shape)\n",
    "\n",
    "questions_vec_weighted_df.to_csv('doc_vec/questions_weighted_vec_'+key+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the vector representations of the doc, one can use cosine similarity to get related posts given one post from the training dataset."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
