{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "import heapq\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2010-07-19T19:12:12.510</td>\n",
       "      <td>36</td>\n",
       "      <td>2577.0</td>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010-09-15T21:08:26.077</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>&lt;bayesian&gt;&lt;prior&gt;&lt;elicitation&gt;</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2010-07-19T19:12:57.157</td>\n",
       "      <td>29</td>\n",
       "      <td>23368.0</td>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2016-06-27T06:44:40.147</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>&lt;distributions&gt;&lt;normality&gt;</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-07-19T19:13:28.577</td>\n",
       "      <td>66</td>\n",
       "      <td>5792.0</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2013-05-27T14:48:36.927</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>&lt;software&gt;&lt;open-source&gt;</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>2010-07-19T19:13:31.617</td>\n",
       "      <td>17</td>\n",
       "      <td>26414.0</td>\n",
       "      <td>I have two groups of data.  Each with a differ...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-09-08T03:00:19.690</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>&lt;distributions&gt;&lt;statistical-significance&gt;</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19T19:14:43.050</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The R-project\\n\\nhttp://www.r-project.org/\\n\\n...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2010-07-19T19:21:15.063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  PostTypeId  ParentId  AcceptedAnswerId             CreationDate  Score  \\\n",
       "0   1           1       NaN              15.0  2010-07-19T19:12:12.510     36   \n",
       "1   2           1       NaN              59.0  2010-07-19T19:12:57.157     29   \n",
       "2   3           1       NaN               5.0  2010-07-19T19:13:28.577     66   \n",
       "3   4           1       NaN             135.0  2010-07-19T19:13:31.617     17   \n",
       "4   5           2       3.0               NaN  2010-07-19T19:14:43.050     87   \n",
       "\n",
       "   ViewCount                                               Body  OwnerUserId  \\\n",
       "0     2577.0  How should I elicit prior distributions from e...          8.0   \n",
       "1    23368.0  In many different statistical methods there is...         24.0   \n",
       "2     5792.0  What are some valuable Statistical Analysis op...         18.0   \n",
       "3    26414.0  I have two groups of data.  Each with a differ...         23.0   \n",
       "4        NaN  The R-project\\n\\nhttp://www.r-project.org/\\n\\n...         23.0   \n",
       "\n",
       "          LastActivityDate                                              Title  \\\n",
       "0  2010-09-15T21:08:26.077                      Eliciting priors from experts   \n",
       "1  2016-06-27T06:44:40.147                                 What is normality?   \n",
       "2  2013-05-27T14:48:36.927  What are some valuable Statistical Analysis op...   \n",
       "3  2010-09-08T03:00:19.690  Assessing the significance of differences in d...   \n",
       "4  2010-07-19T19:21:15.063                                                NaN   \n",
       "\n",
       "                                        Tags  AnswerCount  CommentCount  \\\n",
       "0             <bayesian><prior><elicitation>          5.0             1   \n",
       "1                 <distributions><normality>          7.0             1   \n",
       "2                    <software><open-source>         19.0             4   \n",
       "3  <distributions><statistical-significance>          5.0             2   \n",
       "4                                        NaN          NaN             3   \n",
       "\n",
       "   FavoriteCount  \n",
       "0           23.0  \n",
       "1           10.0  \n",
       "2           39.0  \n",
       "3            5.0  \n",
       "4            NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "post_df = pd.read_csv('all_posts.csv', sep = \"\\t\")\n",
    "post_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215957</th>\n",
       "      <td>299976</td>\n",
       "      <td>2</td>\n",
       "      <td>299970.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-27T01:55:50.117</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n  We cannot simulate separate values (becaus...</td>\n",
       "      <td>8336.0</td>\n",
       "      <td>2017-08-27T01:55:50.117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215958</th>\n",
       "      <td>299977</td>\n",
       "      <td>2</td>\n",
       "      <td>299963.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-27T02:07:55.553</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neural networks could \"learn\" where the labels...</td>\n",
       "      <td>30621.0</td>\n",
       "      <td>2017-08-27T02:07:55.553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215959</th>\n",
       "      <td>299978</td>\n",
       "      <td>2</td>\n",
       "      <td>299669.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-27T03:26:04.497</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calculating $\\displaystyle\\hat{t}_i=\\int t\\, p...</td>\n",
       "      <td>8336.0</td>\n",
       "      <td>2017-08-27T03:26:04.497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215960</th>\n",
       "      <td>299979</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-27T03:52:26.230</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I have collected primary data of BISP with hel...</td>\n",
       "      <td>175092.0</td>\n",
       "      <td>2017-08-27T03:52:26.230</td>\n",
       "      <td>What should be the solution of insignificant (...</td>\n",
       "      <td>&lt;statistical-significance&gt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215961</th>\n",
       "      <td>299980</td>\n",
       "      <td>2</td>\n",
       "      <td>299446.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-27T04:05:22.103</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>While it's possible to combine word embeddings...</td>\n",
       "      <td>107579.0</td>\n",
       "      <td>2017-08-27T04:05:22.103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  PostTypeId  ParentId  AcceptedAnswerId  \\\n",
       "215957  299976           2  299970.0               NaN   \n",
       "215958  299977           2  299963.0               NaN   \n",
       "215959  299978           2  299669.0               NaN   \n",
       "215960  299979           1       NaN               NaN   \n",
       "215961  299980           2  299446.0               NaN   \n",
       "\n",
       "                   CreationDate  Score  ViewCount  \\\n",
       "215957  2017-08-27T01:55:50.117      0        NaN   \n",
       "215958  2017-08-27T02:07:55.553      0        NaN   \n",
       "215959  2017-08-27T03:26:04.497      0        NaN   \n",
       "215960  2017-08-27T03:52:26.230      0        2.0   \n",
       "215961  2017-08-27T04:05:22.103      0        NaN   \n",
       "\n",
       "                                                     Body  OwnerUserId  \\\n",
       "215957  \\n  We cannot simulate separate values (becaus...       8336.0   \n",
       "215958  Neural networks could \"learn\" where the labels...      30621.0   \n",
       "215959  Calculating $\\displaystyle\\hat{t}_i=\\int t\\, p...       8336.0   \n",
       "215960  I have collected primary data of BISP with hel...     175092.0   \n",
       "215961  While it's possible to combine word embeddings...     107579.0   \n",
       "\n",
       "               LastActivityDate  \\\n",
       "215957  2017-08-27T01:55:50.117   \n",
       "215958  2017-08-27T02:07:55.553   \n",
       "215959  2017-08-27T03:26:04.497   \n",
       "215960  2017-08-27T03:52:26.230   \n",
       "215961  2017-08-27T04:05:22.103   \n",
       "\n",
       "                                                    Title  \\\n",
       "215957                                                NaN   \n",
       "215958                                                NaN   \n",
       "215959                                                NaN   \n",
       "215960  What should be the solution of insignificant (...   \n",
       "215961                                                NaN   \n",
       "\n",
       "                              Tags  AnswerCount  CommentCount  FavoriteCount  \n",
       "215957                         NaN          NaN             0            NaN  \n",
       "215958                         NaN          NaN             0            NaN  \n",
       "215959                         NaN          NaN             0            NaN  \n",
       "215960  <statistical-significance>          0.0             0            NaN  \n",
       "215961                         NaN          NaN             0            NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215962"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = post_df.shape[0]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_df['Id'].unique()) == post_df.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    108954\n",
       "2    104797\n",
       "5      1091\n",
       "4      1091\n",
       "6        18\n",
       "3         6\n",
       "7         5\n",
       "Name: PostTypeId, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df['PostTypeId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108954, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Type1 = post_df.loc[post_df['PostTypeId'] == 1]\n",
    "Type1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108937"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Type1['Body'].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Type1['Body'].isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So the unique values of \"Body\" is less than the number of questions. This is because some asked their\n",
    "questions more than once and maybe the titles are different, but the contents are exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Confirm answers to questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the answers to question_id = 1 online and get the rows from the table to check that their parentId is indeed 1. \n",
    "But only on answer is accepted and is called accepted_answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19T19:19:46.160</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John Cook gives some interesting recommendatio...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2010-07-19T19:19:46.160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  PostTypeId  ParentId  AcceptedAnswerId             CreationDate  \\\n",
       "14  15           2       1.0               NaN  2010-07-19T19:19:46.160   \n",
       "\n",
       "    Score  ViewCount                                               Body  \\\n",
       "14     17        NaN  John Cook gives some interesting recommendatio...   \n",
       "\n",
       "    OwnerUserId         LastActivityDate Title Tags  AnswerCount  \\\n",
       "14          6.0  2010-07-19T19:19:46.160   NaN  NaN          NaN   \n",
       "\n",
       "    CommentCount  FavoriteCount  \n",
       "14             0            NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = 'John Cook gives some interesting recommendations.' # answer to question id = 1\n",
    "post_df.loc[post_df['Body'].apply(lambda x: answer in str(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PostTypeId</th>\n",
       "      <th>ParentId</th>\n",
       "      <th>AcceptedAnswerId</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Body</th>\n",
       "      <th>OwnerUserId</th>\n",
       "      <th>LastActivityDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>CommentCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>154</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010-07-19T22:40:47.947</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am currently researching the trial roulette ...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>2010-09-03T17:46:44.017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  PostTypeId  ParentId  AcceptedAnswerId             CreationDate  \\\n",
       "142  154           2       1.0               NaN  2010-07-19T22:40:47.947   \n",
       "\n",
       "     Score  ViewCount                                               Body  \\\n",
       "142     25        NaN  I am currently researching the trial roulette ...   \n",
       "\n",
       "     OwnerUserId         LastActivityDate Title Tags  AnswerCount  \\\n",
       "142        108.0  2010-09-03T17:46:44.017   NaN  NaN          NaN   \n",
       "\n",
       "     CommentCount  FavoriteCount  \n",
       "142             2            NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = 'Experts are given counters (or what one can think of as casino chips) representing equal densities whose total would sum up' # answer to question id = 1\n",
    "post_df.loc[post_df['Body'].apply(lambda x: answer in str(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are PostTypeIds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only post with postTypeId = 1 have titles. Because only questions have titles and answers don't have title.  Or if PostTypeId != 1, then title is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(post_df[\"Title\"].isnull()) + 108954 == post_df.shape[0] # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A guess: if PostTypeId is not 1 and 2, then they are answers and they are not accepted answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "acceptedAnswerId = post_df['AcceptedAnswerId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#34167 answers are accepted.\n",
    "len(acceptedAnswerId)\n",
    "not_accepted = post_df.loc[post_df['Id'].apply(lambda x: x not in acceptedAnswerId)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 7, 5, 4, 6, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_accepted['PostTypeId'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So PostTypeId is not related to whether an answer is accepted or not.\n",
    "\n",
    "** What is it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2182    Use this tag for any *on-topic* question that ...\n",
       "2264    Mixed (aka multilevel or hierarchical) models ...\n",
       "2419    Psychometrics has evolved as a subfield of psy...\n",
       "2778    Model selection is a problem of judging which ...\n",
       "2780    Cluster analysis is the task of partitioning d...\n",
       "2782    Time series are data observed over time (eithe...\n",
       "2784    Hypothesis testing assesses whether data suppo...\n",
       "2935    Prediction of the future events. It is a speci...\n",
       "3714    Stata is a proprietary  cross-platform general...\n",
       "5647    IBM SPSS Statistics (formerly SPSS, i.e. \"Stat...\n",
       "Name: Body, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.loc[post_df['PostTypeId'] == 4]['Body'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question posts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>How should I elicit prior distributions from e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>In many different statistical methods there is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>I have two groups of data.  Each with a differ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>Last year, I read a blog post from Brendan O'C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                              Title  \\\n",
       "0   1                      Eliciting priors from experts   \n",
       "1   2                                 What is normality?   \n",
       "2   3  What are some valuable Statistical Analysis op...   \n",
       "3   4  Assessing the significance of differences in d...   \n",
       "5   6  The Two Cultures: statistics vs. machine learn...   \n",
       "\n",
       "                                                Body  \n",
       "0  How should I elicit prior distributions from e...  \n",
       "1  In many different statistical methods there is...  \n",
       "2  What are some valuable Statistical Analysis op...  \n",
       "3  I have two groups of data.  Each with a differ...  \n",
       "5  Last year, I read a blog post from Brendan O'C...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_df = post_df.loc[post_df['PostTypeId'] == 1][['Id', 'Title','Body']]\n",
    "\n",
    "Q_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The R-project\\n\\nhttp://www.r-project.org/\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Incanter is a Clojure-based, R-like platform (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>See my response to \"Datasets for Running Stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Machine Learning seems to have its basis in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>I second that Jay. Why is R valuable? Here's a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                               Body\n",
       "4    5  The R-project\\n\\nhttp://www.r-project.org/\\n\\n...\n",
       "8    9  Incanter is a Clojure-based, R-like platform (...\n",
       "11  12  See my response to \"Datasets for Running Stati...\n",
       "12  13  Machine Learning seems to have its basis in th...\n",
       "13  14  I second that Jay. Why is R valuable? Here's a..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_df =  post_df.loc[post_df['PostTypeId'] != 1][['Id', 'Body']]\n",
    "A_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original post data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     How should I elicit prior distributions from e...\n",
      "1     In many different statistical methods there is...\n",
      "2     What are some valuable Statistical Analysis op...\n",
      "3     I have two groups of data.  Each with a differ...\n",
      "5     Last year, I read a blog post from Brendan O'C...\n",
      "6     I've been working on a new method for analyzin...\n",
      "7     Sorry, but the emptyness was a bit overwhelmin...\n",
      "9     Many studies in the social sciences use Likert...\n",
      "10    Is there a good, modern treatment covering the...\n",
      "16    I have four competing models which I use to pr...\n",
      "Name: Body, dtype: object\n",
      "4     The R-project\\n\\nhttp://www.r-project.org/\\n\\n...\n",
      "8     Incanter is a Clojure-based, R-like platform (...\n",
      "11    See my response to \"Datasets for Running Stati...\n",
      "12    Machine Learning seems to have its basis in th...\n",
      "13    I second that Jay. Why is R valuable? Here's a...\n",
      "14    John Cook gives some interesting recommendatio...\n",
      "15    Two projects spring to mind:\\n\\n\\nBugs - takin...\n",
      "17    Also see the UCI machine learning Data Reposit...\n",
      "18    Gapminder has a number (430 at the last look) ...\n",
      "19    The assumption of normality assumes your data ...\n",
      "Name: Body, dtype: object\n",
      "0                         Eliciting priors from experts\n",
      "1                                    What is normality?\n",
      "2     What are some valuable Statistical Analysis op...\n",
      "3     Assessing the significance of differences in d...\n",
      "5     The Two Cultures: statistics vs. machine learn...\n",
      "6                Locating freely available data samples\n",
      "7     So how many staticians *does* it take to screw...\n",
      "9     Under what conditions should Likert scales be ...\n",
      "10                Multivariate Interpolation Approaches\n",
      "16               How can I adapt ANOVA for binary data?\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "original_q_posts = Q_df['Body']\n",
    "original_a_posts = A_df['Body']\n",
    "\n",
    "original_q_titles = Q_df['Title']\n",
    "\n",
    "\n",
    "print(original_q_posts[0:10])\n",
    "print(original_a_posts[0:10])\n",
    "\n",
    "print(original_q_titles[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108954,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_q_titles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thefile = open('./data/questions_body.txt', 'w')\n",
    "for post in original_q_posts:\n",
    "    post = str(post).replace('\\n', ' ')\n",
    "    thefile.write(\"%s\\n\" % post)\n",
    "thefile = open('./data/answers_body.txt', 'w')\n",
    "for post in original_a_posts:\n",
    "    post = str(post).replace('\\n', ' ')\n",
    "    thefile.write(\"%s\\n\" % post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108954"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_q_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thefile = open('./data/questions_title.txt', 'w')\n",
    "\n",
    "for i, post in enumerate(original_q_titles):\n",
    "    post = \"*\"+ str(i+1) + str(post).replace('\\n', \"\") \n",
    "    thefile.write(\"%s\\n\" % post)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108953"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_work_status(singleCount, totalCount, currentCount = 0):\n",
    "    currentCount += singleCount\n",
    "    percentage = currentCount/totalCount *100\n",
    "    status = \">\" * int(percentage) +  \" \" * (100-int(percentage))\n",
    "    sys.stdout.write('\\rStatus:[{0}] {1:.2f}%'.format(status, percentage))\n",
    "    sys.stdout.flush()\n",
    "    if percentage >= 100:\n",
    "    \tprint('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lemmatizer = nltk.WordNetLemmatizer()\n",
    "# translation = str.maketrans(string.punctuation,' '*len(string.punctuation)) #define a replacement rule: replace all punctuation by \" \" \n",
    "\n",
    "# def preprocessing(line: str) -> str:\n",
    "#     line = str(line).translate(translation)\n",
    "#     line = nltk.word_tokenize(line.lower())\n",
    "#     line = [lemmatizer.lemmatize(t) for t in line]\n",
    "#     return \" \".join(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                \n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "                #This lowercases, tokenizes, de-accents (optional). – the output are final tokens = unicode strings, that won’t be processed any further.\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'is', 'sunny', 'day']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.utils.simple_preprocess(\"It is a sunny day.\") # remove stop words, lower cases, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It takes some time.\n",
    "fname = './data/'\n",
    "\n",
    "\n",
    "word2vec_question_title_corpus = list(read_corpus(fname + \"questions_title.txt\", tokens_only = True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108954"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec_question_title_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_question_corpus = list(read_corpus(fname + \"questions_body.txt\", tokens_only = True ))\n",
    "word2vec_answer_corpus = list(read_corpus(fname + \"answers_body.txt\", tokens_only = True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['how', 'should', 'elicit', 'prior', 'distributions', 'from', 'experts', 'when', 'fitting', 'bayesian', 'model'], ['in', 'many', 'different', 'statistical', 'methods', 'there', 'is', 'an', 'assumption', 'of', 'normality', 'what', 'is', 'normality', 'and', 'how', 'do', 'know', 'if', 'there', 'is', 'normality'], ['what', 'are', 'some', 'valuable', 'statistical', 'analysis', 'open', 'source', 'projects', 'available', 'right', 'now', 'edit', 'as', 'pointed', 'out', 'by', 'sharpie', 'valuable', 'could', 'mean', 'helping', 'you', 'get', 'things', 'done', 'faster', 'or', 'more', 'cheaply']]\n",
      "[['the', 'project', 'http', 'www', 'project', 'org', 'is', 'valuable', 'and', 'significant', 'because', 'it', 'was', 'the', 'first', 'widely', 'accepted', 'open', 'source', 'alternative', 'to', 'big', 'box', 'packages', 'it', 'mature', 'well', 'supported', 'and', 'standard', 'within', 'many', 'scientific', 'communities', 'some', 'reasons', 'why', 'it', 'is', 'useful', 'and', 'valuable', 'there', 'are', 'some', 'nice', 'tutorials', 'here'], ['incanter', 'is', 'clojure', 'based', 'like', 'platform', 'environment', 'libraries', 'for', 'statistical', 'computing', 'and', 'graphics'], ['see', 'my', 'response', 'to', 'datasets', 'for', 'running', 'statistical', 'analysis', 'on', 'in', 'reference', 'to', 'datasets', 'in']]\n",
      "[['eliciting', 'priors', 'from', 'experts'], ['what', 'is', 'normality'], ['what', 'are', 'some', 'valuable', 'statistical', 'analysis', 'open', 'source', 'projects']]\n",
      "108954\n",
      "107008\n",
      "108954\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_question_corpus[0:3])\n",
    "print(word2vec_answer_corpus[0:3])\n",
    "print(word2vec_question_title_corpus[0:3])\n",
    "\n",
    "print(len(word2vec_question_corpus))\n",
    "print(len(word2vec_answer_corpus))\n",
    "print(len(word2vec_question_title_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108954, 3)\n",
      "(107008, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Q_df.shape)\n",
    "print(A_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It takes some time.\n",
    "doc2vec_question_corpus = list(read_corpus(fname + \"questions_body.txt\" ))\n",
    "doc2vec_answer_corpus = list(read_corpus(fname + \"answers_body.txt\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_question_title_corpus = list(read_corpus(fname + \"questions_title.txt\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['how', 'should', 'elicit', 'prior', 'distributions', 'from', 'experts', 'when', 'fitting', 'bayesian', 'model'], tags=[0]), TaggedDocument(words=['in', 'many', 'different', 'statistical', 'methods', 'there', 'is', 'an', 'assumption', 'of', 'normality', 'what', 'is', 'normality', 'and', 'how', 'do', 'know', 'if', 'there', 'is', 'normality'], tags=[1]), TaggedDocument(words=['what', 'are', 'some', 'valuable', 'statistical', 'analysis', 'open', 'source', 'projects', 'available', 'right', 'now', 'edit', 'as', 'pointed', 'out', 'by', 'sharpie', 'valuable', 'could', 'mean', 'helping', 'you', 'get', 'things', 'done', 'faster', 'or', 'more', 'cheaply'], tags=[2])]\n",
      "[TaggedDocument(words=['the', 'project', 'http', 'www', 'project', 'org', 'is', 'valuable', 'and', 'significant', 'because', 'it', 'was', 'the', 'first', 'widely', 'accepted', 'open', 'source', 'alternative', 'to', 'big', 'box', 'packages', 'it', 'mature', 'well', 'supported', 'and', 'standard', 'within', 'many', 'scientific', 'communities', 'some', 'reasons', 'why', 'it', 'is', 'useful', 'and', 'valuable', 'there', 'are', 'some', 'nice', 'tutorials', 'here'], tags=[0]), TaggedDocument(words=['incanter', 'is', 'clojure', 'based', 'like', 'platform', 'environment', 'libraries', 'for', 'statistical', 'computing', 'and', 'graphics'], tags=[1]), TaggedDocument(words=['see', 'my', 'response', 'to', 'datasets', 'for', 'running', 'statistical', 'analysis', 'on', 'in', 'reference', 'to', 'datasets', 'in'], tags=[2])]\n",
      "[TaggedDocument(words=['eliciting', 'priors', 'from', 'experts'], tags=[0]), TaggedDocument(words=['what', 'is', 'normality'], tags=[1]), TaggedDocument(words=['what', 'are', 'some', 'valuable', 'statistical', 'analysis', 'open', 'source', 'projects'], tags=[2])]\n",
      "108954\n",
      "107008\n",
      "108954\n"
     ]
    }
   ],
   "source": [
    "print(doc2vec_question_corpus[0:3])\n",
    "print(doc2vec_answer_corpus[0:3])\n",
    "print(doc2vec_question_title_corpus[0:3])\n",
    "print(len(doc2vec_question_corpus))\n",
    "print(len(doc2vec_answer_corpus))\n",
    "print(len(doc2vec_question_title_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How to use the titles? Independently or merge into the body? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters for word2vec and doc2vec models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  20.   80.  140.  200.]\n",
      "[ 1.  4.]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "sizes = np.linspace(20, 200, num=4)\n",
    "print(sizes)\n",
    "\n",
    "windows = np.linspace(1,4, num =2 )\n",
    "print(windows)\n",
    "\n",
    "min_counts = [1]\n",
    "print(min_counts )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read from the config file. \n",
    "# paras = json.loads(open('./configure/word2vec.json').read())\n",
    "# print(paras)\n",
    "\n",
    "# min_count = paras['min_count']\n",
    "# size = paras['size']\n",
    "# window = paras['window']\n",
    "# workers = paras['workers']\n",
    "# sg = paras['sg']\n",
    "# alpha = paras[\"alpha\"]\n",
    "# hs = paras[\"hs\"]\n",
    "# negative = paras[\"negative\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_word_model(sentences, **paras):\n",
    "    min_count = paras['min_count']\n",
    "    size = paras['size']\n",
    "    window = paras['window']\n",
    "    model = gensim.models.Word2Vec(sentences, min_count = min_count, size = size, window = window, workers = 4)\n",
    "    fname = 'word2vector_model' + str(int(size))+\"_\"+str(int(window))+\"_\"+str(int(min_count))\n",
    "    model.save(fname)\n",
    "    return model\n",
    "\n",
    "def glance_word(model):\n",
    "#     print('*********vector representation of \\'statistics\\'********')\n",
    "#     print(model['statistics'])\n",
    "#     print('\\n')\n",
    "    print(model)\n",
    "    print('*************most similar words to vector***************')\n",
    "    print(model.most_similar('vector'))\n",
    "    print('\\n')\n",
    "    print(\"**************Similarity of \\'probability\\' and \\'distribution\\'******************\")\n",
    "    print(model.similarity('probability','distribution'))\n",
    "    print('\\n')\n",
    "    print(\"**************Similarity of \\'gaussian\\' and \\'normal\\'******************\")\n",
    "    print(model.similarity('gaussian','normal'))\n",
    "    print('\\n')\n",
    "\n",
    "def filter_corpus(sentences,model, doc = 0):\n",
    "    vocab = list(model.wv.vocab.keys())\n",
    "    print(vocab[0:10])\n",
    "    filtered_sentences = []\n",
    "    i = 0\n",
    "    total_count = len(sentences)\n",
    "    current_count = 0\n",
    "    for sentence in sentences:\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            show_work_status(1000, total_count, current_count)\n",
    "            current_count += 1000\n",
    "        if doc == 0:\n",
    "            words = list(filter(lambda x: x in vocab, sentence))\n",
    "            #words = [word for word in words]\n",
    "        if doc == 1:\n",
    "            words = list(filter(lambda x: x in vocab, sentence.words))\n",
    "            #words = [word for word in words]\n",
    "        filtered_sentences.append(words)\n",
    "    return filtered_sentences\n",
    "\n",
    "def evaluate_word_model(model,topk, min_count):\n",
    "    n =len(word2vec_question_corpus)\n",
    "    random.seed(10)\n",
    "    doc_id = random.randint(0,n)\n",
    "    # pick up a question randomly and find similar questions.\n",
    "    if min_count == 1:\n",
    "        word2vec_question_corpus_filter = word2vec_question_corpus\n",
    "    else:\n",
    "        word2vec_question_corpus_filter = filter_corpus(word2vec_question_corpus, model) \n",
    "    sen_interest = word2vec_question_corpus_filter[doc_id]\n",
    "    print('filtering is done.')\n",
    "    sims = []\n",
    "    current_n = 0\n",
    "    for i in range(int(len(Q_df['Body'])/1000)):\n",
    "#         if i % 500 == 0:\n",
    "#             show_work_status(500,int(len(Q_df['Body'])/1000),current_n)\n",
    "        compared_sen = word2vec_question_corpus_filter[i]\n",
    "        sim = model.wv.n_similarity(sen_interest, compared_sen)\n",
    "        sims.append(sim)\n",
    "    most_similar_index = heapq.nlargest(topk, range(len(sims)), key=sims.__getitem__)\n",
    "    print('*****************The question we are interested in is: ************************')\n",
    "    print(Q_df.iloc[doc_id]['Body'])\n",
    "    print(\"*******************Similar questions are **********************\")\n",
    "    for i in most_similar_index :\n",
    "        print(i)\n",
    "        print('similarity is ', sims[i])\n",
    "        print(Q_df.iloc[i]['Body'])\n",
    "        print(\"**********************\")\n",
    "    return sims, doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training  models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tuning parameters on grids\n",
    "models = {}\n",
    "for size in sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            paras =  {'size':int(size), 'window' : int(window), \"min_count\" :int(min_count)}\n",
    "            model_name =   str(int(size))+\"_\"+str(int(window))+\"_\"+str(int(min_count))\n",
    "            models[model_name] = train_word_model(word2vec_question_corpus, **paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_200_1_1\n",
      "Word2Vec(vocab=114657, size=200, alpha=0.025)\n",
      "*************most similar words to vector***************\n",
      "[('vectors', 0.779895007610321), ('sequence', 0.5380569696426392), ('scalar', 0.5291681885719299), ('numtoswitch', 0.5225784778594971), ('boldsymbol', 0.5199030637741089), ('boltzmann', 0.5158196687698364), ('mathbf', 0.5153965950012207), ('knowledgecenter', 0.5143557786941528), ('tuple', 0.5135056376457214), ('bf', 0.5073824524879456)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.45112888745\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.681192582025\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_140_4_1\n",
      "Word2Vec(vocab=114657, size=140, alpha=0.025)\n",
      "*************most similar words to vector***************\n",
      "[('vectors', 0.7099881172180176), ('scalar', 0.6378396153450012), ('matrix', 0.6202725172042847), ('element', 0.5627481937408447), ('mathbf', 0.5572167038917542), ('eigenvector', 0.5267043709754944), ('vec', 0.5258510112762451), ('bf', 0.5242753624916077), ('tuple', 0.516814112663269), ('column', 0.5162161588668823)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.448961777663\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.706741121688\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_140_1_1\n",
      "Word2Vec(vocab=114657, size=140, alpha=0.025)\n",
      "*************most similar words to vector***************\n",
      "[('vectors', 0.822967529296875), ('matrix', 0.5708754658699036), ('sequence', 0.5672173500061035), ('scalar', 0.5641210079193115), ('mathbf', 0.5640642642974854), ('bf', 0.5628111958503723), ('column', 0.558260440826416), ('tuple', 0.5491951704025269), ('boldsymbol', 0.5479047298431396), ('string', 0.5440750122070312)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.497772343032\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.741915646694\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_80_4_1\n",
      "Word2Vec(vocab=114657, size=80, alpha=0.025)\n",
      "*************most similar words to vector***************\n",
      "[('vectors', 0.7239915132522583), ('scalar', 0.7211562991142273), ('matrix', 0.6850595474243164), ('mathbf', 0.646669864654541), ('sequence', 0.6230834722518921), ('vec', 0.6134064793586731), ('element', 0.6114705204963684), ('bf', 0.6107062101364136), ('input', 0.6015327572822571), ('column', 0.5781445503234863)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.515052670808\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.733536895014\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_20_4_1\n",
      "Word2Vec(vocab=114657, size=20, alpha=0.025)\n",
      "*************most similar words to vector***************\n",
      "[('scalar', 0.8706117868423462), ('transition', 0.8495502471923828), ('space', 0.837902843952179), ('matrix', 0.8325852155685425), ('input', 0.8281059265136719), ('vectors', 0.8119399547576904), ('dimension', 0.8113476037979126), ('vec', 0.7972744703292847), ('subspace', 0.7939809560775757), ('mathbf', 0.7896917462348938)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.78599095272\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.838610883138\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_20_1_1\n",
      "Word2Vec(vocab=114657, size=20, alpha=0.025)\n",
      "*************most similar words to vector***************\n",
      "[('space', 0.9473310708999634), ('vectors', 0.914708137512207), ('sequence', 0.8771323561668396), ('spaces', 0.8736444115638733), ('transition', 0.8613157272338867), ('scalar', 0.8600436449050903), ('maps', 0.8563435077667236), ('dimension', 0.8533729314804077), ('weights', 0.8515586256980896), ('importances', 0.8501777648925781)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.858699372133\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.938681875123\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_200_4_1\n",
      "Word2Vec(vocab=114657, size=200, alpha=0.025)\n",
      "*************most similar words to vector***************\n",
      "[('vectors', 0.6916359663009644), ('scalar', 0.6138845682144165), ('matrix', 0.5778968334197998), ('mathbf', 0.5343183279037476), ('element', 0.5221911668777466), ('column', 0.5164576768875122), ('sequence', 0.5141034126281738), ('subspace', 0.4740733504295349), ('input', 0.46847397089004517), ('eigenvector', 0.4675440788269043)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.41971423772\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.651019859879\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_80_1_1\n",
      "Word2Vec(vocab=114657, size=80, alpha=0.025)\n",
      "*************most similar words to vector***************\n",
      "[('vectors', 0.8457133769989014), ('matrix', 0.7221908569335938), ('string', 0.6666252017021179), ('bf', 0.6623561382293701), ('mathbf', 0.6575301885604858), ('scalar', 0.6508212089538574), ('matrices', 0.6457116603851318), ('sequence', 0.6423742175102234), ('boldsymbol', 0.6296709179878235), ('space', 0.62398362159729)]\n",
      "\n",
      "\n",
      "**************Similarity of 'probability' and 'distribution'******************\n",
      "0.616501384039\n",
      "\n",
      "\n",
      "**************Similarity of 'gaussian' and 'normal'******************\n",
      "0.843183440146\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "for key in models.keys():\n",
    "    print(key)\n",
    "    glance_word(models[key])\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_200_1_1\n",
      "filtering is done.\n",
      "*****************The question we are interested in is: ************************\n",
      "Let $F:(-\\infty,\\infty)\\rightarrow[0,1]$ and $G:(-\\infty,\\infty)\\rightarrow[0,1]$ be two CDFs with PDFs $f$ and $g$, respectively. Is there a connection/inequality between:\n",
      "\n",
      "$$d_1 = \\int_{-\\infty}^{\\infty}\\vert f(t) - g(t) \\vert dt,$$\n",
      "and\n",
      "$$d_2 = \\int_{-\\infty}^{\\infty}\\vert F(t) - G(t) \\vert dt?$$\n",
      "Assuming $d_2$ exists ($d_1$ is always finite).\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "51\n",
      "similarity is  0.846244543109\n",
      "I know this must be standard material, but I had difficulty in finding a proof in this form.\n",
      "\n",
      "Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.\n",
      "\n",
      "Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let\n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\n",
      "\\bar e &amp;= v - X\\bar y\n",
      "\\end{align}\\right.\\quad.$$\n",
      "\n",
      "If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and \n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "u &amp;= c^T\\bar y\\\\\n",
      "s^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\n",
      "\\end{align}\\right.\\quad,$$\n",
      "\n",
      "then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.\n",
      "\n",
      "I would be grateful if you could provide an outline for its proof.\n",
      "\n",
      "**********************\n",
      "71\n",
      "similarity is  0.81279390098\n",
      "If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?\n",
      "\n",
      "**********************\n",
      "18\n",
      "similarity is  0.781151199468\n",
      "I have a data set that I'd expect to follow a Poisson distribution, but it is overdispersed by about 3-fold. At the present, I'm modelling this overdispersion using something like the following code in R.\n",
      "\n",
      "## assuming a median value of 1500\n",
      "med = 1500\n",
      "rawdist = rpois(1000000,med)\n",
      "oDdist = rawDist + ((rawDist-med)*3)\n",
      "\n",
      "\n",
      "Visually, this seems to fit my empirical data very well. If I'm happy with the fit, is there any reason that I should be doing something more complex, like using a negative binomial distribution, as described here? (If so, any pointers or links on doing so would be much appreciated).\n",
      "\n",
      "Oh, and I'm aware that this creates a slightly jagged distribution (due to the multiplication by three), but that shouldn't matter for my application.\n",
      "\n",
      "\n",
      "\n",
      "Update:  For the sake of anyone else who searches and finds this question, here's a simple R function to model an overdispersed poisson using a negative binomial distribution. Set d to the desired mean/variance ratio:\n",
      "\n",
      "rpois.od&lt;-function (n, lambda,d=1) {\n",
      "  if (d==1)\n",
      "    rpois(n, lambda)\n",
      "  else\n",
      "     rnbinom(n, size=(lambda/(d-1)), mu=lambda)\n",
      "}\n",
      "\n",
      "\n",
      "(via the R mailing list: https://stat.ethz.ch/pipermail/r-help/2002-June/022425.html)\n",
      "\n",
      "**********************\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_140_4_1\n",
      "filtering is done.\n",
      "*****************The question we are interested in is: ************************\n",
      "Let $F:(-\\infty,\\infty)\\rightarrow[0,1]$ and $G:(-\\infty,\\infty)\\rightarrow[0,1]$ be two CDFs with PDFs $f$ and $g$, respectively. Is there a connection/inequality between:\n",
      "\n",
      "$$d_1 = \\int_{-\\infty}^{\\infty}\\vert f(t) - g(t) \\vert dt,$$\n",
      "and\n",
      "$$d_2 = \\int_{-\\infty}^{\\infty}\\vert F(t) - G(t) \\vert dt?$$\n",
      "Assuming $d_2$ exists ($d_1$ is always finite).\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "51\n",
      "similarity is  0.666129457882\n",
      "I know this must be standard material, but I had difficulty in finding a proof in this form.\n",
      "\n",
      "Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.\n",
      "\n",
      "Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let\n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\n",
      "\\bar e &amp;= v - X\\bar y\n",
      "\\end{align}\\right.\\quad.$$\n",
      "\n",
      "If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and \n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "u &amp;= c^T\\bar y\\\\\n",
      "s^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\n",
      "\\end{align}\\right.\\quad,$$\n",
      "\n",
      "then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.\n",
      "\n",
      "I would be grateful if you could provide an outline for its proof.\n",
      "\n",
      "**********************\n",
      "71\n",
      "similarity is  0.617783580414\n",
      "If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?\n",
      "\n",
      "**********************\n",
      "12\n",
      "similarity is  0.576401846666\n",
      "How can I find the PDF (probability density function) of a distribution given the CDF (cumulative distribution function)?\n",
      "\n",
      "**********************\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_140_1_1\n",
      "filtering is done.\n",
      "*****************The question we are interested in is: ************************\n",
      "Let $F:(-\\infty,\\infty)\\rightarrow[0,1]$ and $G:(-\\infty,\\infty)\\rightarrow[0,1]$ be two CDFs with PDFs $f$ and $g$, respectively. Is there a connection/inequality between:\n",
      "\n",
      "$$d_1 = \\int_{-\\infty}^{\\infty}\\vert f(t) - g(t) \\vert dt,$$\n",
      "and\n",
      "$$d_2 = \\int_{-\\infty}^{\\infty}\\vert F(t) - G(t) \\vert dt?$$\n",
      "Assuming $d_2$ exists ($d_1$ is always finite).\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "51\n",
      "similarity is  0.850424591051\n",
      "I know this must be standard material, but I had difficulty in finding a proof in this form.\n",
      "\n",
      "Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.\n",
      "\n",
      "Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let\n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\n",
      "\\bar e &amp;= v - X\\bar y\n",
      "\\end{align}\\right.\\quad.$$\n",
      "\n",
      "If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and \n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "u &amp;= c^T\\bar y\\\\\n",
      "s^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\n",
      "\\end{align}\\right.\\quad,$$\n",
      "\n",
      "then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.\n",
      "\n",
      "I would be grateful if you could provide an outline for its proof.\n",
      "\n",
      "**********************\n",
      "71\n",
      "similarity is  0.820730943476\n",
      "If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?\n",
      "\n",
      "**********************\n",
      "18\n",
      "similarity is  0.78331425253\n",
      "I have a data set that I'd expect to follow a Poisson distribution, but it is overdispersed by about 3-fold. At the present, I'm modelling this overdispersion using something like the following code in R.\n",
      "\n",
      "## assuming a median value of 1500\n",
      "med = 1500\n",
      "rawdist = rpois(1000000,med)\n",
      "oDdist = rawDist + ((rawDist-med)*3)\n",
      "\n",
      "\n",
      "Visually, this seems to fit my empirical data very well. If I'm happy with the fit, is there any reason that I should be doing something more complex, like using a negative binomial distribution, as described here? (If so, any pointers or links on doing so would be much appreciated).\n",
      "\n",
      "Oh, and I'm aware that this creates a slightly jagged distribution (due to the multiplication by three), but that shouldn't matter for my application.\n",
      "\n",
      "\n",
      "\n",
      "Update:  For the sake of anyone else who searches and finds this question, here's a simple R function to model an overdispersed poisson using a negative binomial distribution. Set d to the desired mean/variance ratio:\n",
      "\n",
      "rpois.od&lt;-function (n, lambda,d=1) {\n",
      "  if (d==1)\n",
      "    rpois(n, lambda)\n",
      "  else\n",
      "     rnbinom(n, size=(lambda/(d-1)), mu=lambda)\n",
      "}\n",
      "\n",
      "\n",
      "(via the R mailing list: https://stat.ethz.ch/pipermail/r-help/2002-June/022425.html)\n",
      "\n",
      "**********************\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_80_4_1\n",
      "filtering is done.\n",
      "*****************The question we are interested in is: ************************\n",
      "Let $F:(-\\infty,\\infty)\\rightarrow[0,1]$ and $G:(-\\infty,\\infty)\\rightarrow[0,1]$ be two CDFs with PDFs $f$ and $g$, respectively. Is there a connection/inequality between:\n",
      "\n",
      "$$d_1 = \\int_{-\\infty}^{\\infty}\\vert f(t) - g(t) \\vert dt,$$\n",
      "and\n",
      "$$d_2 = \\int_{-\\infty}^{\\infty}\\vert F(t) - G(t) \\vert dt?$$\n",
      "Assuming $d_2$ exists ($d_1$ is always finite).\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "51\n",
      "similarity is  0.665150542688\n",
      "I know this must be standard material, but I had difficulty in finding a proof in this form.\n",
      "\n",
      "Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.\n",
      "\n",
      "Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let\n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\n",
      "\\bar e &amp;= v - X\\bar y\n",
      "\\end{align}\\right.\\quad.$$\n",
      "\n",
      "If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and \n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "u &amp;= c^T\\bar y\\\\\n",
      "s^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\n",
      "\\end{align}\\right.\\quad,$$\n",
      "\n",
      "then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.\n",
      "\n",
      "I would be grateful if you could provide an outline for its proof.\n",
      "\n",
      "**********************\n",
      "71\n",
      "similarity is  0.623201826559\n",
      "If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?\n",
      "\n",
      "**********************\n",
      "12\n",
      "similarity is  0.57878898291\n",
      "How can I find the PDF (probability density function) of a distribution given the CDF (cumulative distribution function)?\n",
      "\n",
      "**********************\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_20_4_1\n",
      "filtering is done.\n",
      "*****************The question we are interested in is: ************************\n",
      "Let $F:(-\\infty,\\infty)\\rightarrow[0,1]$ and $G:(-\\infty,\\infty)\\rightarrow[0,1]$ be two CDFs with PDFs $f$ and $g$, respectively. Is there a connection/inequality between:\n",
      "\n",
      "$$d_1 = \\int_{-\\infty}^{\\infty}\\vert f(t) - g(t) \\vert dt,$$\n",
      "and\n",
      "$$d_2 = \\int_{-\\infty}^{\\infty}\\vert F(t) - G(t) \\vert dt?$$\n",
      "Assuming $d_2$ exists ($d_1$ is always finite).\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "51\n",
      "similarity is  0.667811536516\n",
      "I know this must be standard material, but I had difficulty in finding a proof in this form.\n",
      "\n",
      "Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.\n",
      "\n",
      "Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let\n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\n",
      "\\bar e &amp;= v - X\\bar y\n",
      "\\end{align}\\right.\\quad.$$\n",
      "\n",
      "If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and \n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "u &amp;= c^T\\bar y\\\\\n",
      "s^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\n",
      "\\end{align}\\right.\\quad,$$\n",
      "\n",
      "then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.\n",
      "\n",
      "I would be grateful if you could provide an outline for its proof.\n",
      "\n",
      "**********************\n",
      "71\n",
      "similarity is  0.587069653001\n",
      "If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?\n",
      "\n",
      "**********************\n",
      "12\n",
      "similarity is  0.570988282794\n",
      "How can I find the PDF (probability density function) of a distribution given the CDF (cumulative distribution function)?\n",
      "\n",
      "**********************\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_20_1_1\n",
      "filtering is done.\n",
      "*****************The question we are interested in is: ************************\n",
      "Let $F:(-\\infty,\\infty)\\rightarrow[0,1]$ and $G:(-\\infty,\\infty)\\rightarrow[0,1]$ be two CDFs with PDFs $f$ and $g$, respectively. Is there a connection/inequality between:\n",
      "\n",
      "$$d_1 = \\int_{-\\infty}^{\\infty}\\vert f(t) - g(t) \\vert dt,$$\n",
      "and\n",
      "$$d_2 = \\int_{-\\infty}^{\\infty}\\vert F(t) - G(t) \\vert dt?$$\n",
      "Assuming $d_2$ exists ($d_1$ is always finite).\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "51\n",
      "similarity is  0.918655790662\n",
      "I know this must be standard material, but I had difficulty in finding a proof in this form.\n",
      "\n",
      "Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.\n",
      "\n",
      "Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let\n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\n",
      "\\bar e &amp;= v - X\\bar y\n",
      "\\end{align}\\right.\\quad.$$\n",
      "\n",
      "If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and \n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "u &amp;= c^T\\bar y\\\\\n",
      "s^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\n",
      "\\end{align}\\right.\\quad,$$\n",
      "\n",
      "then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.\n",
      "\n",
      "I would be grateful if you could provide an outline for its proof.\n",
      "\n",
      "**********************\n",
      "71\n",
      "similarity is  0.873587319298\n",
      "If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?\n",
      "\n",
      "**********************\n",
      "55\n",
      "similarity is  0.821844090517\n",
      "For univariate kernel density estimators (KDE), I use Silverman's rule for calculating $h$:\n",
      "\n",
      "\\begin{equation}\n",
      "0.9 \\min(sd, IQR/1.34)\\times n^{-0.2}\n",
      "\\end{equation}\n",
      "\n",
      "What are the standard rules for multivariate KDE (assuming a Normal kernel).\n",
      "\n",
      "**********************\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_200_4_1\n",
      "filtering is done.\n",
      "*****************The question we are interested in is: ************************\n",
      "Let $F:(-\\infty,\\infty)\\rightarrow[0,1]$ and $G:(-\\infty,\\infty)\\rightarrow[0,1]$ be two CDFs with PDFs $f$ and $g$, respectively. Is there a connection/inequality between:\n",
      "\n",
      "$$d_1 = \\int_{-\\infty}^{\\infty}\\vert f(t) - g(t) \\vert dt,$$\n",
      "and\n",
      "$$d_2 = \\int_{-\\infty}^{\\infty}\\vert F(t) - G(t) \\vert dt?$$\n",
      "Assuming $d_2$ exists ($d_1$ is always finite).\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "51\n",
      "similarity is  0.673701574331\n",
      "I know this must be standard material, but I had difficulty in finding a proof in this form.\n",
      "\n",
      "Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.\n",
      "\n",
      "Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let\n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\n",
      "\\bar e &amp;= v - X\\bar y\n",
      "\\end{align}\\right.\\quad.$$\n",
      "\n",
      "If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and \n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "u &amp;= c^T\\bar y\\\\\n",
      "s^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\n",
      "\\end{align}\\right.\\quad,$$\n",
      "\n",
      "then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.\n",
      "\n",
      "I would be grateful if you could provide an outline for its proof.\n",
      "\n",
      "**********************\n",
      "71\n",
      "similarity is  0.626545816755\n",
      "If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?\n",
      "\n",
      "**********************\n",
      "12\n",
      "similarity is  0.586976284687\n",
      "How can I find the PDF (probability density function) of a distribution given the CDF (cumulative distribution function)?\n",
      "\n",
      "**********************\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "title_80_1_1\n",
      "filtering is done.\n",
      "*****************The question we are interested in is: ************************\n",
      "Let $F:(-\\infty,\\infty)\\rightarrow[0,1]$ and $G:(-\\infty,\\infty)\\rightarrow[0,1]$ be two CDFs with PDFs $f$ and $g$, respectively. Is there a connection/inequality between:\n",
      "\n",
      "$$d_1 = \\int_{-\\infty}^{\\infty}\\vert f(t) - g(t) \\vert dt,$$\n",
      "and\n",
      "$$d_2 = \\int_{-\\infty}^{\\infty}\\vert F(t) - G(t) \\vert dt?$$\n",
      "Assuming $d_2$ exists ($d_1$ is always finite).\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "51\n",
      "similarity is  0.864891488794\n",
      "I know this must be standard material, but I had difficulty in finding a proof in this form.\n",
      "\n",
      "Let $e$ be a standard white Gaussian vector of size $N$.  Let all the other matrices in the following be constant.\n",
      "\n",
      "Let $v = Xy + e$, where $X$ is an $N\\times L$ matrix and $y$ is an $N\\times 1$ vector, and let\n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "\\bar y &amp;= (X^TX)^{-1}X^Tv\\\\\n",
      "\\bar e &amp;= v - X\\bar y\n",
      "\\end{align}\\right.\\quad.$$\n",
      "\n",
      "If $c$ is any constant vector, $J = N - \\mathrm{rank}(X)$, and \n",
      "\n",
      "$$\\left\\{\\begin{align}\n",
      "u &amp;= c^T\\bar y\\\\\n",
      "s^2 &amp;= \\bar e^T\\bar ec^T(X^TX)^{-1}c\n",
      "\\end{align}\\right.\\quad,$$\n",
      "\n",
      "then the random variable defined as $t = u/\\sqrt{s^2/J}$ follows a normalized Student's T distribution with J degrees of freedom.\n",
      "\n",
      "I would be grateful if you could provide an outline for its proof.\n",
      "\n",
      "**********************\n",
      "71\n",
      "similarity is  0.832384994094\n",
      "If $X_1, ..., X_n$ are independent identically-distributed random variables, what can be said about the distribution of $\\min(X_1, ..., X_n)$ in general?\n",
      "\n",
      "**********************\n",
      "18\n",
      "similarity is  0.787798262011\n",
      "I have a data set that I'd expect to follow a Poisson distribution, but it is overdispersed by about 3-fold. At the present, I'm modelling this overdispersion using something like the following code in R.\n",
      "\n",
      "## assuming a median value of 1500\n",
      "med = 1500\n",
      "rawdist = rpois(1000000,med)\n",
      "oDdist = rawDist + ((rawDist-med)*3)\n",
      "\n",
      "\n",
      "Visually, this seems to fit my empirical data very well. If I'm happy with the fit, is there any reason that I should be doing something more complex, like using a negative binomial distribution, as described here? (If so, any pointers or links on doing so would be much appreciated).\n",
      "\n",
      "Oh, and I'm aware that this creates a slightly jagged distribution (due to the multiplication by three), but that shouldn't matter for my application.\n",
      "\n",
      "\n",
      "\n",
      "Update:  For the sake of anyone else who searches and finds this question, here's a simple R function to model an overdispersed poisson using a negative binomial distribution. Set d to the desired mean/variance ratio:\n",
      "\n",
      "rpois.od&lt;-function (n, lambda,d=1) {\n",
      "  if (d==1)\n",
      "    rpois(n, lambda)\n",
      "  else\n",
      "     rnbinom(n, size=(lambda/(d-1)), mu=lambda)\n",
      "}\n",
      "\n",
      "\n",
      "(via the R mailing list: https://stat.ethz.ch/pipermail/r-help/2002-June/022425.html)\n",
      "\n",
      "**********************\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "for key in models.keys():\n",
    "    print(key)\n",
    "    evaluate_word_model(models[key], 3, 1)\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec mdoel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_doc_model(sentences, **paras):\n",
    "    min_count = paras['min_count']\n",
    "    size = paras['size']\n",
    "    window = paras['window']\n",
    "    doc2vec_model = gensim.models.doc2vec.Doc2Vec(\n",
    "                min_count = min_count, \n",
    "                 size =size, \n",
    "                 window =window\n",
    "    )\n",
    "    doc2vec_model.build_vocab(sentences)\n",
    "    \n",
    "    fname = 'doc2vector_model' + str(int(size))+\"_\"+str(int(window))+\"_\"+str(int(min_count))\n",
    "    doc2vec_model.save(fname)\n",
    "    return doc2vec_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def glance_doc(model):\n",
    "#     print(model)\n",
    "#     print('*************most similar words to vector***************')\n",
    "#     print(model.docvecs.most_similar(0))\n",
    "#     print('\\n')\n",
    "#     print(\"**************Similarity of \\'probability\\' and \\'distribution\\'******************\")\n",
    "#     print(model.docvecs.similarity('probability','distribution'))\n",
    "#     print('\\n')\n",
    "#     print(\"**************Similarity of \\'gaussian\\' and \\'normal\\'******************\")\n",
    "#     print(model.docvecs.similarity('gaussian','normal'))\n",
    "#     print('\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_doc_model(doc2vec_model,topk, min_count):\n",
    "    n =len(doc2vec_question_corpus)\n",
    "    random.seed(0)\n",
    "    doc_id = random.randint(0,n)\n",
    "    \n",
    "    inferred_vector = doc2vec_model.infer_vector(doc2vec_question_corpus[doc_id].words)\n",
    "    sims = doc2vec_model.docvecs.most_similar([inferred_vector],  topn= len(doc2vec_question_corpus))\n",
    "    \n",
    "    most_similar_index = sims[0:topk]\n",
    "    print('*****************The question we are interested in is: ************************')\n",
    "    print(Q_df.iloc[doc_id]['Body'])\n",
    "    print(\"*******************Similar questions are **********************\")\n",
    "    for i in most_similar_index :\n",
    "        \n",
    "        print('similarity is ', i)\n",
    "        print(Q_df.iloc[i[0]]['Body'])\n",
    "        print(\"**********************\")\n",
    "    return sims, doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each object represents a single sentence, and consists of two simple lists: a list of words and a list of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tuning parameters on grids\n",
    "doc_models = {}\n",
    "for size in sizes:\n",
    "    for window in windows:\n",
    "        for min_count in min_counts:\n",
    "            paras =  {'size':int(size), 'window' : int(window), \"min_count\" :int(min_count)}\n",
    "            model_name = 'doc_'+  str(int(size))+\"_\"+str(int(window))+\"_\"+str(int(min_count))\n",
    "            doc_models[model_name] = train_doc_model(doc2vec_question_corpus, **paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['doc80_4_1', 'doc20_1_1', 'doc140_1_1', 'doc140_4_1', 'doc20_4_1', 'doc200_1_1', 'doc80_1_1', 'doc200_4_1'])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc80_4_1\n",
      "*****************The question we are interested in is: ************************\n",
      "I would be very thankful if somebody could help me and explain the answer in simple terms.\n",
      "\n",
      "I have y, x, z variables. They are countinuous, no missing values. y is dependent variable, x and z independent. There is significant correlation between them (varies from 0.25 to 0.35).\n",
      "\n",
      "My hypothesis is that when z is very high, then x and y dependency is linear (x goes bigger, y smaller).\n",
      "When z is very low, then in the beginning the situation is the same (dependency is linear), but in the end (about last quarter) it changes to quadratic.\n",
      "\n",
      "So basically there are two different relationships between x and y, but the nature of the relationship depends on z which is again a continuous variable. \n",
      "\n",
      "How can I test this model? Is it possible doing linear regression or do I need to use something else?\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "similarity is  (15463, 0.47558698058128357)\n",
      "Could someone please explain what \"$J$\" consists of in this paper, equation 1.5.\n",
      "$$\n",
      "J \\sim N(\\beta, \\sigma^2 I/k)\n",
      "$$\n",
      "What's $\\beta$ here? What's $N$?\n",
      "\n",
      "Also, why are they putting that much effort in deriving $k$ with complicated formulas instead of just saying \"use CV to find the best $k$\"?\n",
      "\n",
      "**********************\n",
      "similarity is  (25661, 0.4738842248916626)\n",
      "I am interested in writing a non-asymptotic rate of convergence for SLLN as a function of number of samples.  \n",
      "\n",
      "From the literature I've read so far, CLT provides an asymptotic convergence rate of $(1/\\sqrt N)$ for SLLN. \n",
      "\n",
      "Also, Berry-Esseen provides a non-asymptotic bound in terms of c.d.f's.\n",
      "\n",
      "$$|F_N(x) - \\Phi(x)| \\le \\frac{C\\mathbb{E}(|X|^3)}{\\sigma^3\\sqrt N}$$ \n",
      "\n",
      "Is there a Berry-Esseen like statement to bound the difference between sample mean and the expected value of the underlying distribution as a function of N (number of samples)? \n",
      "\n",
      "**********************\n",
      "similarity is  (87921, 0.4556872844696045)\n",
      "If you perform, say 10 fold cv with logistic regression and then average the coefficient vectors from each turn, does that average roughly equal the coefficient vector you would get by fitting a logistic regression to the whole dataset?\n",
      "\n",
      "Here is an example using a sklearn logistic regression that performs a random search over C on a very small dataset (1000 obs) with 20% positive cases. There are 5 features. Y axis is coefficient value.  One set of red lines are the coefficients on the full set, the others are the coefficients over the folds (it should be clear which are medians).\n",
      "\n",
      "\n",
      "\n",
      "**********************\n",
      "doc20_1_1\n",
      "*****************The question we are interested in is: ************************\n",
      "I would be very thankful if somebody could help me and explain the answer in simple terms.\n",
      "\n",
      "I have y, x, z variables. They are countinuous, no missing values. y is dependent variable, x and z independent. There is significant correlation between them (varies from 0.25 to 0.35).\n",
      "\n",
      "My hypothesis is that when z is very high, then x and y dependency is linear (x goes bigger, y smaller).\n",
      "When z is very low, then in the beginning the situation is the same (dependency is linear), but in the end (about last quarter) it changes to quadratic.\n",
      "\n",
      "So basically there are two different relationships between x and y, but the nature of the relationship depends on z which is again a continuous variable. \n",
      "\n",
      "How can I test this model? Is it possible doing linear regression or do I need to use something else?\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "similarity is  (53205, 0.7938619256019592)\n",
      "Is there a limit to how big you should make K for k-fold cross validation? I understand as K gets bigger performing the CV will take longer, but aside from that, is there any reason not to make K = n? is there a point where your validation sets are so small that even if you average them, the result is garbage?\n",
      "\n",
      "**********************\n",
      "similarity is  (96417, 0.7830068469047546)\n",
      "This is according to the Ljung-Box $Q$ statistic of residuals squared and ARCH-LM test. Both suggest there are ARCH effects remaining after lag 1 even after I have estimated my GARCH (1,1) model. I have no AR or MA terms in my conditional mean specification, just a constant, and all autocorrelation has been removed as per the $Q$-statistics of the residuals. What would be the best way to then remove the remaining ARCH effects?\n",
      "\n",
      "Is there even a need to remove all ARCH effects after the GARCH estimation (given GARCH models are iid), if one wishes to forecast volatility?\n",
      "\n",
      "**********************\n",
      "similarity is  (21764, 0.7724628448486328)\n",
      "Summary:\n",
      "I created a model that tries to explain success factors in crowdfunding initiatives. I collected the data by scraping it directly from the crowdfunding platform. That of course limits my research in terms of data availability. \n",
      "\n",
      "In order to come up with conceptual model I adopted a theory, that assumes latent variables (such that I do not observe). What I observe is measures or proxies (directly collected form the website) that to some extent depict that latent variables which are embedded in my conceptual framework. Of course I set few hypothesis based on that latent variables.\n",
      "\n",
      "Example: I have adopt a theory about the online community reputation. For measuring reputation I set 2 measures - N of successfully funded projects, N unsuccessfully funded projects. That example is quite simple, however, but for some other concepts I have multiple measures on different scales.\n",
      "\n",
      "Questions: \n",
      "1) How should I test my model reliability or/and validity? Is that necessary? \n",
      "\n",
      "I think I may not consider Cronbach's alpha in that case, since its not a survey approach, and often the measurement scales are different. Another option I was thinking of is CFA, but SPSS have only EFA. Would setting an EFA with defined number of factors equal to the number of my latent variables is an option?\n",
      "\n",
      "2) How I should test (accept/reject) my hypothesis? Should I refer to the measures themselves, or should I refer to the latent variables? How I can create the latent variables if necessary?\n",
      "\n",
      "**********************\n",
      "doc140_1_1\n",
      "*****************The question we are interested in is: ************************\n",
      "I would be very thankful if somebody could help me and explain the answer in simple terms.\n",
      "\n",
      "I have y, x, z variables. They are countinuous, no missing values. y is dependent variable, x and z independent. There is significant correlation between them (varies from 0.25 to 0.35).\n",
      "\n",
      "My hypothesis is that when z is very high, then x and y dependency is linear (x goes bigger, y smaller).\n",
      "When z is very low, then in the beginning the situation is the same (dependency is linear), but in the end (about last quarter) it changes to quadratic.\n",
      "\n",
      "So basically there are two different relationships between x and y, but the nature of the relationship depends on z which is again a continuous variable. \n",
      "\n",
      "How can I test this model? Is it possible doing linear regression or do I need to use something else?\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "similarity is  (25661, 0.3984331786632538)\n",
      "I am interested in writing a non-asymptotic rate of convergence for SLLN as a function of number of samples.  \n",
      "\n",
      "From the literature I've read so far, CLT provides an asymptotic convergence rate of $(1/\\sqrt N)$ for SLLN. \n",
      "\n",
      "Also, Berry-Esseen provides a non-asymptotic bound in terms of c.d.f's.\n",
      "\n",
      "$$|F_N(x) - \\Phi(x)| \\le \\frac{C\\mathbb{E}(|X|^3)}{\\sigma^3\\sqrt N}$$ \n",
      "\n",
      "Is there a Berry-Esseen like statement to bound the difference between sample mean and the expected value of the underlying distribution as a function of N (number of samples)? \n",
      "\n",
      "**********************\n",
      "similarity is  (33963, 0.3416898250579834)\n",
      "For regression analysis, it is often useful to know the data generating process to check how the used method works. While it is fairly simple to do this for a simple linear regression, this is not the case when the dependent variable has to follow a specific distribution. \n",
      "\n",
      "Consider a simple linear regression:\n",
      "\n",
      "N    &lt;- 100\n",
      "x    &lt;- rnorm(N)\n",
      "beta &lt;- 3 + 0.4*rnorm(N)\n",
      "y    &lt;- 1 + x * beta + .75*rnorm(N)\n",
      "\n",
      "\n",
      "Is there any way to use the same approach but to get y be other than normal, say left skewed?\n",
      "\n",
      "**********************\n",
      "similarity is  (87921, 0.3350788354873657)\n",
      "If you perform, say 10 fold cv with logistic regression and then average the coefficient vectors from each turn, does that average roughly equal the coefficient vector you would get by fitting a logistic regression to the whole dataset?\n",
      "\n",
      "Here is an example using a sklearn logistic regression that performs a random search over C on a very small dataset (1000 obs) with 20% positive cases. There are 5 features. Y axis is coefficient value.  One set of red lines are the coefficients on the full set, the others are the coefficients over the folds (it should be clear which are medians).\n",
      "\n",
      "\n",
      "\n",
      "**********************\n",
      "doc140_4_1\n",
      "*****************The question we are interested in is: ************************\n",
      "I would be very thankful if somebody could help me and explain the answer in simple terms.\n",
      "\n",
      "I have y, x, z variables. They are countinuous, no missing values. y is dependent variable, x and z independent. There is significant correlation between them (varies from 0.25 to 0.35).\n",
      "\n",
      "My hypothesis is that when z is very high, then x and y dependency is linear (x goes bigger, y smaller).\n",
      "When z is very low, then in the beginning the situation is the same (dependency is linear), but in the end (about last quarter) it changes to quadratic.\n",
      "\n",
      "So basically there are two different relationships between x and y, but the nature of the relationship depends on z which is again a continuous variable. \n",
      "\n",
      "How can I test this model? Is it possible doing linear regression or do I need to use something else?\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "similarity is  (25661, 0.3984331786632538)\n",
      "I am interested in writing a non-asymptotic rate of convergence for SLLN as a function of number of samples.  \n",
      "\n",
      "From the literature I've read so far, CLT provides an asymptotic convergence rate of $(1/\\sqrt N)$ for SLLN. \n",
      "\n",
      "Also, Berry-Esseen provides a non-asymptotic bound in terms of c.d.f's.\n",
      "\n",
      "$$|F_N(x) - \\Phi(x)| \\le \\frac{C\\mathbb{E}(|X|^3)}{\\sigma^3\\sqrt N}$$ \n",
      "\n",
      "Is there a Berry-Esseen like statement to bound the difference between sample mean and the expected value of the underlying distribution as a function of N (number of samples)? \n",
      "\n",
      "**********************\n",
      "similarity is  (33963, 0.3416898250579834)\n",
      "For regression analysis, it is often useful to know the data generating process to check how the used method works. While it is fairly simple to do this for a simple linear regression, this is not the case when the dependent variable has to follow a specific distribution. \n",
      "\n",
      "Consider a simple linear regression:\n",
      "\n",
      "N    &lt;- 100\n",
      "x    &lt;- rnorm(N)\n",
      "beta &lt;- 3 + 0.4*rnorm(N)\n",
      "y    &lt;- 1 + x * beta + .75*rnorm(N)\n",
      "\n",
      "\n",
      "Is there any way to use the same approach but to get y be other than normal, say left skewed?\n",
      "\n",
      "**********************\n",
      "similarity is  (87921, 0.3350788354873657)\n",
      "If you perform, say 10 fold cv with logistic regression and then average the coefficient vectors from each turn, does that average roughly equal the coefficient vector you would get by fitting a logistic regression to the whole dataset?\n",
      "\n",
      "Here is an example using a sklearn logistic regression that performs a random search over C on a very small dataset (1000 obs) with 20% positive cases. There are 5 features. Y axis is coefficient value.  One set of red lines are the coefficients on the full set, the others are the coefficients over the folds (it should be clear which are medians).\n",
      "\n",
      "\n",
      "\n",
      "**********************\n",
      "doc20_4_1\n",
      "*****************The question we are interested in is: ************************\n",
      "I would be very thankful if somebody could help me and explain the answer in simple terms.\n",
      "\n",
      "I have y, x, z variables. They are countinuous, no missing values. y is dependent variable, x and z independent. There is significant correlation between them (varies from 0.25 to 0.35).\n",
      "\n",
      "My hypothesis is that when z is very high, then x and y dependency is linear (x goes bigger, y smaller).\n",
      "When z is very low, then in the beginning the situation is the same (dependency is linear), but in the end (about last quarter) it changes to quadratic.\n",
      "\n",
      "So basically there are two different relationships between x and y, but the nature of the relationship depends on z which is again a continuous variable. \n",
      "\n",
      "How can I test this model? Is it possible doing linear regression or do I need to use something else?\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "similarity is  (53205, 0.7938619256019592)\n",
      "Is there a limit to how big you should make K for k-fold cross validation? I understand as K gets bigger performing the CV will take longer, but aside from that, is there any reason not to make K = n? is there a point where your validation sets are so small that even if you average them, the result is garbage?\n",
      "\n",
      "**********************\n",
      "similarity is  (96417, 0.7830068469047546)\n",
      "This is according to the Ljung-Box $Q$ statistic of residuals squared and ARCH-LM test. Both suggest there are ARCH effects remaining after lag 1 even after I have estimated my GARCH (1,1) model. I have no AR or MA terms in my conditional mean specification, just a constant, and all autocorrelation has been removed as per the $Q$-statistics of the residuals. What would be the best way to then remove the remaining ARCH effects?\n",
      "\n",
      "Is there even a need to remove all ARCH effects after the GARCH estimation (given GARCH models are iid), if one wishes to forecast volatility?\n",
      "\n",
      "**********************\n",
      "similarity is  (21764, 0.7724628448486328)\n",
      "Summary:\n",
      "I created a model that tries to explain success factors in crowdfunding initiatives. I collected the data by scraping it directly from the crowdfunding platform. That of course limits my research in terms of data availability. \n",
      "\n",
      "In order to come up with conceptual model I adopted a theory, that assumes latent variables (such that I do not observe). What I observe is measures or proxies (directly collected form the website) that to some extent depict that latent variables which are embedded in my conceptual framework. Of course I set few hypothesis based on that latent variables.\n",
      "\n",
      "Example: I have adopt a theory about the online community reputation. For measuring reputation I set 2 measures - N of successfully funded projects, N unsuccessfully funded projects. That example is quite simple, however, but for some other concepts I have multiple measures on different scales.\n",
      "\n",
      "Questions: \n",
      "1) How should I test my model reliability or/and validity? Is that necessary? \n",
      "\n",
      "I think I may not consider Cronbach's alpha in that case, since its not a survey approach, and often the measurement scales are different. Another option I was thinking of is CFA, but SPSS have only EFA. Would setting an EFA with defined number of factors equal to the number of my latent variables is an option?\n",
      "\n",
      "2) How I should test (accept/reject) my hypothesis? Should I refer to the measures themselves, or should I refer to the latent variables? How I can create the latent variables if necessary?\n",
      "\n",
      "**********************\n",
      "doc200_1_1\n",
      "*****************The question we are interested in is: ************************\n",
      "I would be very thankful if somebody could help me and explain the answer in simple terms.\n",
      "\n",
      "I have y, x, z variables. They are countinuous, no missing values. y is dependent variable, x and z independent. There is significant correlation between them (varies from 0.25 to 0.35).\n",
      "\n",
      "My hypothesis is that when z is very high, then x and y dependency is linear (x goes bigger, y smaller).\n",
      "When z is very low, then in the beginning the situation is the same (dependency is linear), but in the end (about last quarter) it changes to quadratic.\n",
      "\n",
      "So basically there are two different relationships between x and y, but the nature of the relationship depends on z which is again a continuous variable. \n",
      "\n",
      "How can I test this model? Is it possible doing linear regression or do I need to use something else?\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "similarity is  (21683, 0.32222890853881836)\n",
      "I am new to regression analysis so please excuse my ignorance.\n",
      "\n",
      "I have collected 5 years of annual panel data, and I would like to regress this data on eviews but it is saying there is an insufficient amount of observations. Is there any way around this? I currently have 3 independent variables, 1 dependent and 3 control variables.\n",
      "\n",
      "My suggestion is to just copy the same data down 4 times and regress that but I am thinking that may give me incorrect results.\n",
      "\n",
      "Any help would be appreciated even if just to confirm that it isn't possible.\n",
      "\n",
      "**********************\n",
      "similarity is  (87921, 0.2865654230117798)\n",
      "If you perform, say 10 fold cv with logistic regression and then average the coefficient vectors from each turn, does that average roughly equal the coefficient vector you would get by fitting a logistic regression to the whole dataset?\n",
      "\n",
      "Here is an example using a sklearn logistic regression that performs a random search over C on a very small dataset (1000 obs) with 20% positive cases. There are 5 features. Y axis is coefficient value.  One set of red lines are the coefficients on the full set, the others are the coefficients over the folds (it should be clear which are medians).\n",
      "\n",
      "\n",
      "\n",
      "**********************\n",
      "similarity is  (77388, 0.27398672699928284)\n",
      "I have four items with binary (yes/no) outcome and one item with nominal outcomes and one respondent can pick at most three of those nominal outcomes. What is the right IRT model for assessing scores in this scenario? I was looking in to grid response and could not find any literature yet. Can anybody tell me if R has any model for grid response?\n",
      "\n",
      "**********************\n",
      "doc80_1_1\n",
      "*****************The question we are interested in is: ************************\n",
      "I would be very thankful if somebody could help me and explain the answer in simple terms.\n",
      "\n",
      "I have y, x, z variables. They are countinuous, no missing values. y is dependent variable, x and z independent. There is significant correlation between them (varies from 0.25 to 0.35).\n",
      "\n",
      "My hypothesis is that when z is very high, then x and y dependency is linear (x goes bigger, y smaller).\n",
      "When z is very low, then in the beginning the situation is the same (dependency is linear), but in the end (about last quarter) it changes to quadratic.\n",
      "\n",
      "So basically there are two different relationships between x and y, but the nature of the relationship depends on z which is again a continuous variable. \n",
      "\n",
      "How can I test this model? Is it possible doing linear regression or do I need to use something else?\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "similarity is  (15463, 0.47558698058128357)\n",
      "Could someone please explain what \"$J$\" consists of in this paper, equation 1.5.\n",
      "$$\n",
      "J \\sim N(\\beta, \\sigma^2 I/k)\n",
      "$$\n",
      "What's $\\beta$ here? What's $N$?\n",
      "\n",
      "Also, why are they putting that much effort in deriving $k$ with complicated formulas instead of just saying \"use CV to find the best $k$\"?\n",
      "\n",
      "**********************\n",
      "similarity is  (25661, 0.4738842248916626)\n",
      "I am interested in writing a non-asymptotic rate of convergence for SLLN as a function of number of samples.  \n",
      "\n",
      "From the literature I've read so far, CLT provides an asymptotic convergence rate of $(1/\\sqrt N)$ for SLLN. \n",
      "\n",
      "Also, Berry-Esseen provides a non-asymptotic bound in terms of c.d.f's.\n",
      "\n",
      "$$|F_N(x) - \\Phi(x)| \\le \\frac{C\\mathbb{E}(|X|^3)}{\\sigma^3\\sqrt N}$$ \n",
      "\n",
      "Is there a Berry-Esseen like statement to bound the difference between sample mean and the expected value of the underlying distribution as a function of N (number of samples)? \n",
      "\n",
      "**********************\n",
      "similarity is  (87921, 0.4556872844696045)\n",
      "If you perform, say 10 fold cv with logistic regression and then average the coefficient vectors from each turn, does that average roughly equal the coefficient vector you would get by fitting a logistic regression to the whole dataset?\n",
      "\n",
      "Here is an example using a sklearn logistic regression that performs a random search over C on a very small dataset (1000 obs) with 20% positive cases. There are 5 features. Y axis is coefficient value.  One set of red lines are the coefficients on the full set, the others are the coefficients over the folds (it should be clear which are medians).\n",
      "\n",
      "\n",
      "\n",
      "**********************\n",
      "doc200_4_1\n",
      "*****************The question we are interested in is: ************************\n",
      "I would be very thankful if somebody could help me and explain the answer in simple terms.\n",
      "\n",
      "I have y, x, z variables. They are countinuous, no missing values. y is dependent variable, x and z independent. There is significant correlation between them (varies from 0.25 to 0.35).\n",
      "\n",
      "My hypothesis is that when z is very high, then x and y dependency is linear (x goes bigger, y smaller).\n",
      "When z is very low, then in the beginning the situation is the same (dependency is linear), but in the end (about last quarter) it changes to quadratic.\n",
      "\n",
      "So basically there are two different relationships between x and y, but the nature of the relationship depends on z which is again a continuous variable. \n",
      "\n",
      "How can I test this model? Is it possible doing linear regression or do I need to use something else?\n",
      "\n",
      "*******************Similar questions are **********************\n",
      "similarity is  (21683, 0.32222890853881836)\n",
      "I am new to regression analysis so please excuse my ignorance.\n",
      "\n",
      "I have collected 5 years of annual panel data, and I would like to regress this data on eviews but it is saying there is an insufficient amount of observations. Is there any way around this? I currently have 3 independent variables, 1 dependent and 3 control variables.\n",
      "\n",
      "My suggestion is to just copy the same data down 4 times and regress that but I am thinking that may give me incorrect results.\n",
      "\n",
      "Any help would be appreciated even if just to confirm that it isn't possible.\n",
      "\n",
      "**********************\n",
      "similarity is  (87921, 0.2865654230117798)\n",
      "If you perform, say 10 fold cv with logistic regression and then average the coefficient vectors from each turn, does that average roughly equal the coefficient vector you would get by fitting a logistic regression to the whole dataset?\n",
      "\n",
      "Here is an example using a sklearn logistic regression that performs a random search over C on a very small dataset (1000 obs) with 20% positive cases. There are 5 features. Y axis is coefficient value.  One set of red lines are the coefficients on the full set, the others are the coefficients over the folds (it should be clear which are medians).\n",
      "\n",
      "\n",
      "\n",
      "**********************\n",
      "similarity is  (77388, 0.27398672699928284)\n",
      "I have four items with binary (yes/no) outcome and one item with nominal outcomes and one respondent can pick at most three of those nominal outcomes. What is the right IRT model for assessing scores in this scenario? I was looking in to grid response and could not find any literature yet. Can anybody tell me if R has any model for grid response?\n",
      "\n",
      "**********************\n"
     ]
    }
   ],
   "source": [
    "for key in doc_models.keys():\n",
    "    print(key)\n",
    "    evaluate_doc_model(doc_models[key],3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
